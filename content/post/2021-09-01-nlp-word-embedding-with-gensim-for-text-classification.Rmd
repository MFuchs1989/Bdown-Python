---
title: NLP - Word Embedding with GENSIM for Text-Classification
author: Michael Fuchs
date: '2021-09-01'
slug: nlp-word-embedding-with-gensim-for-text-classification
categories: []
tags: []
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 5
---



# 1 Introduction

In my last post ([NLP - Text Vectorization](https://michael-fuchs-python.netlify.app/2021/08/01/nlp-text-vectorization/)) I showed how to convert words from a text corpus into real numbers to make them readable for machine learning algorithms.
The problem with BoW or TF-IDF vectorization is that the semantics of the word are not encoded in its vector representation. For example, the words "mobile phone" and "cell phone" have similar meanings, but BoW or TF-IDF does not take this into account and ignores the meaning of the words.

And that's where Word Embedding comes in. 

Word embedding is one of the most popular language modeling techniques used to map words onto vectors of real numbers. It is able to represent words or phrases in a vector space with multiple dimensions that have semantic and syntactic similarity. Word embeddings can be created using various methods. 

**In contrast to BoW or TF-IDF, the word embedding approach vectorizes a word, placing words that have similar meanings closer together**. For example, the words "mobile phone" and "cell phone" would have a similar vector representation. This means that when the word is embedded, the meaning of the word is encoded in the vector.

The motivation behind converting text into semantic vectors is that the word embedding method is not only able to extract the semantic relations, but it should also preserve most of the relevant information about a text corpus.



# 2  Import the Libraries and the Data


```{r, eval=F, echo=T}
import pandas as pd
import numpy as np
import pickle as pk

from nltk.tokenize import word_tokenize
from sklearn.svm import SVC
import statistics

from gensim.models import Word2Vec
from gensim.models import KeyedVectors

from gensim.models.doc2vec import Doc2Vec
from gensim.models.doc2vec import TaggedDocument

from gensim.models import Phrases
from gensim.models.phrases import Phraser

import gensim.downloader as api
```



```{r, eval=F, echo=T}
df = pd.DataFrame({'Rating': [3,5,1,2],
                   'Text': ["I love sunflowers",
                            "Sunflowers fill my heart with joy",
                            "I love to look into the garden and see the flowers",
                            "Flowers especially sunflowers are the most beautiful"]})
df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p1.png)



Here I will quickly create tokens from our sample dataset, since Word2Vec from gensim can only do well with them.



```{r, eval=F, echo=T}
df['Text_Tokenized'] = df['Text'].str.lower().apply(word_tokenize)
df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p2.png)


# 3  Gensim - Word2Vec


Word2Vec from gensim is one of the most popular techniques for learning word embeddings using a flat neural network. It can be used with two methods: 

+ CBOW (Common Bag Of Words): Using the context to predict a target word
+ Skip Gram: Using a word to predict a target context

The corresponding layer structure looks like this: 


![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136s1.png)

Source: [Mikolov T., Chen K., Corrado G. & Dean J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.](https://arxiv.org/pdf/1301.3781.pdf)



When should which method be used?

CBOW is faster and can therefore be used well for large data sets. In addition, the representation of more frequent words is better than with Skip Gram. 

The Skip Gram method works well for smaller data sets and can represent rare words well.



## 3.1  Instantiation


First of all I instantiate the Word2Vec model. 
I use the following parameters:

+ `vector_size`: Determines the size of the vectors we want
+ `window`: Determines the number of words before and after the target word to be considered as context for the word
+ `min_count`: Determines the number of times a word must occur in the text corpus for a word vector to be created 


The exact meaning of these and other parameters can be read here: [GENSIM - Word2vec embeddings](https://radimrehurek.com/gensim/models/word2vec.html)

I deliberately assign the vector size (vector_size) to its own variable here, since I will need it again at a later time. 




```{r, eval=F, echo=T}
vector_size_n_w2v = 5

w2v_model = Word2Vec(vector_size=vector_size_n_w2v,
                     window=3,
                     min_count=1,
                     sg=0) # 0=CBOW, 1=Skip-gram

print(w2v_model)
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p3.png)

Now the creation of the vocabulary, which is to be learned by Word2Vec, takes place. 


```{r, eval=F, echo=T}
w2v_model.build_vocab(df['Text_Tokenized'])
print(w2v_model)
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p4.png)

Finally, the neural network is trained (here with the CBOW method) over 5 epochs.

```{r, eval=F, echo=T}
w2v_model.train(df['Text_Tokenized'], 
                total_examples=w2v_model.corpus_count, 
                epochs=5)
```


These steps do not necessarily have to be performed individually. If the following syntax is used (with the specification of the text corpus), the creation of the vocabulary as well as the training is carried out automatically. 

However, it is recommended to perform the steps separately, as some parameter settings can be made in each case. You can also read all about it here: [GENSIM - Word2vec embeddings](https://radimrehurek.com/gensim/models/word2vec.html)



```{r, eval=F, echo=T}
vector_size_n_w2v = 5

w2v_model = Word2Vec(df['Text_Tokenized'],
                     vector_size=vector_size_n_w2v,
                     window=3,
                     min_count=1,
                     sg=0, # 0=CBOW, 1=Skip-gram
                     epochs=5)

print(w2v_model)
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p5.png)


By supplying sentences directly in the original instantiation call, you essentially asked that one call to also do the build_vocab() & train() steps automatically using those sentences. 

It is recommended to save the Word2Vec model immediately after a training session, as this is usually a very time-consuming task. Furthermore, I also save the metric that was used for the `vector_size` parameter. 


```{r, eval=F, echo=T}
w2v_model.save("word2vec/word2vec_model")

pk.dump(vector_size_n_w2v, open('word2vec/vector_size_w2v_metric.pkl', 'wb'))
```


## 3.2  Exploration of the calculated Values

In the following I show a few commands how to display the calculated and stored values.

```{r, eval=F, echo=T}
# The learned vocabulary:
w2v_model.wv.index_to_key
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p6.png)

```{r, eval=F, echo=T}
# Length of the learned vocabulary:
len(w2v_model.wv.index_to_key)
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p7.png)


```{r, eval=F, echo=T}
# Output of the calculated vector for a given word from the vocabulary:
w2v_model.wv['sunflowers']
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p8.png)


```{r, eval=F, echo=T}
# Length of the calculated vector:
len(w2v_model.wv['sunflowers'])
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p9.png)


```{r, eval=F, echo=T}
# Display the words that are most similar to a given word from the vocabulary:
w2v_model.wv.most_similar('sunflowers')
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p10.png)


## 3.3  Generation of aggregated Sentence Vectors

Now we will generate aggregate sentence vectors based on the word vectors for each word in the given sentence.



```{r, eval=F, echo=T}
words = set(w2v_model.wv.index_to_key )
df['Text_vect'] = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])
                         for ls in df['Text_Tokenized']])


df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p11.png)

Since we used `min_count=1` in the model training, each word was learned by Word2Vec and got a vector (with length of 5). 

Unfortunately, our example sentences have different numbers of words, so the database is correspondingly heterogeneous:

```{r, eval=F, echo=T}
for i, v in enumerate(df['Text_vect']):
    print(len(df['Text_Tokenized'].iloc[i]), len(v))
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p12.png)


As we can see from the output shown above, the first sentence from the dataset is assigned three vectors (each with a length of 5). This would result in a count of 15 features.

The second sentence would be assigned 30 features (6*5).

However, a machine learning model wants to see a consistent set of features for each example. Currently, a model training error would be thrown out.

To get the number of features equal, the next step is to calculate an element-wise average of the different vectors assigned to a sentence. To explain in more detail how this should work, let's look at the three vectors of the first sentence:


```{r, eval=F, echo=T}
content_sentence1_Text_vect = list(df['Text_vect'].loc[0:0])
content_sentence1_Text_vect
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p13.png)


Each of these word vectors has a size of 5 because we set it that way when we trained our Word2Vec model. Now we will average the first element of these three word vectors and store it as the first entry in our final vector. We do this for all further elements so that we finally get a final vector with length 5 which describes the sentence.

I have listed this calculation manually once for our present example:

```{r, eval=F, echo=T}
element1 = [0.1476101,
            -0.03632087,
            -0.01072454]

element2 = [-0.03066945,
            0.05751216,
            0.0047286]

element3 = [-0.09073229,
            0.01985285,
            0.10206699]

element4 = [0.13108101,
            -0.16571797,
            0.18018547]

element5 = [-0.09720321,
            -0.18894958,
            -0.186059]

element1_mean = statistics.mean(element1)
element2_mean = statistics.mean(element2)
element3_mean = statistics.mean(element3)
element4_mean = statistics.mean(element4)
element5_mean = statistics.mean(element5)


manually_calculated_mean_values = [[element1_mean, element2_mean, element3_mean, element4_mean, element5_mean]]

manually_calculated_mean_values_df = pd.DataFrame(manually_calculated_mean_values)
manually_calculated_mean_values_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p14.png)

This is now the expected output for our first record.

Since I don't want to do this manual calculation for each record separately (a vector size of 5 is very small and in real life you have more than 4 entries in the data set) I use a for-loop for this.


## 3.4  Generation of averaged Sentence Vectors

As described above, I will now generate sentence vectors based on the averaging of the word vectors for the words contained in the sentence. If you can remember I assigned the vector size to a separate variable ('vector_size_n') during instantiation. This is needed again in the else statement of the following for-loop.


```{r, eval=F, echo=T}
text_vect_avg = []
for v in df['Text_vect']:
    if v.size:
        text_vect_avg.append(v.mean(axis=0))
    else:
        text_vect_avg.append(np.zeros(vector_size_n, dtype=float)) # the same vector size must be used here as for model training
        
        
df['Text_vect_avg'] = text_vect_avg
df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p15.png)

Let's check again if the vector lengths are now consistent:

```{r, eval=F, echo=T}
# Are our sentence vector lengths consistent?
for i, v in enumerate(df['Text_vect_avg']):
    print(len(df['Text_Tokenized'].iloc[i]), len(v))
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p16.png)


A machine learning algorithm cannot work directly with the column generated above ('Text_vect_avg'). I have added it to the dataset for completeness. However, I continue to work with the created dictionary 'text_vect_avg'.

```{r, eval=F, echo=T}
df_Machine_Learning = pd.DataFrame(text_vect_avg)
df_Machine_Learning
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p17.png)

To make this dataset a little prettier, I add names to the columns:



```{r, eval=F, echo=T}
df_Machine_Learning.columns = ['Element_' + str(i+1) for i in range(0, df_Machine_Learning.shape[1])]
df_Machine_Learning
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p18.png)

Let's do a plausibility check again at this point.

In an earlier step, I manually calculated the final vector for the first sentence. Let's briefly compare the result of the for-loop with the manually calculated result:


```{r, eval=F, echo=T}
df_Machine_Learning.iloc[:1]
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p19.png)



```{r, eval=F, echo=T}
manually_calculated_mean_values_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p20.png)

Fits perfectly.

Now I create my final data set with which I can train a machine learning model in a meaningful way:



```{r, eval=F, echo=T}
final_df = pd.concat([df[['Rating', 'Text']], df_Machine_Learning], axis=1, sort=False)
final_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p21.png)


## 3.5  Model Training


```{r, eval=F, echo=T}
clf = SVC(kernel='linear')
clf.fit(df_Machine_Learning, final_df['Rating'])
```

```{r, eval=F, echo=T}
pk.dump(clf, open('clf_model.pkl', 'wb'))
```



## 3.6  Processing of new Input

What I would like to show is how to deal with new input, i.e. if you want to use a trained model, so that the predictions can be run. 


### 3.6.1  Load the Word2Vec Model


```{r, eval=F, echo=T}
w2v_model_reloaded = Word2Vec.load("word2vec/word2vec_model")
vector_size_n_reloaded = pk.load(open("word2vec/vector_size_w2v_metric.pkl",'rb'))

print(w2v_model_reloaded)
print(vector_size_n_reloaded)
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p22.png)


### 3.6.2  Load the new Input


```{r, eval=F, echo=T}
new_input = ["Flowers I like to see in the park especially sunflowers", 
             "I like flowers"]

print(new_input[0])
print(new_input[1])
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p23.png)

It makes sense to transfer it to a dataframe if you don't get the data that way anyway.


```{r, eval=F, echo=T}
new_input_df = pd.DataFrame(new_input, columns=['New_Input'])
new_input_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p24.png)


### 3.6.3  Pre-Processing of the new Input

Let's apply the steps shown earlier to the new dataset as well so we can use the ML model.


```{r, eval=F, echo=T}
new_input_df['New_Input_Tokenized'] = new_input_df['New_Input'].str.lower().apply(word_tokenize)
new_input_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p25.png)



```{r, eval=F, echo=T}
words = set(w2v_model_reloaded.wv.index_to_key )
new_input_df['New_Input_vect'] = np.array([np.array([w2v_model_reloaded.wv[i] for i in ls if i in words])
                                           for ls in new_input_df['New_Input_Tokenized']])


new_input_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p26.png)



```{r, eval=F, echo=T}
for i, v in enumerate(new_input_df['New_Input_vect']):
    print(len(new_input_df['New_Input_Tokenized'].iloc[i]), len(v))
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p27.png)


Here we see now that only 7 out of 10 (or 2 out of 3 in the second sentence) words were already learned by Word2Vec and only accordingly many word vectors were merged in the column 'New_Input_vect'. 

```{r, eval=F, echo=T}
text_vect_avg = []
for v in new_input_df['New_Input_vect']:
    if v.size:
        text_vect_avg.append(v.mean(axis=0))
    else:
        text_vect_avg.append(np.zeros(vector_size_n_reloaded, dtype=float)) # the same vector size must be used here as for model training
        
        
new_input_df['New_Input_vect_avg'] = text_vect_avg
new_input_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p28.png)





```{r, eval=F, echo=T}
for i, v in enumerate(new_input_df['New_Input_vect_avg']):
    print(len(new_input_df['New_Input_Tokenized'].iloc[i]), len(v))
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p29.png)


Fits, the final vector length is uniform again.

```{r, eval=F, echo=T}
new_input_Machine_Learning_df = pd.DataFrame(text_vect_avg)
new_input_Machine_Learning_df.columns = ['Element_' + str(i+1) for i in range(0, new_input_Machine_Learning_df.shape[1])]
new_input_Machine_Learning_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p30.png)




```{r, eval=F, echo=T}
final_new_input_df = pd.concat([new_input_df[['New_Input']], new_input_Machine_Learning_df], axis=1, sort=False)
final_new_input_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p31.png)



### 3.6.4  Model Predictions

Okay, now we are ready to receive the predictions.


```{r, eval=F, echo=T}
clf_reloaded = pk.load(open("clf_model.pkl",'rb'))

y_pred = clf_reloaded.predict(new_input_Machine_Learning_df)
y_pred
```


```{r, eval=F, echo=T}
final_new_input_df['Prediction'] = y_pred
final_new_input_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p32.png)


## 3.7  Updating the Word2Vec Model

After all, it is not uncommon to want to regularly improve your existing model. Word2Vec offers a simple solution for this.

Let's assume that the content of the new record ('new_input_df') has pleased me quite well and I want to train my existing w2v_model the contained vocabulary.


```{r, eval=F, echo=T}
new_input_df
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p33.png)

In the following, I have compared what Word2Vec have already learned with the new vocabulary and **found three words that the model does not yet know**.

```{r, eval=F, echo=T}
list_new_input = []

for value in new_input_df.New_Input.str.lower().str.split(' '):
    list_new_input.extend(value)

set(list_new_input) - set(list(w2v_model_reloaded.wv.index_to_key))
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p34.png)


Just a reminder, the current existing vocabulary contains 20 words:

```{r, eval=F, echo=T}
len(w2v_model_reloaded.wv.index_to_key)
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p35.png)


### 3.7.1  Updating the Weights

First I load the model I want to update.

```{r, eval=F, echo=T}
w2v_model_reloaded = Word2Vec.load("word2vec/word2vec_model")
```

Then I train the Word2Vec model on the new data set (the tokens, since Word2Vec can only handle this). 

Make sure that you **specify the correct length / the length of the correct dataset**, because this step was not necessary before during instantiation. 

```{r, eval=F, echo=T}
w2v_model_reloaded.train(new_input_df['New_Input_Tokenized'], 
                         total_examples=len(new_input_df),
                         epochs=10)
```


Let's look at the length of the new vocabulary:

```{r, eval=F, echo=T}
len(w2v_model_reloaded.wv.index_to_key)
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p36.png)


Mhhh... strange. My vocabulary has not changed. But what about the weights of the features and consequently the word vectors?

For this I compare the calculated vector (for the word 'flowers') of the model w2v_model and w2v_model_reloaded (which we just updated). 


```{r, eval=F, echo=T}
print(w2v_model.wv['flowers'])
print(w2v_model_reloaded.wv['flowers'])
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p37.png)


Ok the weights have changed at least. 

I also assumed this, since the word 'flowers' was present in the learned vocabulary as well as in the newly learned text corpus. What about the vectors for a word that was not included in the new text corpus?


```{r, eval=F, echo=T}
print(w2v_model.wv['sunflowers'])
print(w2v_model_reloaded.wv['sunflowers'])
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p38.png)


Here the vectors remained the same. However, this **should not have been mandatory**, because finally the semantic relation of several words is considered in Word2Vec. Even if a word is not necessarily relearned, its vector(s) can still be influenced by the newly added words. 

But how do I now also adapt the vocabulary?


### 3.7.2  Updating the Weights and the Vocabulary

Also for this I reload the Word2Vec model, because each execution of a learning process (which the weights updated) is kept.

```{r, eval=F, echo=T}
w2v_model_reloaded = Word2Vec.load("word2vec/word2vec_model")
```

In order to also include the new vocabulary to the existing one, you need to apply the .build_vocab() function before retraining.

```{r, eval=F, echo=T}
w2v_model_reloaded.build_vocab(new_input_df['New_Input_Tokenized'], update=True)
print(w2v_model_reloaded)
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p39.png)

Perfect, that worked (see 'vocab=23'). 

Now the training can be started to adjust the weights of the features as well. 

It is no longer necessary to pay explicit attention to the `total_examples` parameter as in the previous chapter, since this information is already given by the instantiation and can be retrieved using .corpus_count.

```{r, eval=F, echo=T}
w2v_model_reloaded.train(new_input_df['New_Input_Tokenized'], 
                         total_examples=w2v_model_reloaded.corpus_count, 
                         epochs=20)
```

Let's compare again the calculated vectors of the models w2v_model and w2v_model_reloaded for the word 'flowers':


```{r, eval=F, echo=T}
print(w2v_model.wv['flowers'])
print(w2v_model_reloaded.wv['flowers'])
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p40.png)


Great, the vectors have adjusted again.

And what about the vectors for 'sunflowers'?


```{r, eval=F, echo=T}
print(w2v_model.wv['sunflowers'])
print(w2v_model_reloaded.wv['sunflowers'])
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p41.png)


This time there was also an adjustment, because the newly learned vocabulary has created new word contexts and these have influenced the new calculation of the vectors.



## 3.8  Final saving of the Word2Vec Model

### 3.8.1  Saving the entire Model

If you want to continue the model training at a later time, you have to save the complete model (as shown before):

```{r, eval=F, echo=T}
w2v_model_reloaded.save("word2vec/w2v_model_updated")
```


### 3.8.2  Saving the KeyedVectors

If the model training is completely finished and should not be repeated / updated, it is recommended to save only the KeyedVectors, which contain the calculated vectors of the learned vocabulary.

A trained Word2Vec model can reach a very high memory capacity very quickly. KeyedVectors, on the other hand, are much smaller and faster objects that can be used to load the required vectors very quickly while conserving memory.

```{r, eval=F, echo=T}
w2v_model_reloaded_vectors = w2v_model_reloaded.wv

w2v_model_reloaded_vectors.save("word2vec/w2v_model_vectors_updated")
```

Ok, let's reload it once:


```{r, eval=F, echo=T}
w2v_model_vectors_reloaded = KeyedVectors.load("word2vec/w2v_model_vectors_updated", mmap='r')
print(w2v_model_vectors_reloaded)
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p42.png)

Be aware that from now on **the additional argument .wv is no longer needed**. 


```{r, eval=F, echo=T}
w2v_model_vectors_reloaded['sunflowers']
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p43.png)

Here again for comparison that the same values have been saved:

```{r, eval=F, echo=T}
w2v_model_reloaded.wv['sunflowers']
```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p44.png)



















```{r, eval=F, echo=T}

```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p.png)





















```{r, eval=F, echo=T}

```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p.png)





















```{r, eval=F, echo=T}

```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p.png)

























```{r, eval=F, echo=T}

```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p.png)




















```{r, eval=F, echo=T}

```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p.png)





















```{r, eval=F, echo=T}

```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p.png)



















```{r, eval=F, echo=T}

```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p.png)
























```{r, eval=F, echo=T}

```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p.png)















```{r, eval=F, echo=T}

```

![](/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p.png)




