---
title: Ensemble Modeling - Stacking
author: Michael Fuchs
date: '2020-04-24'
slug: ensemble-modeling-stacking
categories:
  - R
tags:
  - R Markdown
---



<div id="table-of-content" class="section level1">
<h1>Table of Content</h1>
<ul>
<li>1 Introduction</li>
<li>2 Backgroundinformation on Stacking</li>
<li>3 Loading the libraries and the data</li>
<li>4 Data pre-processing</li>
<li>4.1 One-hot-encoding</li>
<li>4.2 LabelBinarizer</li>
<li>4.3 Train-Test-Split</li>
<li>4.4 Convert to a numpy array</li>
<li>5 Building a stacked model</li>
<li>5.1 Create a new training set</li>
<li>5.2 Train base models</li>
<li>5.3 Create a new test set</li>
<li>5.4 Fit base models on the complete training set</li>
<li>5.5 Train the stacked model</li>
<li>6 Comparison of the accuracy</li>
<li>7 Conclusion</li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>After <a href="https://michael-fuchs-python.netlify.app/2020/03/07/ensemble-modeling-bagging/">“Bagging”</a> and <a href="https://michael-fuchs-python.netlify.app/2020/03/26/ensemble-modeling-boosting/">“Boosting”</a> we come to the last type of ensemble method: Stacking.</p>
<p>For this post the dataset <em>Bank Data</em> from the platform <a href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing">“UCI Machine Learning repository”</a> was used. A copy of the record is available at <a href="https://drive.google.com/open?id=1MEt3YiQfNxkCl75WSROWf1L5p9_f4FcD" class="uri">https://drive.google.com/open?id=1MEt3YiQfNxkCl75WSROWf1L5p9_f4FcD</a>.</p>
</div>
<div id="backgroundinformation-on-stacking" class="section level1">
<h1>2 Backgroundinformation on Stacking</h1>
<p>The aim of this technique is to increase the predictie power of the classifier, as it involves training multiple models and then using a combiner algorithm to make the final prediction by using the predictions from all these models additional inputs.</p>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44s1.png" /></p>
<p>As you can see in the above pricture, this model ensembling technique combining information from multiple predictive models and using them as features to generate a new model. Stacking uses the predictions of the base models as additional features when training the final model… These are known as meta features. The stacked model essentially acts as a classifier that determines where each model is performing well and where it is performing poorly.</p>
<p>Let’s generate a stacked model step by step.</p>
</div>
<div id="loading-the-libraries-and-the-data" class="section level1">
<h1>3 Loading the libraries and the data</h1>
<pre class="r"><code>import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split


from sklearn.model_selection import KFold

# Our base models
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC
# Our stacking model
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score</code></pre>
<pre class="r"><code>bank = pd.read_csv(&quot;path/to/file/bank.csv&quot;, sep=&quot;;&quot;)
bank.head()</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p1.png" /></p>
<p>The data set before us contains information about whether a customer has signed a contract or not.</p>
<pre class="r"><code>bank[&#39;y&#39;].value_counts().T</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p2.png" /></p>
</div>
<div id="data-pre-processing" class="section level1">
<h1>4 Data pre-processing</h1>
</div>
<div id="one-hot-encoding" class="section level1">
<h1>4.1 One-hot-encoding</h1>
<p>First of all we have to convert the categorical variables into numerical ones again.
To see how this work exactly please have a look at this post: <a href="https://michael-fuchs-python.netlify.app/2019/06/16/types-of-encoder/">“Types of Encoder”</a></p>
<pre class="r"><code>safe_y = bank[[&#39;y&#39;]]

col_to_exclude = [&#39;y&#39;]
bank = bank.drop(col_to_exclude, axis=1)</code></pre>
<pre class="r"><code>#Just select the categorical variables
cat_col = [&#39;object&#39;]
cat_columns = list(bank.select_dtypes(include=cat_col).columns)
cat_data = bank[cat_columns]
cat_vars = cat_data.columns

#Create dummy variables for each cat. variable
for var in cat_vars:
    cat_list = pd.get_dummies(bank[var], prefix=var)
    bank=bank.join(cat_list)

    
data_vars=bank.columns.values.tolist()
to_keep=[i for i in data_vars if i not in cat_vars]

#Create final dataframe
bank_final=bank[to_keep]
bank_final.columns.values</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p3.png" /></p>
<pre class="r"><code>bank = pd.concat([bank_final, safe_y], axis=1)
bank</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p4.png" /></p>
<p>Now we have a data set that contains almost exclusively numerical variables.
But as we can see, we still have to convert the target variable. We do not do this with one hot encoding but with the LabelBinarizer from scikit-learn.</p>
</div>
<div id="labelbinarizer" class="section level1">
<h1>4.2 LabelBinarizer</h1>
<pre class="r"><code>encoder = LabelBinarizer()

encoded_y = encoder.fit_transform(bank.y.values.reshape(-1,1))
encoded_y</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p5.png" /></p>
<pre class="r"><code>bank[&#39;y_encoded&#39;] = encoded_y
bank[&#39;y_encoded&#39;] = bank[&#39;y_encoded&#39;].astype(&#39;int64&#39;)
bank</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p6.png" /></p>
<p>Here we see that the values of the newly generated target variables (here ‘y_encoded’) are now 0 or 1.
Of course we can no longer take the ‘old’ target variable (here ‘y’) into account in the further evaluation. We will throw them out in the next step, the train-test-split.</p>
</div>
<div id="train-test-split" class="section level1">
<h1>4.3 Train-Test-Split</h1>
<pre class="r"><code>x = bank.drop([&#39;y&#39;, &#39;y_encoded&#39;], axis=1)
y = bank[&#39;y_encoded&#39;]

trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
<p>It is not a big deal.</p>
</div>
<div id="convert-to-a-numpy-array" class="section level1">
<h1>4.4 Convert to a numpy array</h1>
<p>For the following steps, it is necessary to convert the generated objects into numpy arrays.</p>
<pre class="r"><code>trainX = trainX.to_numpy()
trainX</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p7.png" /></p>
<pre class="r"><code>testX = testX.to_numpy()
testX</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p8.png" /></p>
<pre class="r"><code>trainY = trainY.to_numpy()
trainY</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p9.png" /></p>
<pre class="r"><code>testY = testY.to_numpy()
testY</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p10.png" /></p>
</div>
<div id="building-a-stacked-model" class="section level1">
<h1>5 Building a stacked model</h1>
<p>In the following I will use a support vector machine (scikit-learns’s LinearSVC) and k-nearest neighbors (scikit-learn’s KneighboorsClassifier) as the base predictors and the stacked model will be a logistic regression classifier.</p>
<p>I explained the exact functioning of these algorithms in these posts:</p>
<ul>
<li><a href="https://michael-fuchs-python.netlify.app/2019/11/08/introduction-to-support-vector-machines/">“Support Vector Machines”</a></li>
<li><a href="https://michael-fuchs-python.netlify.app/2019/12/27/introduction-to-knn-classifier/">“KNN Classifier”</a></li>
<li><a href="https://michael-fuchs-python.netlify.app/2019/10/31/introduction-to-logistic-regression/">“Logistic Regression”</a></li>
</ul>
</div>
<div id="create-a-new-training-set" class="section level1">
<h1>5.1 Create a new training set</h1>
<p>First of all we create a new training set with additional columns for predictions from base predictors.</p>
<pre class="r"><code>trainX_with_metapreds = np.zeros((trainX.shape[0], trainX.shape[1]+2))
trainX_with_metapreds[:, :-2] = trainX
trainX_with_metapreds[:, -2:] = -1
print(trainX_with_metapreds)</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p11.png" /></p>
<pre class="r"><code>print(trainX.shape)
print(trainX_with_metapreds.shape)</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p12.png" /></p>
<p>Here we can see that two more columns have been added.</p>
</div>
<div id="train-base-models" class="section level1">
<h1>5.2 Train base models</h1>
<p>Now we are going to train the base models using the k-fold strategy.</p>
<pre class="r"><code>kf = KFold(n_splits=5, random_state=11)

for train_indices, test_indices in kf.split(trainX):
    kfold_trainX, kfold_testX = trainX[train_indices], trainX[test_indices]
    kfold_trainY, kfold_testY = trainY[train_indices], trainY[test_indices]
    
    svm = LinearSVC(random_state=11, max_iter=1000)
    svm.fit(kfold_trainX, kfold_trainY)
    svm_pred = svm.predict(kfold_testX)
    
    knn = KNeighborsClassifier(n_neighbors=4)
    knn.fit(kfold_trainX, kfold_trainY)
    knn_pred = knn.predict(kfold_testX)
    
    trainX_with_metapreds[test_indices, -2] = svm_pred
    trainX_with_metapreds[test_indices, -1] = knn_pred</code></pre>
</div>
<div id="create-a-new-test-set" class="section level1">
<h1>5.3 Create a new test set</h1>
<p>As I did in chapter 5.1, I will add two placeholder columns for the base model predictions in the test dataset as well.</p>
<pre class="r"><code>testX_with_metapreds = np.zeros((testX.shape[0], testX.shape[1]+2))
testX_with_metapreds[:, :-2] = testX
testX_with_metapreds[:, -2:] = -1
print(testX_with_metapreds)</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p13.png" /></p>
</div>
<div id="fit-base-models-on-the-complete-training-set" class="section level1">
<h1>5.4 Fit base models on the complete training set</h1>
<p>Next, I will train the two base predictors on the complete training set to get the meta prediction values for the test dataset. This is similar to what I did for each fold in chapter 5.2.</p>
<pre class="r"><code>svm = LinearSVC(random_state=11, max_iter=1000)
svm.fit(trainX, trainY)

knn = KNeighborsClassifier(n_neighbors=4)
knn.fit(trainX, trainY)

svm_pred = svm.predict(testX)
knn_pred = knn.predict(testX)

testX_with_metapreds[:, -2] = svm_pred
testX_with_metapreds[:, -1] = knn_pred</code></pre>
</div>
<div id="train-the-stacked-model" class="section level1">
<h1>5.5 Train the stacked model</h1>
<p>The last step is to train the logistic regression model on all the columns of the training dataset plus the meta predictions rom the base estimators.</p>
<pre class="r"><code>lr = LogisticRegression(random_state=11)
lr.fit(trainX_with_metapreds, trainY)
lr_preds_train = lr.predict(trainX_with_metapreds)
lr_preds_test = lr.predict(testX_with_metapreds)

print(&#39;Stacked Classifier:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on test data = {:.4f}&#39;.format(
    accuracy_score(y_true=trainY, y_pred=lr_preds_train),
    accuracy_score(y_true=testY, y_pred=lr_preds_test)
))</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p14.png" /></p>
</div>
<div id="comparison-of-the-accuracy" class="section level1">
<h1>6 Comparison of the accuracy</h1>
<p>To get a sense of the performance boost from stacking, I will calculate the accuracies of the base predictors on the training and test dataset and compare it to that of the stacked model:</p>
<pre class="r"><code># Comparing accuracy with that of base predictors

print(&#39;SVM:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on test data = {:.4f}&#39;.format(
    accuracy_score(y_true=trainY, y_pred=svm.predict(trainX)),
    accuracy_score(y_true=testY, y_pred=svm_pred)
))
print(&#39;kNN:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on test data = {:.4f}&#39;.format(
    accuracy_score(y_true=trainY, y_pred=knn.predict(trainX)),
    accuracy_score(y_true=testY, y_pred=knn_pred)
))</code></pre>
<p><img src="/post/2020-04-24-ensemble-modeling-stacking_files/p44p15.png" /></p>
<p>As we can see we get a higher accuracy on the test dataset with the stacked model as with the base predictors alone.</p>
</div>
<div id="conclusion" class="section level1">
<h1>7 Conclusion</h1>
<p>In this and in the last two posts I presented the use of various ensemble methods.
It has been shown that the use of these methods leads to a significantly better result
than the conventional machine learning algorithms alone.</p>
</div>
