---
title: k-Means Clustering
author: Michael Fuchs
date: '2020-05-19'
slug: k-means-clustering
categories:
  - R
tags:
  - R Markdown
---

# Table of Content

+ 1 Introduction
+ 2 Loading the libraries
+ 3 Introducing k-Means
+ 4 Preparation of the data record
+ 5 Application of k-Means
+ 6 Determine the optimal k for k-Means
+ 7 Conclusion


# 1 Introduction

After dealing with supervised machine learning models, we now come to another important data science area: unsupervised machine learning models.
One class of the unsupervised machine learning models are the cluster algorithms. Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points.

We will start this section with one of the most famous cluster algorithms: k-Means Clustering.

For this post the dataset *House Sales in King County, USA* from the statistic platform ["Kaggle"](https://www.kaggle.com) was used. A copy of the record is available at <https://drive.google.com/open?id=1DNhgjyC8oueXIaJU5wVJ6r8diNwTs1JO>.


# 2 Loading the libraries


```{r, eval=F, echo=T}
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
```


# 3 Introducing k-Means

K-means clustering aims to partition data into k clusters in a way that data points in the same cluster are similar and data points in the different clusters are farther apart. In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.

![](/post/2020-05-19-k-means-clustering_files/p45s1.png)

There are many methods to measure the distance. Euclidean distance is one of most commonly used distance measurements. 


**How the K-means algorithm works**

To process the learning data, the K-means algorithm in data mining starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative calculations to optimize the positions of the centroids.


# 4 Preparation of the data record

At the beginning we throw unnecessary variables out of the data set.

```{r, eval=F, echo=T}
house = pd.read_csv("path/to/file/houce_prices.csv")
house = house.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis=1)
house.head()
```

![](/post/2020-05-19-k-means-clustering_files/p45p1.png)


Now we are creating a new target variable.
To do this we look at a histogram that shows house prices.

```{r, eval=F, echo=T}
plt.hist(house['price'], bins='auto')
plt.title("Histogram for house prices")
plt.xlim(xmin=0, xmax = 1200000)
plt.show()
```

![](/post/2020-05-19-k-means-clustering_files/p45p2.png)

Now we have a rough idea of how we could divide house prices into three categories: cheap, medium and expensive.
To do so we'll use the following function:

```{r, eval=F, echo=T}
def house_price_cat(df):

    if (df['price'] >= 600000):
        return 'expensive'
    
    elif (df['price'] < 600000) and (df['price'] > 300000):
        return 'medium'
              
    elif (df['price'] <= 300000):
        return 'cheap'
```



```{r, eval=F, echo=T}
house['house_price_cat'] = house.apply(house_price_cat, axis = 1)
house.head()
```

![](/post/2020-05-19-k-means-clustering_files/p45p3.png)

As we can see, we now have a new column with three categories of house prices.

```{r, eval=F, echo=T}
house['house_price_cat'].value_counts().T
```

![](/post/2020-05-19-k-means-clustering_files/p45p4.png)

For our further procedure we convert these into numerical values.
There are several ways to do so. 
If you are interested in the different methods, check out this post from me: ["Types of Encoder"](https://michael-fuchs-python.netlify.app/2019/06/16/types-of-encoder/)











```{r, eval=F, echo=T}

```

![](/post/2020-05-19-k-means-clustering_files/p45p1.png)






