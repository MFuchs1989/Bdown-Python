---
title: Hierarchical Clustering
author: Michael Fuchs
date: '2020-06-04'
slug: hierarchical-clustering
categories:
  - R
tags:
  - R Markdown
---



# Table of Content

+ 1 Introduction
+ 2 Loading the libraries
+ 3 Introducing hierarchical clustering
+ 4 Dendrograms explained
+ 5 Hierarchical Clustering with Scikit-Learn




# 1 Introduction

The second cluster algorithm I would like present is hierarchical clustering.
Hierarchical clustering is also a type of unsupervised machine learning algorithm used to cluster unlabeled data points within a dataset. Like ["k-Means Clustering"](https://michael-fuchs-python.netlify.app/2020/05/19/k-means-clustering/), hierarchical clustering also groups together the data points with similar characteristics.

For this post the dataset *Mall_Customers* from the statistic platform ["Kaggle"](https://www.kaggle.com) was used. A copy of the record is available at <https://drive.google.com/file/d/16aeG6dy8j1P70kqTEBsJm4fojK3_a6cq/view?usp=sharing>.


# 2 Loading the libraries


```{r, eval=F, echo=T}
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
```


# 3 Introducing hierarchical clustering

**Theory of hierarchical clustering**

There are two types of hierarchical clustering: 
+ Agglomerative and 
+ Divisive

In the course of the first variant, data points are clustered using a bottom-up approach starting with individual data points, while in the second variant top-down approach is followed where all the data points are treated as one big cluster and the clustering process involves dividing the one big cluster into several small clusters.

In this post we will focus on agglomerative clustering that involves the bottom-up approach since this method is used almost exclusively in the real world.

**Steps to perform hierarchical clustering**

Following are the steps involved in agglomerative clustering:

+ At the beginning, treat each data point as one cluster. Therefore, the number of clusters at the start will be k, while k is an integer representing the number of data points
+ Second: Form a cluster by joining the two closest data points resulting in k-1 clusters
+ Third: Form more clusters by joining the two closest clusters resulting in k-2 clusters


Repeat the above three steps until one big cluster is formed.
Once single cluster is formed, dendrograms are used to divide into multiple clusters depending upon the problem.



# 4 Dendrograms explained

Let me explain the use of dendrograms with the following example dataframe.


```{r, eval=F, echo=T}
df = pd.DataFrame({'Col1': [5, 9, 13, 22, 31, 90, 81, 70, 45, 73, 85],
                   'Col2': [2, 8, 11, 10, 25, 80, 90, 80, 60, 62, 90]})
df

```

![](/post/2020-06-04-hierarchical-clustering_files/p46p1.png)

First we'll convert this dataframe to a numpy array.

```{r, eval=F, echo=T}
arr = np.array(df)
arr
```

![](/post/2020-06-04-hierarchical-clustering_files/p46p2.png)

Now we plot the example dataframe:

```{r, eval=F, echo=T}
labels = range(1, 12)
plt.figure(figsize=(10, 7))
plt.subplots_adjust(bottom=0.1)
plt.scatter(arr[:,0],arr[:,1], label='True Position')

for label, x, y in zip(labels, arr[:, 0], arr[:, 1]):
    plt.annotate(
        label,
        xy=(x, y), xytext=(-3, 3),
        textcoords='offset points', ha='right', va='bottom')
plt.show()
```

![](/post/2020-06-04-hierarchical-clustering_files/p46p3.png)

Now we have a rough idea of the underlying distribution of the data points.
Let's plot our first dendrogram:

```{r, eval=F, echo=T}
linked = shc.linkage(arr, 'single')

labelList = range(1, 12)

plt.figure(figsize=(10, 7))
shc.dendrogram(linked,
            orientation='top',
            labels=labelList,
            distance_sort='descending',
            show_leaf_counts=True)
plt.axhline(y=23, color='r', linestyle='--')
plt.show()
```

![](/post/2020-06-04-hierarchical-clustering_files/p46p4.png)


The algorithm starts by finding the two points that are closest to each other on the basis of euclidean distance.
The vertical height of the dendogram shows the euclidean distances between points.


In the end, we can read from the present graphic that data points 1-5 forms a cluster as well as 6,7,8, 10 and 11.
Data point 9 seems to be a own cluster.

We can see that the largest vertical distance without any horizontal line passing through it is represented by blue line. 
So I draw a horizontal red line within the dendogram that passes through the blue line. 
Since it crosses the blue line at three points, therefore the number of clusters will be 3.



# 5 Hierarchical Clustering with Scikit-Learn

Now it's time to use scikits'AgglomerativeClustering class and call its fit_predict method to predict the clusters that each data point belongs to.

```{r, eval=F, echo=T}
hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
hc.fit_predict(arr)
```

![](/post/2020-06-04-hierarchical-clustering_files/p46p5.png)

Finally, let's plot our clusters.

```{r, eval=F, echo=T}
plt.scatter(arr[:,0],arr[:,1], c=hc.labels_, cmap='rainbow')
```

![](/post/2020-06-04-hierarchical-clustering_files/p46p6.png)























```{r, eval=F, echo=T}

```

![](/post/2020-06-04-hierarchical-clustering_files/p46p.png)









