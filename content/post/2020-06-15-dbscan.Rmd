---
title: DBSCAN
author: Michael Fuchs
date: '2020-06-15'
slug: dbscan
categories:
  - R
tags:
  - R Markdown
---



# Table of Content

+ 1 Introduction
+ 2 Loading the libraries
+ 3 Introducing DBSCAN
+ 4 DBSCAN with Scikit-Learn
+ 4.1 Data preparation
+ 4.2 DBSCAN
+ 5 Conclusion


# 1 Introduction

The next unsupervised machine learning cluster algorithms is the Density-Based Spatial Clustering and Application with Noise (DBSCAN).
DBSCAN is a density-based clusering algorithm, which can be used to identify clusters of any shape in a data set containing noise and outliers.



# 2 Loading the libraries


```{r, eval=F, echo=T}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN

from sklearn.metrics.cluster import adjusted_rand_score
```


# 3 Introducing DBSCAN


The basic idea behind DBSCAN is derived from a human intuitive clustering method. 

If you have a look at the picture below you can easily identify 2 clusters along with several points of noise, because of the differences in the density of points.

Clusters are dense regions in the data space, separated by regions of lower density of points. The Density-Based Spatial Clustering and Application with Noise algorithm is based on this intuitive notion of “clusters” and “noise”. The key idea is that for each point of a cluster, the neighborhood of a given radius has to contain at least a minimum number of points.


![](/post/2020-06-15-dbscan_files/p47s1.png)



# 4 DBSCAN with Scikit-Learn


In the following I will show you an example of some of the strengths of DBSCAN clustering when k-means clustering doesn’t seem to handle the data shape well.


# 4.1 Data preparation

For an illustrative example, I will create a data set artificially.


```{r, eval=F, echo=T}
# generate some random cluster data
X, y = make_blobs(random_state=170, n_samples=500, centers = 5)
rng = np.random.RandomState(74)
```


```{r, eval=F, echo=T}
# transform the data to be stretched
transformation = rng.normal(size=(2, 2))
X = np.dot(X, transformation)
```


```{r, eval=F, echo=T}
plt.scatter(X[:, 0], X[:, 1])
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```


![](/post/2020-06-15-dbscan_files/p47p1.png)



```{r, eval=F, echo=T}
# scaling the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```


# 4.1 k-Means

f you want to find out exactly how k-Means works have a look at this post of mine: ["k-Means Clustering"](https://michael-fuchs-python.netlify.app/2020/05/19/k-means-clustering/)
 
```{r, eval=F, echo=T}
# cluster the data into five clusters
kmeans = KMeans(n_clusters=5)
kmeans.fit(X_scaled)
y_pred = kmeans.predict(X_scaled)
```


```{r, eval=F, echo=T}
# plot the cluster assignments and cluster centers

plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap="viridis")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
```


![](/post/2020-06-15-dbscan_files/p47p2.png)

We can measure the performance with the adjusted_rand_score

```{r, eval=F, echo=T}
#k-means performance:
print("ARI =", adjusted_rand_score(y, y_pred))
```

![](/post/2020-06-15-dbscan_files/p47p3.png)


# 4.2 DBSCAN



```{r, eval=F, echo=T}
# cluster the data into five clusters
dbscan = DBSCAN(eps=0.123, min_samples = 2)

clusters = dbscan.fit_predict(X_scaled)
```


```{r, eval=F, echo=T}
# plot the cluster assignments
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap="viridis")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
```


![](/post/2020-06-15-dbscan_files/p47p4.png)

```{r, eval=F, echo=T}
#DBSCAN performance:
print("ARI =", adjusted_rand_score(y, clusters))
```


![](/post/2020-06-15-dbscan_files/p47p5.png)


Here we see that the performance of the DBSCANS is much higher than k-means with this data set.


Let's have a look at the labels:

```{r, eval=F, echo=T}
labels = dbscan.labels_
labels
```


![](/post/2020-06-15-dbscan_files/p47p6.png)


We can also add them to the original record.

```{r, eval=F, echo=T}
X_df = pd.DataFrame(X)
db_cluster = pd.DataFrame(labels)  

df = pd.concat([X_df, db_cluster], axis=1)
df.columns = ['Feature 1', 'Feature 2', 'db_cluster']
df.head()
```


![](/post/2020-06-15-dbscan_files/p47p7.png)


Let's have a detailed look at the generated clusters:


```{r, eval=F, echo=T}
view_cluster = df['db_cluster'].value_counts().T
view_cluster = pd.DataFrame(data=view_cluster)
view_cluster = view_cluster.reset_index()
view_cluster.columns = ['db_cluster', 'count']
view_cluster.sort_values(by='db_cluster', ascending=False)
```

![](/post/2020-06-15-dbscan_files/p47p8.png)


As we can see from the overview above, a point was assigned to cluster -1.
This is not a cluster at all, it's a noisy point.



```{r, eval=F, echo=T}
# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
print('Estimated number of clusters: %d' % n_clusters_)
```


![](/post/2020-06-15-dbscan_files/p47p9.png)



# 5 Conclusion

The example in this post showed that clustering also depends heavily on the distribution of the data points and that it is always worth trying out several cluster algorithms.


