---
title: Random sampling
author: Michael Fuchs
date: '2019-05-16'
slug: random-sampling
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#preparation">2 Preparation</a></li>
<li><a href="#split-methods">3 Split-Methods</a>
<ul>
<li><a href="#customer-churn-model">3.1 Customer Churn Model</a></li>
<li><a href="#train-test-split-via-scikit-learn">3.2 Train-Test Split via scikit-learn</a></li>
</ul></li>
<li><a href="#train-test-validation-split">4 Train-Test-Validation Split</a></li>
<li><a href="#conclusion">5 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>Splitting the dataset in training and testing the dataset is one operation every Data Scientist has to perform befor applying any models. The training dataset is the one on which the model is built and the testing dataset is used to check the accuracy of the model. Generally, the training and testing datasets are split in the ratio of 75:25 or 80:20. There are various ways to split the data into two halves. Here I will show two methods to do this.</p>
<p>For this post the dataset <em>flight</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used. You can download it from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets">GitHub Repository</a>.</p>
<p><strong>Loading the libraries and the data</strong></p>
<pre class="r"><code>import pandas as pd
import numpy as np</code></pre>
<pre class="r"><code>flight = pd.read_csv(&quot;path/to/file/flight.csv&quot;)</code></pre>
</div>
<div id="preparation" class="section level1">
<h1>2 Preparation</h1>
<p>For the two methods shown below, the first hundred lines from the record <em>flight</em> are used.</p>
<pre class="r"><code>sampling = flight.iloc[0:100,:]
sampling.shape</code></pre>
<p><img src="/post/2019-05-16-random-sampling_files/p10p1.png" /></p>
</div>
<div id="split-methods" class="section level1">
<h1>3 Split-Methods</h1>
<div id="customer-churn-model" class="section level2">
<h2>3.1 Customer Churn Model</h2>
<p>The division took place here in a ratio of 80:20.</p>
<pre class="r"><code>a=np.random.randn(len(sampling))
check=a&lt;0.8
training=sampling[check]
testing=sampling[~check]</code></pre>
<pre class="r"><code>len(training)</code></pre>
<p><img src="/post/2019-05-16-random-sampling_files/p10p2.png" /></p>
<pre class="r"><code>len(testing)</code></pre>
<p><img src="/post/2019-05-16-random-sampling_files/p10p3.png" /></p>
</div>
<div id="train-test-split-via-scikit-learn" class="section level2">
<h2>3.2 Train-Test Split via scikit-learn</h2>
<pre class="r"><code>from sklearn.model_selection import train_test_split
train, test = train_test_split(sampling, test_size = 0.2)</code></pre>
<pre class="r"><code>len(train)</code></pre>
<p><img src="/post/2019-05-16-random-sampling_files/p10p4.png" /></p>
<pre class="r"><code>len(test)</code></pre>
<p><img src="/post/2019-05-16-random-sampling_files/p10p5.png" /></p>
</div>
</div>
<div id="train-test-validation-split" class="section level1">
<h1>4 Train-Test-Validation Split</h1>
<p>Particular in the deep learning area (for example artificial neural networks), it is necessary to hold back part of the data set for validation purposes in addition to the training and test part.We can also do this with the train test split function shown above from scikit-learn. You only have to use this function twice in a row and change the percentage of the division. Let’s see here with the self-generated sample data set:</p>
<pre class="r"><code>df = pd.DataFrame(np.random.randint(0,100,size=(10000, 4)), columns=[&#39;Var1&#39;, &#39;Var2&#39;, &#39;Var3&#39;, &#39;Target_Var&#39;])
df.head()</code></pre>
<p><img src="/post/2019-05-16-random-sampling_files/p10p6.png" /></p>
<pre class="r"><code>df.shape</code></pre>
<p><img src="/post/2019-05-16-random-sampling_files/p10p7.png" /></p>
<p>Ok we have generated 4 columns with 10k rows each.</p>
<p>Now we assign the predictors and the criterion to an object. This is a common step to train a machine learning model and could have already done with the previous methods as well.</p>
<pre class="r"><code>x = df.drop(&#39;Target_Var&#39;, axis=1)
y = df[&#39;Target_Var&#39;]</code></pre>
<p>Now we use the train_test_split function twice. First split with 80:20, second with 75:25.</p>
<pre class="r"><code>trainX_FULL, testX, trainY_FULL, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
<pre class="r"><code>trainX, validationX, trainY, validationY = train_test_split(trainX_FULL, trainY_FULL, test_size = 0.25)</code></pre>
<p>As a result, we receive a training part of 6,000 observations and a validation and test part of 2,000 observations each:</p>
<pre class="r"><code>print(trainX.shape)
print(validationX.shape)
print(testX.shape)
print(trainY.shape)
print(validationY.shape)
print(testY.shape)</code></pre>
<p><img src="/post/2019-05-16-random-sampling_files/p10p8.png" /></p>
</div>
<div id="conclusion" class="section level1">
<h1>5 Conclusion</h1>
<p>Now we are ready for predictive modelling.</p>
</div>
