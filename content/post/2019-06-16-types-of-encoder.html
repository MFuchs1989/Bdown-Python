---
title: Types of Encoder
author: Michael Fuchs
date: '2019-06-16'
slug: types-of-encoder
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#loading-the-libraries-and-the-data">2 Loading the libraries and the data</a></li>
<li><a href="#encoder-for-predictor-variables">3 Encoder for predictor variables</a>
<ul>
<li><a href="#one-hot-encoder">3.1 One Hot Encoder</a>
<ul>
<li><a href="#via-scikit-learn">3.1.1 via scikit-learn</a></li>
<li><a href="#via-pandas">3.1.2 via pandas</a></li>
</ul></li>
<li><a href="#ordinal-encoder">3.2 Ordinal Encoder</a></li>
<li><a href="#multilabelbinarizer">3.3 MultiLabelBinarizer</a></li>
</ul></li>
<li><a href="#encoder-for-target-variables">4 Encoder for target variables</a>
<ul>
<li><a href="#label-binarizer">4.1 Label Binarizer</a></li>
<li><a href="#label-encoding">4.2 Label Encoding</a></li>
</ul></li>
<li><a href="#inverse-transformation">5 Inverse Transformation</a></li>
<li><a href="#export-encoder-to-use-in-another-program">6 Export Encoder to use in another program</a></li>
<li><a href="#conclusion">7 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29s1.png" /></p>
<p>As mentioned in my previous <a href="https://michael-fuchs-python.netlify.com/2019/06/14/the-use-of-dummy-variables/">“post”</a>, before you can start modeling, a lot of preparatory work is often necessary when preparing the data. In this post the most common encoding algorithms from the scikit-learn library will be presented and how they are to be used.</p>
</div>
<div id="loading-the-libraries-and-the-data" class="section level1">
<h1>2 Loading the libraries and the data</h1>
<pre class="r"><code>import numpy as np
import pandas as pd


from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.preprocessing import LabelBinarizer
from sklearn.preprocessing import LabelEncoder


import pickle as pk</code></pre>
<pre class="r"><code>df = pd.DataFrame({&#39;Job&#39;: [&#39;Doctor&#39;, &#39;Farmer&#39;, &#39;Electrician&#39;, &#39;Teacher&#39;, &#39;Pilot&#39;],
                   &#39;Emotional_State&#39;: [&#39;good&#39;, &#39;bad&#39;, &#39;neutral&#39;, &#39;very_good&#39;, &#39;excellent&#39;],
                   &#39;Age&#39;: [32,22,62,44, 54],
                   &#39;Salary&#39;: [4700, 2400,4500,2500, 3500],
                   &#39;Purchased&#39;: [&#39;Yes&#39;, &#39;No&#39;, &#39;No&#39;, &#39;Yes&#39;, &#39;No&#39;]})
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p1.png" /></p>
</div>
<div id="encoder-for-predictor-variables" class="section level1">
<h1>3 Encoder for predictor variables</h1>
<div id="one-hot-encoder" class="section level2">
<h2>3.1 One Hot Encoder</h2>
<p>I already wrote about the functioning and creation of dummy variables in my post <a href="https://michael-fuchs-python.netlify.com/2019/06/14/the-use-of-dummy-variables/">“The use of dummy variables”</a>. In scikit-learn this function is known as One Hot Encoding.</p>
<div id="via-scikit-learn" class="section level3">
<h3>3.1.1 via scikit-learn</h3>
<p>In a nutshell One Hot Encoder encode categorical features as a one-hot numeric array:</p>
<pre class="r"><code>encoder = OneHotEncoder()

OHE = encoder.fit_transform(df.Job.values.reshape(-1,1)).toarray()
df_OH = pd.DataFrame(OHE, columns = [&quot;Job_&quot; + str(encoder.categories_[0][i]) 
                                     for i in range(len(encoder.categories_[0]))])


df_OH_final = pd.concat([df, df_OH], axis=1)
df_OH_final</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p2.png" /></p>
</div>
<div id="via-pandas" class="section level3">
<h3>3.1.2 via pandas</h3>
<pre class="r"><code>df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p3.png" /></p>
<p>You can also create dummy variables with the .get_dummies function from pandas. I think this method is more practical because it uses less syntax.</p>
<pre class="r"><code>df_dummies = pd.get_dummies(df, prefix=[&#39;Job&#39;], columns=[&#39;Job&#39;])
df_dummies</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p4.png" /></p>
<p>How to use this function in data analysis is explained in detail in this <a href="https://michael-fuchs-python.netlify.com/2019/06/14/the-use-of-dummy-variables/">“post”</a>.</p>
</div>
</div>
<div id="ordinal-encoder" class="section level2">
<h2>3.2 Ordinal Encoder</h2>
<p>In some cases, categorical variables follow a certain order (in our example here, this is the column ‘Emotional_State’).</p>
<pre class="r"><code>df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p5.png" /></p>
<p>Hereby One hot encoding would result in the loss of valuable information (ranking).
Here you can see how the Ordinal Encoder from scikit-learn works:</p>
<pre class="r"><code>encoder = OrdinalEncoder()

ord_Emotional_State = encoder.fit_transform(df.Emotional_State.values.reshape(-1,1))
ord_Emotional_State</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p6.png" /></p>
<p>Now we insert the generated array into the existing dataframe:</p>
<pre class="r"><code>df[&#39;ord_Emotional_State&#39;] = ord_Emotional_State
df[&#39;ord_Emotional_State&#39;] = df[&#39;ord_Emotional_State&#39;].astype(&#39;int64&#39;)
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p7.png" /></p>
<p>But in my opinion Ordinal Encoder from scikit-learn has a big disadvantage. The order is assigned arbitrarily:</p>
<pre class="r"><code>df[[&#39;Emotional_State&#39;, &#39;ord_Emotional_State&#39;]].sort_values(by=&#39;ord_Emotional_State&#39;, ascending=False)</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p8.png" /></p>
<p>The assigned order makes little sense in reality. I would therefore suggest the following method.
A sensible order is first defined and then mapped to the desired variable:</p>
<pre class="r"><code>Emotional_State_dict = {&#39;bad&#39; : 0,
                        &#39;neutral&#39; : 1,
                        &#39;good&#39; : 2,
                        &#39;very_good&#39; : 3,
                        &#39;excellent&#39; : 4}

df[&#39;Emotional_State_Ordinal&#39;] = df.Emotional_State.map(Emotional_State_dict)
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p9.png" /></p>
<p>Now we have a sensible order:</p>
<pre class="r"><code>df[[&#39;Emotional_State&#39;, &#39;Emotional_State_Ordinal&#39;]].sort_values(by=&#39;Emotional_State_Ordinal&#39;, ascending=False)</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p10.png" /></p>
</div>
<div id="multilabelbinarizer" class="section level2">
<h2>3.3 MultiLabelBinarizer</h2>
<p>MultiLabelBinarizer basically works something like One Hot Encoding.
The difference is that for a given column, a row can contain not only one value but several.
Have a look at this example:</p>
<pre class="r"><code>df = pd.DataFrame({&quot;genre&quot;: [[&quot;action&quot;, &quot;drama&quot;,&quot;fantasy&quot;], [&quot;fantasy&quot;,&quot;action&quot;, &quot;animation&quot;], [&quot;drama&quot;, &quot;action&quot;], [&quot;sci-fi&quot;, &quot;action&quot;]],
                  &quot;title&quot;: [&quot;Twilight&quot;, &quot;Alice in Wonderland&quot;, &quot;Tenet&quot;, &quot;Star Wars&quot;]})
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p34.png" /></p>
<p>Here we have assigned multiple genres for each film listed. Makes sense.
To create a matrix with one column for each genre listed we need MultiLabelBinarizer.</p>
<pre class="r"><code>mlb = MultiLabelBinarizer()

res = pd.DataFrame(mlb.fit_transform(df[&#39;genre&#39;]),
                   columns=mlb.classes_,
                   index=df[&#39;genre&#39;].index)
res</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p35.png" /></p>
<p>Now all we have to do is delete the old column from the original data set and merge the two data sets (df and res).</p>
<pre class="r"><code>df = df.drop(&#39;genre&#39;, axis=1)
df = pd.concat([df, res], axis=1, sort=False)
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p36.png" /></p>
<p>That’s it.</p>
</div>
</div>
<div id="encoder-for-target-variables" class="section level1">
<h1>4 Encoder for target variables</h1>
<p>Before that, we looked at which encoding methods make sense for predictor variables. Now let’s look at which ones make sense for target variables.</p>
<div id="label-binarizer" class="section level2">
<h2>4.1 Label Binarizer</h2>
<p>Let’s have a look at the original dataframe.</p>
<pre class="r"><code>df = df.drop([&#39;ord_Emotional_State&#39;, &#39;Emotional_State_Ordinal&#39;], axis=1)
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p11.png" /></p>
<p>The Label Binarizer function from scikit-learn is able to convert binary variables (variables with only two classes) into numerical values (0 &amp; 1).</p>
<pre class="r"><code>encoder = LabelBinarizer()

encoded_Purchased = encoder.fit_transform(df.Purchased.values.reshape(-1,1))
encoded_Purchased</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p12.png" /></p>
<p>Now we are integrating this array back into our data set:</p>
<pre class="r"><code>df[&#39;Purchased_Encoded&#39;] = encoded_Purchased
df[&#39;Purchased_Encoded&#39;] = df[&#39;Purchased_Encoded&#39;].astype(&#39;int64&#39;)
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p13.png" /></p>
</div>
<div id="label-encoding" class="section level2">
<h2>4.2 Label Encoding</h2>
<p>Unfortunately the label binarizer is no longer sufficient to prepare the data for multiclass classification algorithms. Hereby we need Label Encoding. In the following example, the column ‘Job’ should be our target variable.</p>
<pre class="r"><code>df = df[[&#39;Emotional_State&#39;, &#39;Salary&#39;, &#39;Purchased&#39;, &#39;Job&#39;]]
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p14.png" /></p>
<p>The Label Encoder now generates a numerical value for each individual class within this categorical variable.</p>
<pre class="r"><code>encoder = LabelEncoder()

df[&#39;Job_Encoded&#39;] = encoder.fit_transform(df.Job)
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p15.png" /></p>
<p>The syntax below shows which class has been assigned which value.</p>
<pre class="r"><code>target = df[&#39;Job&#39;]   
integerEncoded = encoder.fit_transform(target)
integerMapping=dict(zip(target,integerEncoded))
integerMapping</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p16.png" /></p>
<p>You can also use the .inverse_transform function to find out which classes have been assigned the values (here) 0 and 1.</p>
<pre class="r"><code>encoder.inverse_transform([0, 1])</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p17.png" /></p>
<p>Finally, it is shown how to apply the .inverse_transform function to an entire column and add it back to the original dataframe.</p>
<pre class="r"><code>target_encoded = df[&#39;Job_Encoded&#39;]
invers_transformed = encoder.inverse_transform(target_encoded)
df[&#39;Job_Invers_Transformed&#39;] = invers_transformed
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p18.png" /></p>
<p>I would not recommend the use of this encoder for predictor variables, because the assigned order (0 &lt; 1 &lt; 2 &lt; 3 …) could have an incorrect influence on the model. Use One Hot Encoding instead.</p>
</div>
</div>
<div id="inverse-transformation" class="section level1">
<h1>5 Inverse Transformation</h1>
<p>Now that we have learned some methods of encoding I would like to introduce the inverse_transform function.
The encoding of data is usually a necessary step for the training of machine learning algorithms.
For a good interpretation of the results it is usually advantageous to transform the coded data back again.
But this is easy to do.</p>
<p>We take this dataframe as an example:</p>
<pre class="r"><code>df = pd.DataFrame({&#39;Job&#39;: [&#39;Doctor&#39;, &#39;Farmer&#39;, &#39;Electrician&#39;, &#39;Teacher&#39;, &#39;Pilot&#39;],
                   &#39;Emotional_State&#39;: [&#39;good&#39;, &#39;bad&#39;, &#39;neutral&#39;, &#39;very_good&#39;, &#39;excellent&#39;],
                   &#39;Age&#39;: [32,22,62,44, 54],
                   &#39;Salary&#39;: [4700, 2400,4500,2500, 3500],
                   &#39;Purchased&#39;: [&#39;Yes&#39;, &#39;No&#39;, &#39;No&#39;, &#39;Yes&#39;, &#39;No&#39;]})
df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p19.png" /></p>
<p>and use one-hot encoding again:</p>
<pre class="r"><code>encoder = OneHotEncoder()

OHE_fit = encoder.fit(df.Job.values.reshape(-1,1))
OHE_transform = OHE_fit.transform(df.Job.values.reshape(-1,1)).toarray()

OHE_transform</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p20.png" /></p>
<p>Can save the result as before in a dataframe. Both methods work.</p>
<pre class="r"><code>df_OHE = pd.DataFrame(OHE_transform, columns = [&quot;Job_&quot; + str(encoder.categories_[0][i]) 
                                     for i in range(len(encoder.categories_[0]))])

df_OHE</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p21.png" /></p>
<p>Now we are ready to use the inverse_transform function.</p>
<pre class="r"><code>re_transformed_array = encoder.inverse_transform(OHE_transform)
re_transformed_array</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p22.png" /></p>
<pre class="r"><code>re_transformed_df = encoder.inverse_transform(df_OHE)
re_transformed_df</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p23.png" /></p>
<p>As we can see the inverse_transform function works with the created array as well as with the created dataframe.
Now I append the re_transformed_array to the dataframe (df_OHE).</p>
<pre class="r"><code>df_OHE[&#39;inverse_transform&#39;] = re_transformed_array
df_OHE</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p23z1.png" /></p>
</div>
<div id="export-encoder-to-use-in-another-program" class="section level1">
<h1>6 Export Encoder to use in another program</h1>
<p>When we develop machine learning algorithms it is important to store the (in our current case) encoders separately so that they can be used again later.</p>
<pre class="r"><code>pk.dump(encoder, open(&#39;encoder.pkl&#39;, &#39;wb&#39;))</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p23z2.png" /></p>
<p>Now we reload the just saved encoder (encoder.pkl).</p>
<pre class="r"><code>encoder_reload = pk.load(open(&quot;encoder.pkl&quot;,&#39;rb&#39;))</code></pre>
<p>Now let’s test the reloaded encoder with the following dataframe.</p>
<pre class="r"><code>df_new = pd.DataFrame({&#39;Job_Doctor&#39;: [1,0,0,0,0],
                   &#39;Job_Electrician&#39;: [0,1,0,0,0],
                   &#39;Job_Farmer&#39;: [0,0,0,0,1],
                   &#39;Job_Pilot&#39;: [0,0,0,1,0],
                   &#39;Job_Teacher&#39;: [0,0,1,0,0]})
df_new</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p24.png" /></p>
<pre class="r"><code>re_transformed_df_new = encoder_reload.inverse_transform(df_new)
re_transformed_df_new</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p25.png" /></p>
<p>It works!</p>
<p>Of course you will need less the inverse_transform function of a stored encoder, but the advantage of an already fitted encoder is that you will notice immediately if something has changed compared to the original files (which you also used during training).</p>
<p>What do I mean specifically?</p>
<p>Suppose we have developed an algorithm using OneHotEncoding to prepare the data.
Now we get new data on the basis of which we should make new predictions.
Logically we have to convert the categorical data into numerical data (via OHE). Ideally in exactly the same way as with the original data on which the training of the used algorithm is based.</p>
<p>Therefore we store the encoder separately and load it for new data to practice OHE. This way we can be sure that</p>
<ul>
<li>we get the same encoding and</li>
<li>we also have the same learned categories.</li>
</ul>
<p>If new categories are added and the encoder is applied to the wrong column, we will see this immediately as the following examples will show.</p>
<pre class="r"><code>df_dummy1 = pd.DataFrame({&#39;Job&#39;: [&#39;Doctor&#39;, &#39;Farmer&#39;, &#39;Electrician&#39;, &#39;Teacher&#39;, &#39;Pilot&#39;],
                   &#39;Emotional_State&#39;: [&#39;good&#39;, &#39;bad&#39;, &#39;neutral&#39;, &#39;very_good&#39;, &#39;excellent&#39;],
                   &#39;Age&#39;: [32,22,62,44, 54],
                   &#39;Salary&#39;: [4700, 2400,4500,2500, 3500],
                   &#39;Purchased&#39;: [&#39;Yes&#39;, &#39;No&#39;, &#39;No&#39;, &#39;Yes&#39;, &#39;No&#39;]})
df_dummy1</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p26.png" /></p>
<pre class="r"><code>test_df_dummy1 = encoder_reload.transform(df_dummy1.Emotional_State.values.reshape(-1,1)).toarray()
test_df_dummy1</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p27.png" /></p>
<p>Here we have specified a wrong column on which the encoder was not trained.</p>
<pre class="r"><code>df_dummy1_part2 = pd.DataFrame({&#39;Job&#39;: [&#39;craftsman&#39;, &#39;merchant&#39;, &#39;sales&#39;]})
df_dummy1_part2</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p28.png" /></p>
<pre class="r"><code>test_df_dummy1_part2 = encoder_reload.transform(df_dummy1_part2.Job.values.reshape(-1,1)).toarray()
test_df_dummy1_part2</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p29.png" /></p>
<p>Here we have tried to apply the encoder to new categories. Logically this does not work either. In such a case, the training of the algorithm would have to be reset.</p>
<pre class="r"><code>df_dummy2 = pd.DataFrame({&#39;Job_A&#39;: [1,0,0,0,0],
                   &#39;Job_B&#39;: [0,1,0,0,0],
                   &#39;Job_C&#39;: [0,0,0,0,1],
                   &#39;Job_D&#39;: [0,0,0,1,0],
                   &#39;Job_E&#39;: [0,0,1,0,0]})
df_dummy2</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p30.png" /></p>
<pre class="r"><code>test_df_dummy2 = encoder_reload.inverse_transform(df_dummy2)
test_df_dummy2</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p31.png" /></p>
<p>In this example we changed the column names but used the same number. This works technically but the result makes no sense.</p>
<pre class="r"><code>df_dummy3 = pd.DataFrame({&#39;Job_A&#39;: [1,0,0,0,0],
                   &#39;Job_B&#39;: [0,1,0,0,0],
                   &#39;Job_C&#39;: [0,0,0,0,1],
                   &#39;Job_D&#39;: [0,0,0,1,0]})
df_dummy3</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p32.png" /></p>
<pre class="r"><code>test_df_dummy3 = encoder_reload.inverse_transform(df_dummy3)
test_df_dummy3</code></pre>
<p><img src="/post/2019-06-16-types-of-encoder_files/p29p33.png" /></p>
<p>Here we have now left out a column. The reloaded encoder does not allow this either.</p>
</div>
<div id="conclusion" class="section level1">
<h1>7 Conclusion</h1>
<p>Here is a brief overview of which encoding methods are available and when to use them:</p>
<ul>
<li><p>One Hot Encoder: Generates additional features by transforming categorical variables and converts them into numerical values.</p></li>
<li><p>Ordinal Encoder: Transforms categorical variables into numerical ones and puts them in a meaningful order.</p></li>
<li><p>Label Binarizer: Transforms a categorical target variable into a binary numeric value.</p></li>
<li><p>Label Encoding: Transforms the classes of a multiclass categorical target variable into a numeric value.</p></li>
</ul>
</div>
