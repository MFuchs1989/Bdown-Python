---
title: AutoML using PyCaret - Classification
author: Michael Fuchs
date: '2022-01-01'
slug: automl-using-pycaret-classification
categories: []
tags: []
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#loading-the-libraries-and-data">2 Loading the Libraries and Data</a></li>
<li><a href="#pycaret---classification">3 PyCaret - Classification</a>
<ul>
<li><a href="#setup">3.1 Setup</a></li>
<li><a href="#compare-models">3.2 Compare Models</a>
<ul>
<li><a href="#comparison-of-specific-models">3.2.1 Comparison of Specific Models</a></li>
<li><a href="#further-settings">3.2.2 Further Settings</a></li>
</ul></li>
<li><a href="#model-evaluation">3.3 Model Evaluation</a></li>
<li><a href="#model-training">3.4 Model Training</a></li>
<li><a href="#model-optimization">3.5 Model Optimization</a>
<ul>
<li><a href="#tune-the-model">3.5.1 Tune the Model</a></li>
<li><a href="#retrieve-the-tuner">3.5.2 Retrieve the Tuner</a></li>
<li><a href="#automatically-choose-better">3.5.3 Automatically Choose Better</a></li>
<li><a href="#ensemble_models">3.5.4 ensemble_models</a></li>
<li><a href="#blend_models">3.5.5 blend_models</a></li>
<li><a href="#stack_models">3.5.6 stack_models</a></li>
<li><a href="#further-methods">3.5.7 Further Methods</a></li>
</ul></li>
<li><a href="#model-evaluation-after-training">3.6 Model Evaluation after Training</a></li>
<li><a href="#model-predictions">3.7 Model Predictions</a>
<ul>
<li><a href="#on-testx">3.7.1 On testX</a></li>
<li><a href="#on-unseen-data">3.7.2 On unseen data</a></li>
</ul></li>
<li><a href="#model-finalization">3.8 Model Finalization</a></li>
<li><a href="#saving-the-pipeline-model">3.9 Saving the Pipeline &amp; Model</a></li>
</ul></li>
<li><a href="#conclusion">4 Conclusion¶</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133s1.png" /></p>
<p>Source: <a href="https://github.com/pycaret">PyCaret-GitHub</a></p>
<p>Data Scientists tend to make their tasks more and more effective and efficient. This also applies to the field of machine learning model training.
There is a wide range of algorithms that can be used for regression and classification problems.
<a href="https://michael-fuchs-python.netlify.app/2021/05/11/machine-learning-pipelines/">Machine Learning Pipelines</a> have helped to simplify and speed up the process of finding the best model.</p>
<p>Now we come to the next stage: <strong>Automated Machine Learning Libraries</strong></p>
<p>In my research, I came across <a href="https://pycaret.org/"><strong>PyCaret</strong></a> in this regard.</p>
<p><a href="https://pycaret.org/">PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows.</a></p>
<p>It can be used to work on the following problems:</p>
<ul>
<li><a href="https://pycaret.gitbook.io/docs/get-started/quickstart#classification">Classification</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/quickstart#regression">Regression</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/quickstart#clustering">Clustering</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/quickstart#anomaly-detection">Anomaly Detection</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/quickstart#natural-language-processing">Natural Language Processing</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/quickstart#association-rules-mining">Association Rules Mining</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/quickstart#time-series-beta">Time Series (beta)</a></li>
</ul>
<p>When I read through the <a href="https://pycaret.gitbook.io/docs/">GitBook</a> and the <a href="https://pycaret.readthedocs.io/en/latest/">API Reference</a> PyCaret was very promising and I can say I was not disappointed.</p>
<p>In the following I will show how to create a classification algorithm with PyCaret and what other possibilities you have with this AutoML library.</p>
</div>
<div id="loading-the-libraries-and-data" class="section level1">
<h1>2 Loading the Libraries and Data</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

import pycaret.classification as pycc</code></pre>
<pre class="r"><code>class Color:
   PURPLE = &#39;\033[95m&#39;
   CYAN = &#39;\033[96m&#39;
   DARKCYAN = &#39;\033[36m&#39;
   BLUE = &#39;\033[94m&#39;
   GREEN = &#39;\033[92m&#39;
   YELLOW = &#39;\033[93m&#39;
   RED = &#39;\033[91m&#39;
   BOLD = &#39;\033[1m&#39;
   UNDERLINE = &#39;\033[4m&#39;
   END = &#39;\033[0m&#39;</code></pre>
<pre class="r"><code>bird_df = pd.read_csv(&#39;bird.csv&#39;).drop(&#39;id&#39;, axis=1)
bird_df.head()</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p1.png" /></p>
<pre class="r"><code>bird_df.isnull().sum()</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p2.png" /></p>
</div>
<div id="pycaret---classification" class="section level1">
<h1>3 PyCaret - Classification</h1>
<div id="setup" class="section level2">
<h2>3.1 Setup</h2>
<p>As a first step, I initialize the training environment. At the same time, the transformation pipeline is created.
This setup function takes two parameters:</p>
<ul>
<li>the dataset</li>
<li>the target variable</li>
</ul>
<pre class="r"><code>summary_preprocess = pycc.setup(bird_df, target = &#39;type&#39;)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p3.png" /></p>
<p>First we can check if the data types of all variables were recognized correctly. If this is the case, as here, we can press Enter.</p>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p4.png" /></p>
<p>Now we get a detailed overview (only a part of it is shown here) of the initialized setup and the performed transformation steps of the pipeline.</p>
<p>Default Transformations:</p>
<ul>
<li>Missing Value Imputation</li>
<li>Perfect Collinearity Removal</li>
<li>One-Hot Encoding</li>
<li>Train-Test Split</li>
</ul>
<p>All transformations can be set individually in the setup function. See here: <a href="https://pycaret.gitbook.io/docs/get-started/preprocessing">PyCaret Official - Preprocessing</a></p>
<p>These include:</p>
<ul>
<li><a href="https://pycaret.gitbook.io/docs/get-started/preprocessing/data-preparation">Data Preparation</a>
<ul>
<li>Missing Values</li>
<li>Data Types</li>
<li>Encoding</li>
</ul></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/preprocessing/scale-and-transform">Scale and Transform</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/preprocessing/feature-engineering">Feature Engineering</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/preprocessing/feature-selection">Feature Selection</a> and</li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/preprocessing/other-setup-parameters">Other Parameters</a></li>
</ul>
<p>Use these preprocessing and transformation parameters in the setup as needed for your present data set. In my example I leave it at the default settings.</p>
<p>Using the get_config function we can view the edited record.</p>
<pre class="r"><code>x = pycc.get_config(&#39;X&#39;)
y = pycc.get_config(&#39;y&#39;)
trainX = pycc.get_config(&#39;X_train&#39;)
testX = pycc.get_config(&#39;X_test&#39;)
trainY = pycc.get_config(&#39;y_train&#39;)
testY = pycc.get_config(&#39;y_test&#39;)</code></pre>
<pre class="r"><code>x</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p5.png" /></p>
<pre class="r"><code>y</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p6.png" /></p>
<p>Here we see how the target variable ‘type’ has been recoded to apply machine learning algorithms. This step can also be seen in the summary_preprocess in line 4 (index 3).</p>
<p>If you want to get other values or information from the initiated setup here is a list of variables that can be retrieved using the get_config function: <a href="https://pycaret.gitbook.io/docs/get-started/functions/others#get_config">Variables accessible by get_config function</a></p>
</div>
<div id="compare-models" class="section level2">
<h2>3.2 Compare Models</h2>
<p>Now that all preprocessing steps are completed, we can check the performance of the classification algorithms available in PyCaret.</p>
<pre class="r"><code>available_models = pycc.models()
available_models</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p7.png" /></p>
<p>Now let’s compare these models:</p>
<pre class="r"><code>best_clf = pycc.compare_models()</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p8.png" /></p>
<p>Another nice thing about this view is that the best values of each metric are highlighted in yellow. The Extra Trees Classifier achieved the highest value for almost every metric, so all highlighted fields are listed under this algorithm.</p>
<p>Let’s take a detailed look at the best model from the comparison:</p>
<pre class="r"><code>print(best_clf)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p9.png" /></p>
<p><strong>Changing the sort order</strong></p>
<p>Of course, the highest Accuracy is not always the decisive metric, so sorting can also be done according to other evaluation metrics. Here, for example, the syntax to sort by AUC score:</p>
<pre class="r"><code>best_clf_AUC = pycc.compare_models(sort = &#39;AUC&#39;)</code></pre>
<div id="comparison-of-specific-models" class="section level3">
<h3>3.2.1 Comparison of Specific Models</h3>
<p>Since model training can take different amounts of time, depending on the size of the data set, it is sometimes advisable to compare only certain models.</p>
<pre class="r"><code>best_clf_specific = pycc.compare_models(include = [&#39;ada&#39;, &#39;svm&#39;, &#39;nb&#39;])</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p10.png" /></p>
<p><strong>Exclusion of specific models:</strong></p>
<p>Alternatively, certain models can simply be excluded from the performance comparison. See the following syntax:</p>
<pre class="r"><code>best_clf_excl = pycc.compare_models(exclude = [&#39;dummy&#39;, &#39;ridge&#39;])</code></pre>
</div>
<div id="further-settings" class="section level3">
<h3>3.2.2 Further Settings</h3>
<p>In PyCaret you have the possibility to make further settings in the model comparison. These are:</p>
<ul>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/train#set-the-budget-time">Setting the Budget Time</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/train#set-the-probability-threshold">Setting the Probability Threshold</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/train#disable-cross-validation">Disable Cross-Validation</a></li>
</ul>
<p>The syntax to these settings would look like this:</p>
<pre class="r"><code>best_clf_further_settings = pycc.compare_models(budget_time = 0.7, 
                                                probability_threshold = 0.75, 
                                                cross_validation=False)</code></pre>
</div>
</div>
<div id="model-evaluation" class="section level2">
<h2>3.3 Model Evaluation</h2>
<p>Now that we have seen in our comparison (best_clf) that the Extra Trees Classifier has achieved the best values, I would like to go into the evaluation for this algorithm.</p>
<pre class="r"><code>print(best_clf)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p11.png" /></p>
<pre class="r"><code>evaluation_best_clf = pycc.evaluate_model(best_clf)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p12.png" /></p>
<p>Since the evaluate_model function can only be used in notebooks, the plot_model function can also be used as an alternative.</p>
<pre class="r"><code>pycc.plot_model(best_clf, plot = &#39;auc&#39;)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p13.png" /></p>
<pre class="r"><code>pycc.plot_model(best_clf, plot = &#39;confusion_matrix&#39;)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p14.png" /></p>
<pre class="r"><code>pycc.plot_model(best_clf, 
                plot = &#39;confusion_matrix&#39;, 
                plot_kwargs = {&#39;percent&#39; : True})</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p15.png" /></p>
<p><strong>Saving image files</strong></p>
<p>If you want to save the output graphics in PyCaret, you have to set the safe parameter to True.
The syntax would look like this:</p>
<pre class="r"><code>pycc.plot_model(best_clf, 
                plot = &#39;confusion_matrix&#39;, 
                plot_kwargs = {&#39;percent&#39; : True},
                save = True)</code></pre>
<p>Here is an overview of possible graphics in PyCaret: <a href="https://pycaret.gitbook.io/docs/get-started/functions/analyze#examples-by-module">Examples by module</a></p>
</div>
<div id="model-training" class="section level2">
<h2>3.4 Model Training</h2>
<p>In comparing the classification algorithms, we saw that Extra Trees Classifier performed the best.</p>
<pre class="r"><code>print(best_clf)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p16.png" /></p>
<p>So we will do the model training with it as well. This is done in PyCaret with the create_model function, which also automatically includes cross-validation. Here I set the number of cross-validation runs to 5 (default = 10).</p>
<pre class="r"><code>et_clf = pycc.create_model(&#39;et&#39;, fold = 5)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p17.png" /></p>
<p>For some specific algorithms, the hyperparameters can be set specifically. Here is an example with the Decision Tree Classifier:</p>
<pre class="r"><code>et_clf_custom_param = pycc.create_model(&#39;et&#39;, 
                                        max_depth = 5)</code></pre>
<p><strong>Further Settings</strong></p>
<p>As with model comparison, additional settings can be made during model training:</p>
<ul>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/train#set-the-probability-threshold">Setting the Probability Threshold</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/train#disable-cross-validation">Disable Cross-Validation</a></li>
</ul>
<p>The syntax to these settings would look like this:</p>
<pre class="r"><code>et_clf_further_settings = pycc.create_model(&#39;et&#39;, 
                                            probability_threshold = 0.75, 
                                            cross_validation=False)</code></pre>
<p><strong>Access performance metrics grid</strong></p>
<p>The performance metrics are only displayed here, but not returned.
However, they can be retrieved with the pull function.</p>
<pre class="r"><code>et_clf_results = pycc.pull()
et_clf_results</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p18.png" /></p>
<p>Filtered here according to the lines Mean and SD:</p>
<pre class="r"><code>et_clf_results.loc[[&#39;Mean&#39;, &#39;SD&#39;]]</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p19.png" /></p>
<p>If you want to train multiple models or the same model with different configurations, you can do this with a for-loop. Have a look at this link: <a href="https://pycaret.gitbook.io/docs/get-started/functions/train#train-models-in-a-loop">Train models in a loop</a></p>
</div>
<div id="model-optimization" class="section level2">
<h2>3.5 Model Optimization</h2>
<p>There are several possibilities in PyCaret to further improve the performance of a model. I would like to present these in the following sections.</p>
<div id="tune-the-model" class="section level3">
<h3>3.5.1 Tune the Model</h3>
<p>If you want to tune the hyperparameters of the model, you have to use the tune_model function.</p>
<p>Since n_iter is set to 10 by default, I always like to increase this value as it improves the optimization. However, one must also mention here that the calculation time increases as a result.</p>
<pre class="r"><code>et_clf_tuned = pycc.tune_model(et_clf, n_iter = 50)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p20.png" /></p>
<p><strong>Access performance metrics grid</strong></p>
<p>Again, the performance metrics are only displayed but not returned.
Therefore we use the pull function again:</p>
<pre class="r"><code>et_clf_tuned_results = pycc.pull()
et_clf_tuned_results</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p21.png" /></p>
<pre class="r"><code>print(Color.BOLD +&#39;Comparison of hyperparameters&#39; + Color.END)
print()
print(Color.GREEN + Color.UNDERLINE + &#39;ExtraTreesClassifier:&#39; + Color.END)
print()
print(et_clf)
print()
print(Color.RED + Color.UNDERLINE + &#39;ExtraTreesClassifier with Optimization:&#39; + Color.END)
print()
print(et_clf_tuned)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p22.png" /></p>
<p>You can spend a lot of time optimizing ML algorithms, so there are also some parameters you can set in the tune_model function. These are among others:</p>
<ul>
<li>custom_grid</li>
<li>search_library</li>
<li>search_algorithm</li>
<li>early_stopping</li>
</ul>
<p>You can read about the other parameters here: <a href="https://pycaret.readthedocs.io/en/latest/api/classification.html#pycaret.classification.tune_model">pycaret.classification.tune_model()</a></p>
</div>
<div id="retrieve-the-tuner" class="section level3">
<h3>3.5.2 Retrieve the Tuner</h3>
<p>At this point I would like to go into more detail about one parameter: <em>return_tuner=True</em></p>
<p>So far, we have only received the best model back from the tune_model function after optimizing the hyperparameters.
But sometimes it is necessary to get back the tuner itself. This can be done with the additional argument return_tuner=True.</p>
<pre class="r"><code>et_clf_tuned, et_clf_tuner = pycc.tune_model(et_clf, 
                                             n_iter = 50,
                                             return_tuner=True)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p23.png" /></p>
<p>Here it is:</p>
<pre class="r"><code>et_clf_tuner</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p24.png" /></p>
</div>
<div id="automatically-choose-better" class="section level3">
<h3>3.5.3 Automatically Choose Better</h3>
<p>A tuned model does not always deliver better results. See the performance overview of the two models in the comparison:</p>
<pre class="r"><code>print(Color.BOLD +&#39;Comparison of performance metrics&#39; + Color.END)
print()
print(Color.GREEN + Color.UNDERLINE + &#39;ExtraTreesClassifier:&#39; + Color.END)
print()
print(et_clf_results.loc[[&#39;Mean&#39;]])
print()
print(Color.RED + Color.UNDERLINE + &#39;ExtraTreesClassifier with Optimization:&#39; + Color.END)
print()
print(et_clf_tuned_results.loc[[&#39;Mean&#39;]])</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p25.png" /></p>
<p>To prevent this from accidentally becoming a problem, there is the choose_better parameter. If this is set to True, an improved model is always returned. Even if the optimization of the hyperparameters does not result in an improvement, the original model is returned instead of a worse model as was the case above.</p>
<pre class="r"><code>et_clf_tuned_better, \
et_clf_tuned_better_tuner = pycc.tune_model(et_clf,n_iter = 50, 
                                            return_tuner=True,
                                            choose_better = True)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p26.png" /></p>
<p>Please do not be confused by the performance overview. This shows the metrics for the best model that the tuner has created. If the performance cannot be increased, the worse model <strong>is not</strong> returned by the tuner but <strong>the old model is used</strong> (now also stored under the object et_clf_tuned_better).</p>
<p>The grid of performance indicators can again be assigned to an object by pull:</p>
<pre class="r"><code>et_clf_tuned_better_results = pycc.pull()</code></pre>
<p>If someone does not like this approach, they are welcome to continue with the old model (et_clf).</p>
</div>
<div id="ensemble_models" class="section level3">
<h3>3.5.4 ensemble_models</h3>
<p>Another way to improve the existing model is to create an ensemble. For this we have two options here:</p>
<ul>
<li><a href="https://michael-fuchs-python.netlify.app/2020/03/07/ensemble-modeling-bagging/">Bagging</a></li>
<li><a href="https://michael-fuchs-python.netlify.app/2020/03/26/ensemble-modeling-boosting/">Boosting</a></li>
</ul>
<p>By default, PyCaret always uses 10 estimators for these two methods. However, I want more to be used right away, so I specify this parameter for both methods right away.</p>
<p>I will train new models for both methods and then compare performance.</p>
<p><strong>Method: Bagging:</strong></p>
<pre class="r"><code># Train the bagged Model 
et_clf_bagged = pycc.ensemble_model(et_clf, 
                                    method = &#39;Bagging&#39;, 
                                    fold = 5,
                                    n_estimators = 100)

# Obtaining the performance overview
et_clf_bagged_results = pycc.pull()</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p27.png" /></p>
<p><strong>Method: Boosting:</strong></p>
<pre class="r"><code># Train the boosted Model 
et_clf_boosted = pycc.ensemble_model(et_clf, 
                                     method = &#39;Boosting&#39;, 
                                     fold = 5,
                                     n_estimators = 100)

# Obtaining the performance overview
et_clf_boosted_results = pycc.pull()</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p28.png" /></p>
<p><strong>Overview of the performance metrics:</strong></p>
<pre class="r"><code>print(Color.BOLD +&#39;Comparison of performance metrics&#39; + Color.END)
print()
print(&#39;ExtraTreesClassifier:&#39;)
print()
print(et_clf_results.loc[[&#39;Mean&#39;]])
print()
print(&#39;ExtraTreesClassifier with Optimization:&#39;)
print()
print(et_clf_tuned_results.loc[[&#39;Mean&#39;]])
print()
print(&#39;ExtraTreesClassifier bagged:&#39;)
print()
print(et_clf_bagged_results.loc[[&#39;Mean&#39;]])
print()
print(&#39;ExtraTreesClassifier boosted:&#39;)
print()
print(et_clf_boosted_results.loc[[&#39;Mean&#39;]])</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p29.png" /></p>
<p>Since this is a little hard to read, I always quite like to create the following overview:</p>
<pre class="r"><code>et_clf_results_df = et_clf_results.loc[[&#39;Mean&#39;]]
et_clf_tuned_results_df = et_clf_tuned_results.loc[[&#39;Mean&#39;]]
et_clf_bagged_results_df = et_clf_bagged_results.loc[[&#39;Mean&#39;]]
et_clf_boosted_results_df = et_clf_boosted_results.loc[[&#39;Mean&#39;]]

comparison_df = pd.concat([et_clf_results_df,
                          et_clf_tuned_results_df,
                          et_clf_bagged_results_df,
                          et_clf_boosted_results_df]).reset_index()

comparison_df = comparison_df.drop(&#39;index&#39;, axis=1) 
comparison_df.insert(0, &quot;Model&quot;, [&#39;et_clf&#39;, &#39;et_clf_tuned&#39;, 
                                  &#39;et_clf_bagged&#39;, &#39;et_clf_boosted&#39;])

comparison_df.style.highlight_max(color = &#39;lightgreen&#39;, axis = 0)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p30.png" /></p>
<p>The overview shows us that the et_clf_boosted model performs best.</p>
<p>As with the tune_model function, there is also the <em>choose_better = True</em> parameter here. Use this one if you like.</p>
</div>
<div id="blend_models" class="section level3">
<h3>3.5.5 blend_models</h3>
<p>Another way I would like to introduce to improve performance is to use voting classifiers.</p>
<p>I have also written a post about this topic before see here: <a href="https://michael-fuchs-python.netlify.app/2020/05/05/ensemble-modeling-voting/">Ensemble Modeling - Voting</a></p>
<pre class="r"><code># Training of multiple models
lr_clf_voting = pycc.create_model(&#39;lr&#39;, fold = 5)
dt_clf_voting = pycc.create_model(&#39;dt&#39;, fold = 5)
et_clf_voting = pycc.create_model(&#39;et&#39;, fold = 5)

voting_clf = pycc.blend_models([lr_clf_voting, 
                                dt_clf_voting, 
                                et_clf_voting])

# Obtaining the performance overview
voting_clf_results = pycc.pull()</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p31.png" /></p>
<p><strong>Dynamic input estimators</strong></p>
<p>The list of input estimators can also be created automatically using the compare_models function. We have used this function in chapter 3.2. Here the N best models are used as input list.</p>
<pre class="r"><code># Training of N best models
voting_clf_dynamic = pycc.blend_models(pycc.compare_models(n_select = 3))
# Obtaining the performance overview
voting_clf_dynamic_results = pycc.pull()</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p32.png" /></p>
<p>Here you can see which estimator was finally used:</p>
<pre class="r"><code>voting_clf_dynamic.estimators_</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p33.png" /></p>
<p>Here is a performance overview again. I have left out the models that have not brought any improvement, so that it remains clear.</p>
<pre class="r"><code>et_clf_results_df = et_clf_results.loc[[&#39;Mean&#39;]]
et_clf_boosted_results_df = et_clf_boosted_results.loc[[&#39;Mean&#39;]]
voting_clf_results_df = voting_clf_results.loc[[&#39;Mean&#39;]]
voting_clf_dynamic_results_df = voting_clf_dynamic_results.loc[[&#39;Mean&#39;]]

comparison_df2 = pd.concat([et_clf_results_df,
                            et_clf_boosted_results_df,
                            voting_clf_results_df,
                            voting_clf_dynamic_results_df]).reset_index()

comparison_df2 = comparison_df2.drop(&#39;index&#39;, axis=1) 
comparison_df2.insert(0, &quot;Model&quot;, [&#39;et_clf&#39;,
                                   &#39;et_clf_boosted&#39;,
                                   &#39;voting_clf&#39;, 
                                   &#39;voting_clf_dynamic&#39;])

comparison_df2.style.highlight_max(color = &#39;lightgreen&#39;, axis = 0)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p34.png" /></p>
<p>We have managed to find an even better model!</p>
<p>With blend_models, there are other parameters that could improve performance. These are:</p>
<ul>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/optimize#changing-the-method">Changing the method</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/optimize#changing-the-weights">Changing the weights</a></li>
</ul>
<p>As with the tune_model function, there is also the <em>choose_better = True</em> parameter here. Use this one if you like.</p>
</div>
<div id="stack_models" class="section level3">
<h3>3.5.6 stack_models</h3>
<p>Which function should not be missing under the chapter Model Optimization and is also available in PyCaret is the stack_models function.</p>
<p>I have already written a post about this topic for those who want to learn more about this method: <a href="https://michael-fuchs-python.netlify.app/2020/04/24/ensemble-modeling-stacking/">Ensemble Modeling - Stacking</a></p>
<p>The procedure and syntax is almost identical to the blend_models function.</p>
<pre class="r"><code># Training of multiple models
lr_clf_stacked = pycc.create_model(&#39;lr&#39;, fold = 5)
dt_clf_stacked = pycc.create_model(&#39;dt&#39;, fold = 5)
et_clf_stacked = pycc.create_model(&#39;et&#39;, fold = 5)

stacked_clf = pycc.stack_models([lr_clf_stacked, 
                                dt_clf_stacked, 
                                et_clf_stacked])

# Obtaining the performance overview
stacked_clf_results = pycc.pull()</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p35.png" /></p>
<p><strong>Dynamic input estimators</strong></p>
<p>As with the blend_models function, the list of input estimators can also be created automatically with the compare_models function. Here again the N best models are used as input list.</p>
<pre class="r"><code># Training of N best models
stacked_clf_dynamic = pycc.stack_models(pycc.compare_models(n_select = 3))
# Obtaining the performance overview
stacked_clf_dynamic_results = pycc.pull()</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p36.png" /></p>
<p>And here again the overview of the estimators that were used.</p>
<pre class="r"><code>stacked_clf_dynamic.estimators_</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p37.png" /></p>
<p>Let’s take another look at the performance overviews. Again, I have only listed the relevant ones and omitted the overviews for less performant algorithms.</p>
<pre class="r"><code>et_clf_results_df = et_clf_results.loc[[&#39;Mean&#39;]]
voting_clf_dynamic_results_df = voting_clf_dynamic_results.loc[[&#39;Mean&#39;]]
stacked_clf_results_results_df = stacked_clf_results.loc[[&#39;Mean&#39;]]
stacked_clf_dynamic_results_df = stacked_clf_dynamic_results.loc[[&#39;Mean&#39;]]

comparison_df3 = pd.concat([et_clf_results_df,
                            voting_clf_dynamic_results_df,
                            stacked_clf_results_results_df,
                            stacked_clf_dynamic_results_df]).reset_index()

comparison_df3 = comparison_df3.drop(&#39;index&#39;, axis=1) 
comparison_df3.insert(0, &quot;Model&quot;, [&#39;et_clf&#39;, 
                                   &#39;voting_clf_dynamic&#39;, 
                                   &#39;stacked_clf&#39;,
                                   &#39;stacked_clf_dynamic&#39;])

comparison_df3.style.highlight_max(color = &#39;lightgreen&#39;, axis = 0)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p38.png" /></p>
<p>We were able to improve performance once again.</p>
<p>With stack_models, there are other parameters that could improve performance. These are:</p>
<ul>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/optimize#changing-the-method-1">Changing the method</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/optimize#changing-the-meta-model">Changing the meta-model</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/optimize#restacking">Restacking</a></li>
</ul>
<p>As with the tune_model function, there is also the <em>choose_better = True</em> parameter here. Use this one if you like.</p>
</div>
<div id="further-methods" class="section level3">
<h3>3.5.7 Further Methods</h3>
<p>There are two more methods in PyCaret that I would like to mention briefly here:</p>
<ul>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/optimize#optimize_threshold">optimize_threshold</a></li>
<li><a href="https://pycaret.gitbook.io/docs/get-started/functions/optimize#calibrate_model">calibrate_model</a></li>
</ul>
<p>Please read the documentation and decide for yourself if these methods help you.</p>
</div>
</div>
<div id="model-evaluation-after-training" class="section level2">
<h2>3.6 Model Evaluation after Training</h2>
<p>I have already discussed the evaluation possibilities via PyCaret in chapter 3.3. Here we only looked at how well the performance of the algorithms fit the training part of our data to get an impression of which model might fit best.</p>
<p>In the following I will do this again <strong>with a trained model for the test part</strong>. For this I use the model created in chapter 3.5.6 (stacked_clf_dynamic), because this gave the best performance values during the model optimization.</p>
<pre class="r"><code>type(stacked_clf_dynamic)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p39.png" /></p>
<pre class="r"><code>pycc.plot_model(stacked_clf_dynamic, plot = &#39;auc&#39;, scale = 2)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p40.png" /></p>
<p><strong>Use train data</strong></p>
<p>If you want to know what performance the trained model has on the training data, you can use_train_data = True.
Here is an example:</p>
<pre class="r"><code>pycc.plot_model(stacked_clf_dynamic, plot = &#39;auc&#39;,
                scale = 2,
                use_train_data = True)</code></pre>
<p><strong>Saving image files</strong></p>
<p>If you want to save the output graphics in PyCaret, you have to set the safe parameter to True.
The syntax would look like this:</p>
<pre class="r"><code>pycc.plot_model(stacked_clf_dynamic, plot = &#39;auc&#39;,
                scale = 2,
                save = True)</code></pre>
<p>Here is an overview of possible graphics in PyCaret: <a href="https://pycaret.gitbook.io/docs/get-started/functions/analyze#classification">Examples by module</a></p>
</div>
<div id="model-predictions" class="section level2">
<h2>3.7 Model Predictions</h2>
<p>Now we come to the part where we make predictions with our trained algorithm. Here we use the following model, since it has shown the best validation values:</p>
<pre class="r"><code>type(stacked_clf_dynamic)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p41.png" /></p>
<div id="on-testx" class="section level3">
<h3>3.7.1 On testX</h3>
<p>As a reminder, here is the test part of our data set, which was generated by the Train-Test Split in the setup function (see chapter 3.1).</p>
<pre class="r"><code>testX</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p42.png" /></p>
<pre class="r"><code># Make model predictions on testX
stacked_clf_dynamic_pred = pycc.predict_model(stacked_clf_dynamic)
stacked_clf_dynamic_pred</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p43.png" /></p>
<p>As we can see, three more columns have been added to the testX part:</p>
<ul>
<li>type -&gt; These are the true labels in their original form (not recoded)</li>
<li>Label -&gt; these are the predicted labels (also not recoded)</li>
<li>Score -&gt; These are the score values for the predicted labels</li>
</ul>
<pre class="r"><code># Obtaining the performance overview
stacked_clf_dynamic_pred_results = pycc.pull()
stacked_clf_dynamic_pred_results</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p44.png" /></p>
<p>If we look at the performance values for the prediction, we see that with an accuracy of over 88%, this is even better than that achieved during model training:</p>
<pre class="r"><code>stacked_clf_dynamic_results_df</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p45.png" /></p>
<p>The variation comes from the fact that we used cross-validation in model training for this algorithm. Keep this in mind when reporting results to your supervisor or client.</p>
</div>
<div id="on-unseen-data" class="section level3">
<h3>3.7.2 On unseen data</h3>
<p>Now is the time to make predictions about unseen data. I have created my own data set for this purpose:</p>
<pre class="r"><code>unseen_df = pd.DataFrame(np.array([[165.00,11.45,156.00,8.68,80.25,7.85,143.00,8.25,86.61,6.63], 
                                   [63.48,4.03,77.33,3.65,26.84,2.31,48.82,2.51,22.91,1.7], 
                                   [13.52,1.28,17.88,1.07,15.1,1.05,25.14,1.23,17.81,0.69], 
                                   [20.25,2.35,25.14,1.76,20.17,1.37,27.67,1.41,15.68,1.55], 
                                   [62.49,4.75,69.66,3.99,57.3,4.6,77.6,4.26,60.31,3.86], 
                                   [31.72,2.64,40,1.99,20.36,1.59,32.21,1.62,17.72,1.52], 
                                   [28.66,2.48,33.24,2.02,29.33,2.26,50.64,2.05,35.99,1.85],
                                   [68.15,3.84,72.31,3.42,24.23,1.9,50.26,2.13,29.16,2.05]]),
                         columns=[&#39;huml&#39;, &#39;humw&#39;, &#39;ulnal&#39;, &#39;ulnaw&#39;, &#39;feml&#39;, 
                                  &#39;femw&#39;, &#39;tibl&#39;, &#39;tibw&#39;, &#39;tarl&#39;, &#39;tarw&#39;])


unseen_df</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p46.png" /></p>
<p>Of course, the data set must contain the same number of variables as it did during setup and model training.</p>
<p>If you use the predict_model function on new (unseen) data, you have to pass the parameter <strong>data = ‘name_of_the_new_data’</strong> to the function.
Furthermore I use <strong>raw_score = True</strong> to see which scores are calculated for the respective classes.</p>
<pre class="r"><code># Make model predictions on unseen data
stacked_clf_dynamic_pred_unseen = pycc.predict_model(stacked_clf_dynamic,
                                                     raw_score = True,
                                                     data = unseen_df)

stacked_clf_dynamic_pred_unseen</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p47.png" /></p>
<p>Done! We made our first prediction with the model we created.</p>
<p>Below I show again the effect of the raw_score = True setting. To do this, I selected the newly generated columns:</p>
<pre class="r"><code>stacked_clf_dynamic_pred_unseen.loc[:,&#39;Label&#39; : &#39;Score_W&#39;]</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p48.png" /></p>
<p>The column ‘Label’ shows again the predicted labels.
After that come the score values for the respective classes.</p>
<p>So the model says with a probability of over 98% that the observation from line 1 belongs to the class ‘SW’.</p>
</div>
</div>
<div id="model-finalization" class="section level2">
<h2>3.8 Model Finalization</h2>
<p>Last but not least, we finalize our model.
Here, the final model is trained again on both the trainX part and the testX part.
No more changes are made to the model parameters.</p>
<pre class="r"><code>stacked_clf_dynamic_final = pycc.finalize_model(stacked_clf_dynamic)
stacked_clf_dynamic_final</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p49.png" /></p>
</div>
<div id="saving-the-pipeline-model" class="section level2">
<h2>3.9 Saving the Pipeline &amp; Model</h2>
<p>To be able to use the created pipeline and the trained model in another place, we have to save it as a last step.</p>
<pre class="r"><code>pycc.save_model(stacked_clf_dynamic_final, 
                &#39;stacked_clf_dynamic_final_pipeline&#39;)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p50.png" /></p>
<p><strong>Reload a Pipeline</strong></p>
<p>Let’s review this as well:</p>
<pre class="r"><code>pipeline_reload = pycc.load_model(&#39;stacked_clf_dynamic_final_pipeline&#39;)</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p51.png" /></p>
<p>Ready for new predictions</p>
<pre class="r"><code>unseen_df2 = pd.DataFrame(np.array([[34.07,3.26,41.34,2.62,24.25,2.3,36.35,2.13,24.69,2.12]]),
                          columns=[&#39;huml&#39;, &#39;humw&#39;, &#39;ulnal&#39;, &#39;ulnaw&#39;, &#39;feml&#39;, 
                                   &#39;femw&#39;, &#39;tibl&#39;, &#39;tibw&#39;, &#39;tarl&#39;, &#39;tarw&#39;])


unseen_df2</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p52.png" /></p>
<pre class="r"><code>pipeline_reload_pred_unseen2 = pycc.predict_model(pipeline_reload,
                                                  raw_score = True,
                                                  data = unseen_df2)

pipeline_reload_pred_unseen2</code></pre>
<p><img src="/post/2022-01-01-automl-using-pycaret-classification_files/p133p53.png" /></p>
<p>Works !</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>4 Conclusion¶</h1>
<p>In this post I showed how to use PyCaret to solve a classification problem and demonstrated the power behind this AutoML library.</p>
<p>With PyCaret you have the possibility to quickly and easily run different scenarios in model training and find the best fitting algorithm for your problem.</p>
<p>Thanks also to the PyCaret team for giving me quick feedback on my questions.</p>
<p><strong>Limitations</strong></p>
<p>In this post, I applied very few data pre-processing steps. Scaling or feature engineering, for example, could have increased the performance even more.</p>
</div>
