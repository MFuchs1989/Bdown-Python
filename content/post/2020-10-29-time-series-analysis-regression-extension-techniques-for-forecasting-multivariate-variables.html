---
title: Time Series Analysis - Regression Extension Techniques for Multivariate Time Series
author: Michael Fuchs
date: '2020-10-29'
slug: time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#import-the-libraries-and-the-data">2 Import the libraries and the data</a></li>
<li><a href="#eda">3 EDA</a></li>
<li><a href="#definition-of-required-functions">4 Definition of required functions</a></li>
<li><a href="#stationarity">5 Stationarity</a>
<ul>
<li><a href="#check-for-stationary">5.1 Check for stationary</a></li>
<li><a href="#train-test-split">5.2 Train Test Split</a></li>
<li><a href="#make-data-stationary">5.3 Make data stationary</a></li>
<li><a href="#check-again-for-stationary">5.4 Check again for stationary</a></li>
</ul></li>
<li><a href="#cointegration-test">6 Cointegration Test</a></li>
<li><a href="#regression-extension-techniques-for-forecasting-multivariate-variables">7 Regression Extension Techniques for Forecasting Multivariate Variables</a>
<ul>
<li><a href="#vector-autoregression-var">7.1 Vector Autoregression (VAR)</a>
<ul>
<li><a href="#get-best-ar-terms">7.1.1 Get best AR Terms</a></li>
<li><a href="#fit-var">7.1.2 Fit VAR</a></li>
<li><a href="#inverse-transformation">7.1.3 Inverse Transformation</a></li>
<li><a href="#evaluation-of-var">7.1.4 Evaluation of VAR</a></li>
</ul></li>
<li><a href="#varma">7.2 VARMA</a>
<ul>
<li><a href="#get-best-p-q-and-tr-terms">7.2.1 Get best p, q and tr Terms</a></li>
<li><a href="#fit-varma">7.2.2 Fit VARMA</a></li>
<li><a href="#inverse-transformation-1">7.2.3 Inverse Transformation</a></li>
<li><a href="#evaluation-of-varma">7.2.4 Evaluation of VARMA</a></li>
</ul></li>
<li><a href="#varma-with-auto-arima">7.3 VARMA with Auto Arima</a>
<ul>
<li><a href="#get-best-p-and-q">7.3.1 Get best p and q</a></li>
<li><a href="#fit-varma_2">7.3.2 Fit VARMA_2</a></li>
<li><a href="#inverse-transformation-2">7.3.3 Inverse Transformation</a></li>
<li><a href="#evaluation-of-varma_2">7.3.4 Evaluation of VARMA_2</a></li>
</ul></li>
</ul></li>
<li><a href="#conclusion">8 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>In my last post (<a href="https://michael-fuchs-python.netlify.app/2020/10/27/time-series-analysis-regression-extension-techniques-for-forecasting-univariate-variables/">“Regression Extension Techniques for Univariate Time Series”</a>) I showed how to make time series predictions of single variables. Now we come to the exciting topic of how to do this for multiple target variables at the same time.</p>
<p>For this post the dataset <em>FB</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used. You can download it from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets/Time%20Series%20Analysis">“GitHub Repository”</a>.</p>
</div>
<div id="import-the-libraries-and-the-data" class="section level1">
<h1>2 Import the libraries and the data</h1>
<pre class="r"><code>import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
%matplotlib inline

# Libraries to define the required functions
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.vector_ar.vecm import coint_johansen

from pmdarima.model_selection import train_test_split as time_train_test_split
from sklearn import metrics
from sklearn.model_selection import ParameterGrid

from statsmodels.tsa.api import VAR
from statsmodels.tsa.statespace.varmax import VARMAX
from pmdarima import auto_arima


import warnings
warnings.filterwarnings(&quot;ignore&quot;)</code></pre>
<pre class="r"><code>df = pd.read_csv(&#39;FB.csv&#39;)
df.head()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p1.png" /></p>
</div>
<div id="eda" class="section level1">
<h1>3 EDA</h1>
<pre class="r"><code>for feature in df[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]]:
    df[str(feature)].plot(figsize=(15, 6))
    plt.xlabel(&quot;Date&quot;)
    plt.ylabel(feature)
    plt.title(f&quot;{str(feature)} price of Facebook stocks before stationary&quot;)
    plt.show()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p2.png" /></p>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p3.png" /></p>
<pre class="r"><code>for feature in df[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]]:
    plt.figure(1, figsize=(15,6))
    plt.subplot(211)
    plt.title(f&quot;{str(feature)} Histogram before stationary&quot;)
    df[str(feature)].hist()
    plt.subplot(212)
    df[str(feature)].plot(kind=&#39;kde&#39;)
    plt.title(f&quot;{str(feature)} Kernal Density Estimator before stationary&quot;)
    plt.show()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p4.png" /></p>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p5.png" /></p>
</div>
<div id="definition-of-required-functions" class="section level1">
<h1>4 Definition of required functions</h1>
<pre class="r"><code>def mean_absolute_percentage_error(y_true, y_pred):
    &#39;&#39;&#39;
    Calculate the mean absolute percentage error as a metric for evaluation
    
    Args:
        y_true (float64): Y values for the dependent variable (test part), numpy array of floats 
        y_pred (float64): Predicted values for the dependen variable (test parrt), numpy array of floats
    
    Returns:
        Mean absolute percentage error 
    &#39;&#39;&#39;    
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100</code></pre>
<pre class="r"><code>def timeseries_evaluation_metrics_func(y_true, y_pred):
    &#39;&#39;&#39;
    Calculate the following evaluation metrics:
        - MSE
        - MAE
        - RMSE
        - MAPE
        - R²
    
    Args:
        y_true (float64): Y values for the dependent variable (test part), numpy array of floats 
        y_pred (float64): Predicted values for the dependen variable (test parrt), numpy array of floats
    
    Returns:
        MSE, MAE, RMSE, MAPE and R² 
    &#39;&#39;&#39;    
    #print(&#39;Evaluation metric results: &#39;)
    print(f&#39;MSE is : {metrics.mean_squared_error(y_true, y_pred)}&#39;)
    print(f&#39;MAE is : {metrics.mean_absolute_error(y_true, y_pred)}&#39;)
    print(f&#39;RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}&#39;)
    print(f&#39;MAPE is : {mean_absolute_percentage_error(y_true, y_pred)}&#39;)
    print(f&#39;R2 is : {metrics.r2_score(y_true, y_pred)}&#39;,end=&#39;\n\n&#39;)</code></pre>
<pre class="r"><code>def Augmented_Dickey_Fuller_Test_func(series , column_name):
    &#39;&#39;&#39;
    Calculates statistical values whether the available data are stationary or not 
    
    Args:
        series (float64): Values of the column for which stationarity is to be checked, numpy array of floats 
        column_name (str): Name of the column for which stationarity is to be checked
    
    Returns:
        p-value that indicates whether the data are stationary or not
    &#39;&#39;&#39; 
    print (f&#39;Results of Dickey-Fuller Test for column: {column_name}&#39;)
    dftest = adfuller(series, autolag=&#39;AIC&#39;)
    dfoutput = pd.Series(dftest[0:4], index=[&#39;Test Statistic&#39;,&#39;p-value&#39;,&#39;No Lags Used&#39;,&#39;Number of Observations Used&#39;])
    for key,value in dftest[4].items():
       dfoutput[&#39;Critical Value (%s)&#39;%key] = value
    print (dfoutput)
    if dftest[1] &lt;= 0.05:
        print(&quot;Conclusion:====&gt;&quot;)
        print(&quot;Reject the null hypothesis&quot;)
        print(&quot;Data is stationary&quot;)
    else:
        print(&quot;Conclusion:====&gt;&quot;)
        print(&quot;Fail to reject the null hypothesis&quot;)
        print(&quot;Data is non-stationary&quot;)</code></pre>
<pre class="r"><code>def cointegration_test(df): 
    &#39;&#39;&#39;
    Test if there is a long-run relationship between features
    
    Args:
        dataframe (float64): Values of the columns to be checked, numpy array of floats 
    
    Returns:
        True or False whether a variable has a long-run relationship between other features
    &#39;&#39;&#39; 
    res = coint_johansen(df,-1,5)
    d = {&#39;0.90&#39;:0, &#39;0.95&#39;:1, &#39;0.99&#39;:2}
    traces = res.lr1
    cvts = res.cvt[:, d[str(1-0.05)]]
    def adjust(val, length= 6): 
        return str(val).ljust(length)
    print(&#39;Column Name   &gt;  Test Stat &gt; C(95%)    =&gt;   Signif  \n&#39;, &#39;--&#39;*20)
    for col, trace, cvt in zip(df.columns, traces, cvts):
        print(adjust(col), &#39;&gt; &#39;, adjust(round(trace,2), 9), &quot;&gt;&quot;, adjust(cvt, 8), &#39; =&gt;  &#39; , trace &gt; cvt)</code></pre>
<pre class="r"><code>def inverse_diff(actual_df, pred_df):
    &#39;&#39;&#39;
    Transforms the differentiated values back
    
    Args:
        actual dataframe (float64): Values of the columns, numpy array of floats 
        predicted dataframe (float64): Values of the columns, numpy array of floats 
    
    Returns:
        Dataframe with the predicted values
    &#39;&#39;&#39;
    df_res = pred_df.copy()
    columns = actual_df.columns
    for col in columns: 
        df_res[str(col)+&#39;_1st_inv_diff&#39;] = actual_df[col].iloc[-1] + df_res[str(col)].cumsum()
    return df_res</code></pre>
</div>
<div id="stationarity" class="section level1">
<h1>5 Stationarity</h1>
<div id="check-for-stationary" class="section level2">
<h2>5.1 Check for stationary</h2>
<pre class="r"><code>for name, column in df[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]].iteritems():
    Augmented_Dickey_Fuller_Test_func(df[name],name)
    print(&#39;\n&#39;)</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p6.png" /></p>
</div>
<div id="train-test-split" class="section level2">
<h2>5.2 Train Test Split</h2>
<pre class="r"><code>X = df[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39; ]]

trainX, testX = time_train_test_split(X, test_size=30)</code></pre>
</div>
<div id="make-data-stationary" class="section level2">
<h2>5.3 Make data stationary</h2>
<pre class="r"><code>train_diff = trainX.diff()
train_diff.dropna(inplace = True)</code></pre>
<pre class="r"><code>for name, column in train_diff.iteritems():
    Augmented_Dickey_Fuller_Test_func(train_diff[name],name)
    print(&#39;\n&#39;)</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p7.png" /></p>
</div>
<div id="check-again-for-stationary" class="section level2">
<h2>5.4 Check again for stationary</h2>
<pre class="r"><code>for feature in train_diff:
    train_diff[str(feature)].plot(figsize=(15, 6))
    plt.xlabel(&quot;Date&quot;)
    plt.ylabel(feature)
    plt.title(f&quot;{str(feature)} price of Facebook stocks before stationary&quot;)
    plt.show()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p8.png" /></p>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p9.png" /></p>
<pre class="r"><code>for feature in train_diff:
    plt.figure(1, figsize=(15,6))
    plt.subplot(211)
    plt.title(f&quot;{str(feature)} Histogram after stationary&quot;)
    train_diff[str(feature)].hist()
    plt.subplot(212)
    train_diff[str(feature)].plot(kind=&#39;kde&#39;)
    plt.title(f&quot;{str(feature)} Kernal Density Estimator after stationary&quot;)
    plt.show()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p10.png" /></p>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p11.png" /></p>
</div>
</div>
<div id="cointegration-test" class="section level1">
<h1>6 Cointegration Test</h1>
<p>A cointegration test is the co-movement among underlying variables over the long run. This long-run estimation feature distinguishes it from correlation. Two or more variables are cointegrated if ond only if they share common trends.</p>
<p>In comparison: The Correlation is simply a measure of the degree of mutual association between two or more variables.</p>
<pre class="r"><code>cointegration_test(train_diff)</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p12.png" /></p>
<p>As we can see from the output, there is the presence of a long-run relationship between features.</p>
</div>
<div id="regression-extension-techniques-for-forecasting-multivariate-variables" class="section level1">
<h1>7 Regression Extension Techniques for Forecasting Multivariate Variables</h1>
<div id="vector-autoregression-var" class="section level2">
<h2>7.1 Vector Autoregression (VAR)</h2>
<p>Vector Autoregression (VAR) is a stochastic process model utilized to seize the linear relation among the multiple variables of time-series data. VAR is a bidirectional model, while others are undirectional. In a undirectionla model, a predictor influences the target variable, but not vice versa. In a bidirectional model, variables influence each other.</p>
<div id="get-best-ar-terms" class="section level3">
<h3>7.1.1 Get best AR Terms</h3>
<p>First of all we fit the VAR model wth AR terms between 1 to 9 and choose the best AR component.</p>
<pre class="r"><code>resu = []
df_results_VAR = pd.DataFrame()

for i in [1,2,3,4,5,6,7,8,9]:
    fit_v = VAR(train_diff).fit(i)
    aic = fit_v.aic
    bic = fit_v.bic
    df1 = {&#39;AR_Term&#39;:i, &#39;AIC&#39;: aic, &#39;BIC&#39;: bic}
    df_results_VAR = df_results_VAR.append(df1, ignore_index=True)
    clist = [&#39;AR_Term&#39;,&#39;AIC&#39;,&#39;BIC&#39;]
    df_results_VAR = df_results_VAR[clist] </code></pre>
<pre class="r"><code>df_results_VAR.sort_values(by=[&#39;AIC&#39;, &#39;BIC&#39;], ascending=True)</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p13.png" /></p>
<pre class="r"><code>best_values_VAR = df_results_VAR.sort_values(by=[&#39;AIC&#39;, &#39;BIC&#39;]).head(1)
best_values_VAR</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p14.png" /></p>
<pre class="r"><code>AR_Term_value_VAR = best_values_VAR[&#39;AR_Term&#39;].iloc[0]

print(&quot;AR_Term_value_VAR: &quot;, AR_Term_value_VAR)</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p15.png" /></p>
<p>Autoregressive AR(9) appears to be providing the least AIC.</p>
</div>
<div id="fit-var" class="section level3">
<h3>7.1.2 Fit VAR</h3>
<pre class="r"><code>model = VAR(train_diff)

results = model.fit(int(AR_Term_value_VAR))
display(results.summary())
z = results.forecast(y=train_diff.values, steps=len(testX))
df_pred = pd.DataFrame(z, columns=train_diff.columns)</code></pre>
<pre class="r"><code>df_pred[&quot;new_index&quot;] = range(len(trainX), len(X))
df_pred = df_pred.set_index(&quot;new_index&quot;)</code></pre>
<pre class="r"><code>df_pred.head()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p16.png" /></p>
</div>
<div id="inverse-transformation" class="section level3">
<h3>7.1.3 Inverse Transformation</h3>
<pre class="r"><code>res = inverse_diff(df[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]],df_pred)
res.head()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p17.png" /></p>
</div>
<div id="evaluation-of-var" class="section level3">
<h3>7.1.4 Evaluation of VAR</h3>
<pre class="r"><code>for i in [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39; ]:
    print(f&#39;Evaluation metric for {i}&#39;)
    timeseries_evaluation_metrics_func(testX[str(i)] , res[str(i)+&#39;_1st_inv_diff&#39;])</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p18.png" /></p>
<pre class="r"><code>for i in [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39; ]:
    
    plt.rcParams[&quot;figure.figsize&quot;] = [10,7]
    plt.plot(trainX[str(i)], label=&#39;Train &#39;+str(i))
    plt.plot(testX[str(i)], label=&#39;Test &#39;+str(i))
    plt.plot(res[str(i)+&#39;_1st_inv_diff&#39;], label=&#39;Predicted &#39;+str(i))
    plt.legend(loc=&#39;best&#39;)
    plt.show()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p19.png" /></p>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p20.png" /></p>
</div>
</div>
<div id="varma" class="section level2">
<h2>7.2 VARMA</h2>
<p>A VARMA model is another extension of the ARMA model for a multivariate time-series model that contains a vector autoregressive (VAR) component, as well as the vector moving average (VMA). The method is used for multivariate time-series data deprived of trend and seasonal components.</p>
<p>Let’s define a parameter grid for selecting AR(p), MA(q) and trend (tr).</p>
<div id="get-best-p-q-and-tr-terms" class="section level3">
<h3>7.2.1 Get best p, q and tr Terms</h3>
<pre class="r"><code>param_grid = {&#39;p&#39;: [1,2,3], &#39;q&#39;:[1,2,3], &#39;tr&#39;: [&#39;n&#39;,&#39;c&#39;,&#39;t&#39;,&#39;ct&#39;]}
pg = list(ParameterGrid(param_grid))</code></pre>
<p>In the following I will calculate the rmse for all available variables. Since one must decide at the end for the best rmse value of only one variable, this must not be calculated at this point for all further variables (and/or the syntax necessary for it must be written).</p>
<pre class="r"><code>df_results_VARMA = pd.DataFrame(columns=[&#39;p&#39;, &#39;q&#39;, &#39;tr&#39;,&#39;RMSE open&#39;,&#39;RMSE high&#39;,&#39;RMSE low&#39;,&#39;RMSE close&#39;])

for a,b in enumerate(pg):
    p = b.get(&#39;p&#39;)
    q = b.get(&#39;q&#39;)
    tr = b.get(&#39;tr&#39;)
    model = VARMAX(train_diff, order=(p,q), trend=tr).fit()
    z = model.forecast(y=train_diff[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]].values, steps=len(testX))
    df_pred = pd.DataFrame(z, columns=[ &#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;])
    res = inverse_diff(df[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]],df_pred)
    openrmse = np.sqrt(metrics.mean_squared_error(testX.Open, res.Open_1st_inv_diff))
    highrmse = np.sqrt(metrics.mean_squared_error(testX.High, res.High_1st_inv_diff))
    lowrmse = np.sqrt(metrics.mean_squared_error(testX.Low, res.Low_1st_inv_diff))
    closermse = np.sqrt(metrics.mean_squared_error(testX.Close, res.Close_1st_inv_diff))
    df_results_VARMA = df_results_VARMA.append({&#39;p&#39;: p, &#39;q&#39;: q, &#39;tr&#39;: tr,&#39;RMSE open&#39;: openrmse,
                                                &#39;RMSE high&#39;:highrmse,&#39;RMSE low&#39;:lowrmse,
                                                &#39;RMSE close&#39;:closermse }, ignore_index=True)</code></pre>
<pre class="r"><code>df_results_VARMA.sort_values(by=[&#39;RMSE open&#39;, &#39;RMSE high&#39;, &#39;RMSE low&#39;, &#39;RMSE close&#39;]).head()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p21.png" /></p>
<pre class="r"><code>best_values_VARMA = df_results_VARMA.sort_values(by=[&#39;RMSE open&#39;, &#39;RMSE high&#39;, &#39;RMSE low&#39;, &#39;RMSE close&#39;]).head(1)
best_values_VARMA</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p22.png" /></p>
<pre class="r"><code>p_value_VARMA = best_values_VARMA[&#39;p&#39;].iloc[0]
q_value_VARMA = best_values_VARMA[&#39;q&#39;].iloc[0] 
tr_value_VARMA = best_values_VARMA[&#39;tr&#39;].iloc[0] 

print(&quot;p_value_VARMA: &quot;, p_value_VARMA)
print(&quot;q_value_VARMA: &quot;, q_value_VARMA)
print(&quot;tr_value_VARMA: &quot;, tr_value_VARMA)</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p23.png" /></p>
</div>
<div id="fit-varma" class="section level3">
<h3>7.2.2 Fit VARMA</h3>
<pre class="r"><code>model = VARMAX(train_diff[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]], 
               order=(p_value_VARMA, q_value_VARMA),trends = tr_value_VARMA).fit(disp=False)
result = model.forecast(steps = len(testX))</code></pre>
</div>
<div id="inverse-transformation-1" class="section level3">
<h3>7.2.3 Inverse Transformation</h3>
<pre class="r"><code>res = inverse_diff(df[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]], result)

res.head()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p24.png" /></p>
</div>
<div id="evaluation-of-varma" class="section level3">
<h3>7.2.4 Evaluation of VARMA</h3>
<pre class="r"><code>for i in [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]:
    print(f&#39;Evaluation metric for {i}&#39;)
    timeseries_evaluation_metrics_func(testX[str(i)] , res[str(i)+&#39;_1st_inv_diff&#39;])</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p25.png" /></p>
<pre class="r"><code>for i in [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]:
    
    plt.rcParams[&quot;figure.figsize&quot;] = [10,7]
    plt.plot(trainX[str(i)], label=&#39;Train &#39;+str(i))
    plt.plot(testX[str(i)], label=&#39;Test &#39;+str(i))
    plt.plot(res[str(i)+&#39;_1st_inv_diff&#39;], label=&#39;Predicted &#39;+str(i))
    plt.legend(loc=&#39;best&#39;)
    plt.show()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p26.png" /></p>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p27.png" /></p>
</div>
</div>
<div id="varma-with-auto-arima" class="section level2">
<h2>7.3 VARMA with Auto Arima</h2>
<p>We can also use the auto_arima function from the pmdarima librarie to determine p and q.</p>
<div id="get-best-p-and-q" class="section level3">
<h3>7.3.1 Get best p and q</h3>
<pre class="r"><code>pq = []
for name, column in train_diff[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]].iteritems():
    print(f&#39;Searching order of p and q for : {name}&#39;)
    stepwise_model = auto_arima(train_diff[name],start_p=1, start_q=1,max_p=7, max_q=7, seasonal=False,
        trace=True,error_action=&#39;ignore&#39;,suppress_warnings=True, stepwise=True,maxiter=1000)
    parameter = stepwise_model.get_params().get(&#39;order&#39;)
    print(f&#39;optimal order for:{name} is: {parameter} \n\n&#39;)
    pq.append(stepwise_model.get_params().get(&#39;order&#39;))</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p28.png" /></p>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p29.png" /></p>
<pre class="r"><code>pq</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p30.png" /></p>
<pre class="r"><code>df_results_VARMA_2 = pd.DataFrame(columns=[&#39;p&#39;, &#39;q&#39;,&#39;RMSE Open&#39;,&#39;RMSE High&#39;,&#39;RMSE Low&#39;,&#39;RMSE Close&#39;])

for i in pq:
    if i[0]== 0 and i[2] ==0:
        pass
    else:
        print(f&#39; Running for {i}&#39;)
        model = VARMAX(train_diff[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]], order=(i[0],i[2])).fit(disp=False)
        result = model.forecast(steps = len(testX))
        inv_res = inverse_diff(df[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]], result)
        Opensrmse = np.sqrt(metrics.mean_squared_error(testX[&#39;Open&#39;], inv_res.Open_1st_inv_diff))
        Highrmse = np.sqrt(metrics.mean_squared_error(testX[&#39;High&#39;], inv_res.High_1st_inv_diff))
        Lowrmse = np.sqrt(metrics.mean_squared_error(testX[&#39;Low&#39;], inv_res.Low_1st_inv_diff))
        Closermse = np.sqrt(metrics.mean_squared_error(testX[&#39;Close&#39;], inv_res.Close_1st_inv_diff))
        df_results_VARMA_2 = df_results_VARMA_2.append({&#39;p&#39;: i[0], &#39;q&#39;: i[2], &#39;RMSE Open&#39;:Opensrmse,
                                                        &#39;RMSE High&#39;:Highrmse,&#39;RMSE Low&#39;:Lowrmse,
                                                        &#39;RMSE Close&#39;:Closermse }, ignore_index=True)</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p31.png" /></p>
<pre class="r"><code>df_results_VARMA_2.sort_values(by=[&#39;RMSE Open&#39;, &#39;RMSE High&#39;, &#39;RMSE Low&#39;, &#39;RMSE Close&#39;])</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p32.png" /></p>
<pre class="r"><code>best_values_VAR_2 = df_results_VARMA_2.sort_values(by=[&#39;RMSE Open&#39;, &#39;RMSE High&#39;, 
                                                       &#39;RMSE Low&#39;, &#39;RMSE Close&#39;]).head(1)
best_values_VAR_2</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p33.png" /></p>
<pre class="r"><code>p_value_VARMA_2 = best_values_VAR_2[&#39;p&#39;].iloc[0]
q_value_VARMA_2 = best_values_VAR_2[&#39;q&#39;].iloc[0] 

print(&quot;p_value_VARMA_2: &quot;, p_value_VARMA_2)
print(&quot;q_value_VARMA_2: &quot;, q_value_VARMA_2)</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p34.png" /></p>
</div>
<div id="fit-varma_2" class="section level3">
<h3>7.3.2 Fit VARMA_2</h3>
<pre class="r"><code>model = VARMAX(train_diff[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]], 
               order=(int(p_value_VARMA_2),int(q_value_VARMA_2))).fit(disp=False)
result = model.forecast(steps = len(testX))</code></pre>
</div>
<div id="inverse-transformation-2" class="section level3">
<h3>7.3.3 Inverse Transformation</h3>
<pre class="r"><code>res = inverse_diff(df[[&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39; ]],result)
res.head()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p35.png" /></p>
</div>
<div id="evaluation-of-varma_2" class="section level3">
<h3>7.3.4 Evaluation of VARMA_2</h3>
<pre class="r"><code>for i in [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39;]:
    print(f&#39;Evaluation metric for {i}&#39;)
    timeseries_evaluation_metrics_func(testX[str(i)] , res[str(i)+&#39;_1st_inv_diff&#39;])</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p36.png" /></p>
<pre class="r"><code>for i in [&#39;Open&#39;, &#39;High&#39;, &#39;Low&#39;, &#39;Close&#39; ]:
    
    plt.rcParams[&quot;figure.figsize&quot;] = [10,7]
    plt.plot(trainX[str(i)], label=&#39;Train &#39;+str(i))
    plt.plot(testX[str(i)], label=&#39;Test &#39;+str(i))
    plt.plot(res[str(i)+&#39;_1st_inv_diff&#39;], label=&#39;Predicted &#39;+str(i))
    plt.legend(loc=&#39;best&#39;)
    plt.show()</code></pre>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p37.png" /></p>
<p><img src="/post/2020-10-29-time-series-analysis-regression-extension-techniques-for-forecasting-multivariate-variables_files/p97p38.png" /></p>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>8 Conclusion</h1>
<p>Wow another great chapter created!
In this post about time series prediction of multiple target variables, I introduced the VAR and VARMA algorithms.</p>
<p><strong>References</strong></p>
<p>Christ, M., Braun, N., Neuffer, J., &amp; Kempa-Liehr, A. W. (2018). Time series feature extraction on basis of scalable hypothesis tests (tsfresh–a python package). Neurocomputing, 307, 72-77.</p>
<p>Faouzi, J., &amp; Janati, H. (2020). pyts: A Python Package for Time Series Classification. Journal of Machine Learning Research, 21(46), 1-6.</p>
<p>McKinney, W., Perktold, J., &amp; Seabold, S. (2011). Time series analysis in Python with statsmodels. Jarrodmillman Com, 96-102.</p>
<p>Pal, A., &amp; Prakash, P. K. S. (2017). Practical Time Series Analysis: Master Time Series Data Processing, Visualization, and Modeling using Python. Packt Publishing Ltd.</p>
<p>Roberts, W., Williams, G. P., Jackson, E., Nelson, E. J., &amp; Ames, D. P. (2018). Hydrostats: A Python package for characterizing errors between observed and predicted time series. Hydrology, 5(4), 66.</p>
<p>Vishwas, B. V., &amp; Patel, A. (2020). Hands-on Time Series Analysis with Python.</p>
</div>
