---
title: Bayesian Gaussian Mixture Models
author: Michael Fuchs
date: '2020-06-26'
slug: bayesian-gaussian-mixture-models
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#loading-the-libraries">2 Loading the libraries</a></li>
<li><a href="#generating-some-test-data">3 Generating some test data</a></li>
<li><a href="#bayesian-gaussian-mixture-models-in-action">4 Bayesian Gaussian Mixture Models in action</a></li>
<li><a href="#conclusion">5 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>In my last post I reported on <a href="https://michael-fuchs-python.netlify.app/2020/06/24/gaussian-mixture-models/">“Gaussian Mixture Models”</a>.
Now we come to an kind of extension of GMM the Bayesian Gaussian Mixture Models.
As we have seen at <a href="https://michael-fuchs-python.netlify.app/2020/06/24/gaussian-mixture-models/">“GMM”</a>, we could either only infer the number of clusters by eye or by comparing the theoretical information criterions <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">“AIC”</a> and <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">“BIC”</a> for different k.</p>
<p>Rather than manually search for the optimal number of k, you can use the Bayesian Gaussian Mixture Model, which is capable of giving weights equal (or close) to zero to unnecessary cluster. Set the parameter <em>n_components</em> to a value that you have good reason to believe is greater than the optimal number of clusters, and the algorithm will eliminate the unnecessary cluster automatically.</p>
</div>
<div id="loading-the-libraries" class="section level1">
<h1>2 Loading the libraries</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

# For generating some data
from sklearn.datasets import make_blobs

from matplotlib import pyplot as plt

from sklearn.mixture import BayesianGaussianMixture</code></pre>
</div>
<div id="generating-some-test-data" class="section level1">
<h1>3 Generating some test data</h1>
<p>For the following example, I will generate some sample data.</p>
<pre class="r"><code>X, y = make_blobs(n_samples=350, centers=4, cluster_std=0.60)
plt.scatter(X[:, 0], X[:, 1], cmap=&#39;viridis&#39;)</code></pre>
<p><img src="/post/2020-06-26-bayesian-gaussian-mixture-models_files/p53p1.png" /></p>
</div>
<div id="bayesian-gaussian-mixture-models-in-action" class="section level1">
<h1>4 Bayesian Gaussian Mixture Models in action</h1>
<pre class="r"><code>bay_gmm = BayesianGaussianMixture(n_components=10, n_init=10)

bay_gmm.fit(X)</code></pre>
<p><img src="/post/2020-06-26-bayesian-gaussian-mixture-models_files/p53p2.png" /></p>
<p>With the following syntax we’ll receive the calculated weights:</p>
<pre class="r"><code>bay_gmm_weights = bay_gmm.weights_
np.round(bay_gmm_weights, 2)</code></pre>
<p><img src="/post/2020-06-26-bayesian-gaussian-mixture-models_files/p53p3.png" /></p>
<p>As we can see three clusters have been identified.
This makes sense after looking at the scatter plot, even though we specified to generate 4 clusters when creating the data.
But two clusters are so close together that they were correctly identified as one cluster.</p>
<pre class="r"><code>n_clusters_ = (np.round(bay_gmm_weights, 2) &gt; 0).sum()
print(&#39;Estimated number of clusters: &#39; + str(n_clusters_))</code></pre>
<p><img src="/post/2020-06-26-bayesian-gaussian-mixture-models_files/p53p4.png" /></p>
<p>Let’s do the predictions:</p>
<pre class="r"><code>y_pred  = bay_gmm.predict(X)</code></pre>
<pre class="r"><code>plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=&quot;viridis&quot;)
plt.xlabel(&quot;Feature 1&quot;)
plt.ylabel(&quot;Feature 2&quot;)</code></pre>
<p><img src="/post/2020-06-26-bayesian-gaussian-mixture-models_files/p53p5.png" /></p>
<p>As with <a href="https://michael-fuchs-python.netlify.app/2020/06/24/gaussian-mixture-models/">“Gaussian Mixture Models”</a>, we have a ‘predict_proba’ function here.</p>
<pre class="r"><code>props = bay_gmm.predict_proba(X)
props = props.round(3)
props</code></pre>
<p><img src="/post/2020-06-26-bayesian-gaussian-mixture-models_files/p53p6.png" /></p>
<pre class="r"><code>size = 50 * props.max(1) ** 2  # square emphasizes differences
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=&#39;viridis&#39;, s=size)</code></pre>
<p><img src="/post/2020-06-26-bayesian-gaussian-mixture-models_files/p53p7.png" /></p>
<p>In this graphic for the visualization of the probability predictions, we can also see that some points, that have received a lower probability, are shown a little smaller in the graphic.</p>
</div>
<div id="conclusion" class="section level1">
<h1>5 Conclusion</h1>
<p>As a supplement to the post <a href="https://michael-fuchs-python.netlify.app/2020/06/24/gaussian-mixture-models/">“Gaussian Mixture Models”</a>, I have explained in this post how a reasonable number of clusters can be determined with the help of Bayesian Gaussian Mixture Models and how this algorithm can be used.</p>
</div>
