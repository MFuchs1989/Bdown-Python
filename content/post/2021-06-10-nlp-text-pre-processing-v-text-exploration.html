---
title: NLP - Text Pre-Processing V (Text Exploration)
author: Michael Fuchs
date: '2021-06-10'
slug: nlp-text-pre-processing-v-text-exploration
categories: []
tags: []
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#import-the-libraries-and-the-data">2 Import the Libraries and the Data</a></li>
<li><a href="#definition-of-required-functions">3 Definition of required Functions</a></li>
<li><a href="#text-pre-processing">4 Text Pre-Processing</a>
<ul>
<li><a href="#text-cleaning">4.1 (Text Cleaning)</a></li>
<li><a href="#tokenization">4.2 (Tokenization)</a></li>
<li><a href="#stop-words">4.3 (Stop Words)</a></li>
<li><a href="#digression-pos-ner">4.4 (Digression: POS &amp; NER)</a></li>
<li><a href="#normalization">4.5 (Normalization)</a></li>
<li><a href="#removing-single-characters">4.6 (Removing Single Characters)</a></li>
<li><a href="#text-exploration">4.7 Text Exploration</a>
<ul>
<li><a href="#descriptive-statistics">4.7.1 Descriptive Statistics</a>
<ul>
<li><a href="#most-common-words">4.7.1.1 Most common Words</a></li>
<li><a href="#least-common-words">4.7.1.2 Least common Words</a></li>
</ul></li>
<li><a href="#text-visualization">4.7.2 Text Visualization</a>
<ul>
<li><a href="#via-bar-charts">4.7.2.1 via Bar Charts</a></li>
<li><a href="#via-word-clouds">4.7.2.2 via Word Clouds</a></li>
</ul></li>
<li><a href="#application-to-the-example-string">4.7.3 <strong>Application</strong> to the Example String</a>
<ul>
<li><a href="#most-common-words-1">4.7.3.1 Most common Words</a></li>
<li><a href="#least-common-words-1">4.7.3.2 Least common Words</a></li>
</ul></li>
<li><a href="#application-to-the-dataframe">4.7.4 <strong>Application</strong> to the DataFrame</a>
<ul>
<li><a href="#to-the-whole-df">4.7.4.1 to the whole DF</a>
<ul>
<li><a href="#most-common-words-2">4.7.4.1.1 Most common Words</a></li>
<li><a href="#least-common-words-2">4.7.4.1.2 Least common Words</a></li>
</ul></li>
<li><a href="#divided-by-rating">4.7.4.2 divided by Rating</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#conclusion">5 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>Now that we have completed some pre-processing steps, I always like to start text exploration and visualization at this point.</p>
<p>For this publication the processed dataset <em>Amazon Unlocked Mobile</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used as well as the created Example String. You can download both files from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets/NLP/Text%20Pre-Processing%20V%20(Text%20Exploration)">“GitHub Repository”</a>.</p>
</div>
<div id="import-the-libraries-and-the-data" class="section level1">
<h1>2 Import the Libraries and the Data</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

import pickle as pk

import warnings
warnings.filterwarnings(&quot;ignore&quot;)


from bs4 import BeautifulSoup
import unicodedata
import re

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize

from nltk.corpus import stopwords


from nltk.corpus import wordnet
from nltk import pos_tag
from nltk import ne_chunk

from nltk.stem.porter import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

from nltk.probability import FreqDist
import matplotlib.pyplot as plt
from wordcloud import WordCloud</code></pre>
<pre class="r"><code>pd.set_option(&#39;display.max_colwidth&#39;, 30)</code></pre>
<pre class="r"><code>df = pd.read_csv(&#39;Amazon_Unlocked_Mobile_small_Part_IV.csv&#39;)
df.head(3).T</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p1.png" /></p>
<pre class="r"><code>df[&#39;Reviews_cleaned_wo_single_char&#39;] = df[&#39;Reviews_cleaned_wo_single_char&#39;].astype(str)</code></pre>
<pre class="r"><code>clean_text_wo_single_char = pk.load(open(&quot;clean_text_wo_single_char.pkl&quot;,&#39;rb&#39;))
clean_text_wo_single_char</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p2.png" /></p>
</div>
<div id="definition-of-required-functions" class="section level1">
<h1>3 Definition of required Functions</h1>
<p>All functions are summarized here. I will show them again where they are used during this post if they are new and have not been explained yet.</p>
<pre class="r"><code>def token_and_unique_word_count_func(text):
    &#39;&#39;&#39;
    Outputs the number of words and unique words
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Uses the FreqDist function to count unique words
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Prints:
        Number of existing tokens and number of unique words
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    fdist = FreqDist(words) 
    
    print(&#39;Number of tokens: &#39; + str(len(words))) 
    print(&#39;Number of unique words: &#39; + str(len(fdist)))</code></pre>
<pre class="r"><code>def most_common_word_func(text, n_words=25):
    &#39;&#39;&#39;
    Returns a DataFrame with the most commonly used words from a text with their frequencies
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Uses the FreqDist function to determine the word frequency
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        A DataFrame with the most commonly occurring words (by default = 25) with their frequencies
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    fdist = FreqDist(words) 
    
    n_words = n_words
    
    df_fdist = pd.DataFrame({&#39;Word&#39;: fdist.keys(),
                             &#39;Frequency&#39;: fdist.values()})
    df_fdist = df_fdist.sort_values(by=&#39;Frequency&#39;, ascending=False).head(n_words)
    
    return df_fdist</code></pre>
<pre class="r"><code>def least_common_word_func(text, n_words=25):
    &#39;&#39;&#39;
    Returns a DataFrame with the least commonly used words from a text with their frequencies
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Uses the FreqDist function to determine the word frequency
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        A DataFrame with the least commonly occurring words (by default = 25) with their frequencies
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    fdist = FreqDist(words) 
    
    n_words = n_words
    
    df_fdist = pd.DataFrame({&#39;Word&#39;: fdist.keys(),
                             &#39;Frequency&#39;: fdist.values()})
    df_fdist = df_fdist.sort_values(by=&#39;Frequency&#39;, ascending=False).tail(n_words)
    
    return df_fdist</code></pre>
</div>
<div id="text-pre-processing" class="section level1">
<h1>4 Text Pre-Processing</h1>
<div id="text-cleaning" class="section level2">
<h2>4.1 (Text Cleaning)</h2>
<p>I have already described this part in an earlier post. See here: <a href="https://michael-fuchs-python.netlify.app/2021/05/22/nlp-text-pre-processing-i-text-cleaning/#text-cleaning">Text Cleaning</a></p>
</div>
<div id="tokenization" class="section level2">
<h2>4.2 (Tokenization)</h2>
<p>I have already described this part in an earlier post. See here: <a href="https://michael-fuchs-python.netlify.app/2021/05/25/nlp-text-pre-processing-ii-tokenization-and-stop-words/#tokenization">Text Pre-Processing II-Tokenization</a></p>
</div>
<div id="stop-words" class="section level2">
<h2>4.3 (Stop Words)</h2>
<p>I have already described this part in an earlier post. See here: <a href="https://michael-fuchs-python.netlify.app/2021/05/25/nlp-text-pre-processing-ii-tokenization-and-stop-words/#stop-words">Text Pre-Processing II-Stop Words</a></p>
</div>
<div id="digression-pos-ner" class="section level2">
<h2>4.4 (Digression: POS &amp; NER)</h2>
<p>I have already described this part in an earlier post. See here: <a href="https://michael-fuchs-python.netlify.app/2021/05/31/nlp-text-pre-processing-iii-pos-ner-and-normalization/#digression-pos-ner">Text Pre-Processing III-POS &amp; NER</a></p>
</div>
<div id="normalization" class="section level2">
<h2>4.5 (Normalization)</h2>
<p>I have already described this part in an earlier post. See here: <a href="https://michael-fuchs-python.netlify.app/2021/05/31/nlp-text-pre-processing-iii-pos-ner-and-normalization/#normalization">Text Pre-Processing III-Normalization</a></p>
</div>
<div id="removing-single-characters" class="section level2">
<h2>4.6 (Removing Single Characters)</h2>
<p>I have already described this part in the previous post. See here: <a href="https://michael-fuchs-python.netlify.app/2021/06/05/nlp-text-pre-processing-iv-single-character-removal/#removing-single-characters">Text Pre-Processing IV-Removing Single Characters</a></p>
</div>
<div id="text-exploration" class="section level2">
<h2>4.7 Text Exploration</h2>
<div id="descriptive-statistics" class="section level3">
<h3>4.7.1 Descriptive Statistics</h3>
<p>For better readability, I have added punctuation to the following example sentence. At this point in the text preprocessing, these would no longer be present, nor would stop words or other words with little or no information content.</p>
<p>But that doesn’t matter. You can use this analysis in different places, you just have to keep in mind how clean your text already is and whether punctuation marks or similar are counted.</p>
<pre class="r"><code>text_for_exploration = \
&quot;To begin to toboggan first buy a toboggan, but do not buy too big a toboggan. \
Too big a toboggan is too big a toboggan to buy to begin to toboggan.&quot;
text_for_exploration</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p3.png" /></p>
<pre class="r"><code>def token_and_unique_word_count_func(text):
    &#39;&#39;&#39;
    Outputs the number of words and unique words
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Uses the FreqDist function to count unique words
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Prints:
        Number of existing tokens and number of unique words
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    fdist = FreqDist(words) 
    
    print(&#39;Number of tokens: &#39; + str(len(words))) 
    print(&#39;Number of unique words: &#39; + str(len(fdist)))</code></pre>
<pre class="r"><code>token_and_unique_word_count_func(text_for_exploration)</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p4.png" /></p>
<div id="most-common-words" class="section level4">
<h4>4.7.1.1 Most common Words</h4>
<pre class="r"><code>def most_common_word_func(text, n_words=25):
    &#39;&#39;&#39;
    Returns a DataFrame with the most commonly used words from a text with their frequencies
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Uses the FreqDist function to determine the word frequency
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        A DataFrame with the most commonly occurring words (by default = 25) with their frequencies
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    fdist = FreqDist(words) 
    
    n_words = n_words
    
    df_fdist = pd.DataFrame({&#39;Word&#39;: fdist.keys(),
                             &#39;Frequency&#39;: fdist.values()})
    df_fdist = df_fdist.sort_values(by=&#39;Frequency&#39;, ascending=False).head(n_words)
    
    return df_fdist</code></pre>
<pre class="r"><code>most_common_word_func(text_for_exploration)</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p5.png" /></p>
<pre class="r"><code>df_most_common_words_10 = most_common_word_func(text_for_exploration, n_words=10)
df_most_common_words_10</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p6.png" /></p>
</div>
<div id="least-common-words" class="section level4">
<h4>4.7.1.2 Least common Words</h4>
<pre class="r"><code>def least_common_word_func(text, n_words=25):
    &#39;&#39;&#39;
    Returns a DataFrame with the least commonly used words from a text with their frequencies
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Uses the FreqDist function to determine the word frequency
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        A DataFrame with the least commonly occurring words (by default = 25) with their frequencies
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    fdist = FreqDist(words) 
    
    n_words = n_words
    
    df_fdist = pd.DataFrame({&#39;Word&#39;: fdist.keys(),
                             &#39;Frequency&#39;: fdist.values()})
    df_fdist = df_fdist.sort_values(by=&#39;Frequency&#39;, ascending=False).tail(n_words)
    
    return df_fdist</code></pre>
<pre class="r"><code>least_common_word_func(text_for_exploration, 3)</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p7.png" /></p>
</div>
</div>
<div id="text-visualization" class="section level3">
<h3>4.7.2 Text Visualization</h3>
<p>Note: I apply the visualization only once to the most common words for now.</p>
<div id="via-bar-charts" class="section level4">
<h4>4.7.2.1 via Bar Charts</h4>
<pre class="r"><code>plt.figure(figsize=(11,7))
plt.bar(df_most_common_words_10[&#39;Word&#39;], 
        df_most_common_words_10[&#39;Frequency&#39;])

plt.xlabel(&#39;Most common Words&#39;)
plt.ylabel(&quot;Frequency&quot;)
plt.title(&quot;Frequency distribution of the 10 most common words&quot;)

plt.show()</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p8.png" /></p>
</div>
<div id="via-word-clouds" class="section level4">
<h4>4.7.2.2 via Word Clouds</h4>
<p>With the WordCloud function, you can also have the most frequently occurring words displayed visually. The advantage is that by default all stop words or irrelevant characters are removed from the display.
The parameters that can still be set can be read <a href="https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html">here</a>.</p>
<pre class="r"><code>wordcloud = WordCloud(width = 800, height = 800,
                background_color =&#39;white&#39;,
                min_font_size = 10).generate(text_for_exploration)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;)
plt.axis(&quot;off&quot;)
  
plt.show()</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p9.png" /></p>
</div>
</div>
<div id="application-to-the-example-string" class="section level3">
<h3>4.7.3 <strong>Application</strong> to the Example String</h3>
<pre class="r"><code>clean_text_wo_single_char</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p10.png" /></p>
<pre class="r"><code>token_and_unique_word_count_func(clean_text_wo_single_char)</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p11.png" /></p>
<div id="most-common-words-1" class="section level4">
<h4>4.7.3.1 Most common Words</h4>
<pre class="r"><code>df_most_common_words = most_common_word_func(clean_text_wo_single_char)

df_most_common_words.head(10)</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p12.png" /></p>
<pre class="r"><code>plt.figure(figsize=(11,7))
plt.bar(df_most_common_words[&#39;Word&#39;], 
        df_most_common_words[&#39;Frequency&#39;])

plt.xticks(rotation = 45)

plt.xlabel(&#39;Most common Words&#39;)
plt.ylabel(&quot;Frequency&quot;)
plt.title(&quot;Frequency distribution of the 25 most common words&quot;)

plt.show()</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p13.png" /></p>
</div>
<div id="least-common-words-1" class="section level4">
<h4>4.7.3.2 Least common Words</h4>
<pre class="r"><code>df_least_common_words = least_common_word_func(clean_text_wo_single_char, n_words=10)

df_least_common_words</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p14.png" /></p>
<pre class="r"><code>plt.figure(figsize=(11,7))
plt.bar(df_least_common_words[&#39;Word&#39;], 
        df_least_common_words[&#39;Frequency&#39;])

plt.xticks(rotation = 45)

plt.xlabel(&#39;Least common Words&#39;)
plt.ylabel(&quot;Frequency&quot;)
plt.title(&quot;Frequency distribution of the 10 least common words&quot;)

plt.show()</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p15.png" /></p>
</div>
</div>
<div id="application-to-the-dataframe" class="section level3">
<h3>4.7.4 <strong>Application</strong> to the DataFrame</h3>
<div id="to-the-whole-df" class="section level4">
<h4>4.7.4.1 to the whole DF</h4>
<p>As mentioned at the end of the last chapter (<a href="https://michael-fuchs-python.netlify.app/2021/06/05/nlp-text-pre-processing-iv-single-character-removal/#application-to-the-dataframe">Removing Single Characters</a>), I will continue to work with the ‘Reviews_cleaned_wo_single_char’ column. Here we have removed only characters with a length of 1 from the text.</p>
<p>In order for me to apply the functions written for this chapter, I first need to create a text corpus of all the rows from the ‘Reviews_cleaned_wo_single_char’ column.</p>
<pre class="r"><code>text_corpus = df[&#39;Reviews_cleaned_wo_single_char&#39;].str.cat(sep=&#39; &#39;)

text_corpus</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p16.png" /></p>
<pre class="r"><code>token_and_unique_word_count_func(text_corpus)</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p17.png" /></p>
<div id="most-common-words-2" class="section level5">
<h5>4.7.4.1.1 Most common Words</h5>
<pre class="r"><code>df_most_common_words_text_corpus = most_common_word_func(text_corpus)

df_most_common_words_text_corpus.head(10)</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p18.png" /></p>
<pre class="r"><code>plt.figure(figsize=(11,7))
plt.bar(df_most_common_words_text_corpus[&#39;Word&#39;], 
        df_most_common_words_text_corpus[&#39;Frequency&#39;])

plt.xticks(rotation = 45)

plt.xlabel(&#39;Most common Words&#39;)
plt.ylabel(&quot;Frequency&quot;)
plt.title(&quot;Frequency distribution of the 25 most common words&quot;)

plt.show()</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p19.png" /></p>
<p>As we can see, the word ‘phone’ is by far the most common.</p>
<p>However, this approach is very one-sided if one considers that the comments refer to different ratings. So let’s take a closer look at them in the next step.</p>
</div>
<div id="least-common-words-2" class="section level5">
<h5>4.7.4.1.2 Least common Words</h5>
<pre class="r"><code>df_least_common_words_text_corpus = least_common_word_func(text_corpus, n_words=10)

df_least_common_words_text_corpus</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p20.png" /></p>
<pre class="r"><code>plt.figure(figsize=(11,7))
plt.bar(df_least_common_words_text_corpus[&#39;Word&#39;], 
        df_least_common_words_text_corpus[&#39;Frequency&#39;])

plt.xticks(rotation = 45)

plt.xlabel(&#39;Least common Words&#39;)
plt.ylabel(&quot;Frequency&quot;)
plt.title(&quot;Frequency distribution of the 10 least common words&quot;)

plt.show()</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p21.png" /></p>
</div>
</div>
<div id="divided-by-rating" class="section level4">
<h4>4.7.4.2 divided by Rating</h4>
<p>As we can see from the output below, users were given the option to give 5 different ratings. The 1 stands for a bad rating and the 5 for a very good one.</p>
<pre class="r"><code>df[&#39;Rating&#39;].value_counts()</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p22.png" /></p>
<p>For further analysis, I will assign corresponding lables to the different ratings:</p>
<ul>
<li>Rating 1-2: negative</li>
<li>Rating 3: neutral</li>
<li>Rating 4-5: positive</li>
</ul>
<p>I can do this using the following function:</p>
<pre class="r"><code>def label_func(rating):
    if rating &lt;= 2:
        return &#39;negative&#39;
    if rating &gt;= 4:
        return &#39;positive&#39;
    else:
        return &#39;neutral&#39;

df[&#39;Label&#39;] = df[&#39;Rating&#39;].apply(label_func)    </code></pre>
<p>I personally prefer to have the ‘Rating’ and ‘Label’ columns together. So far, the ‘Label’ column is at the end of the record because it was just newly created. However, the order can be changed with the following command.</p>
<p>Do this column reordering only once, otherwise more and more columns will be put in first place.</p>
<pre class="r"><code>cols = list(df.columns)
cols = [cols[-1]] + cols[:-1]
df = df[cols]</code></pre>
<p>As you can see from the partial output shown below, the labels were assigned correctly.</p>
<pre class="r"><code>df.T</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p23.png" /></p>
<p>Now I divide the data set according to their labels (‘positive’, ‘neutral’ and ‘negative’).</p>
<pre class="r"><code>positive_review = df[(df[&quot;Label&quot;] == &#39;positive&#39;)][&#39;Reviews_cleaned_wo_single_char&#39;]
neutral_review = df[(df[&quot;Label&quot;] == &#39;neutral&#39;)][&#39;Reviews_cleaned_wo_single_char&#39;]
negative_review = df[(df[&quot;Label&quot;] == &#39;negative&#39;)][&#39;Reviews_cleaned_wo_single_char&#39;]</code></pre>
<p>According to the division, I create a separate text corpus for each.</p>
<pre class="r"><code>text_corpus_positive_review = positive_review.str.cat(sep=&#39; &#39;)
text_corpus_neutral_review = neutral_review.str.cat(sep=&#39; &#39;)
text_corpus_negative_review = negative_review.str.cat(sep=&#39; &#39;)</code></pre>
<p>Then I can use the most_common_word function again and save the results in a separate data set.</p>
<pre class="r"><code>df_most_common_words_text_corpus_positive_review = most_common_word_func(text_corpus_positive_review)
df_most_common_words_text_corpus_neutral_review = most_common_word_func(text_corpus_neutral_review)
df_most_common_words_text_corpus_negative_review = most_common_word_func(text_corpus_negative_review)</code></pre>
<p>Now I can use a for-loop to visually display the 25 most frequently occurring words in each of the partial data sets.</p>
<pre class="r"><code>splited_data = [df_most_common_words_text_corpus_positive_review,
                df_most_common_words_text_corpus_neutral_review,
                df_most_common_words_text_corpus_negative_review]

color_list = [&#39;green&#39;, &#39;red&#39;, &#39;cyan&#39;]
title_list = [&#39;Positive Review&#39;, &#39;Neutral Review&#39;, &#39;Negative Review&#39;]


for item in range(3):
    plt.figure(figsize=(11,7))
    plt.bar(splited_data[item][&#39;Word&#39;], 
            splited_data[item][&#39;Frequency&#39;],
            color=color_list[item])
    plt.xticks(rotation = 45)
    plt.xlabel(&#39;Most common Words&#39;)
    plt.ylabel(&quot;Frequency&quot;)
    plt.title(&quot;Frequency distribution of the 25 most common words&quot;)
    plt.suptitle(title_list[item], fontsize=15)
    plt.show()</code></pre>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p24.png" /></p>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p25.png" /></p>
<p><img src="/post/2021-06-10-nlp-text-pre-processing-v-text-exploration_files/p127p26.png" /></p>
<p>Note: I omit the same implementation for the least frequent words at this point. It would follow the same principle as I showed before.</p>
</div>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>5 Conclusion</h1>
<p>In this post I showed how to explore and visualize text blocks easily and quickly. Based on these insights, our next steps will emerge, which I will present in the following post.</p>
<p>Here we only need to save the DataFrame, since we have made changes. This was not the case with the Example String.</p>
<pre class="r"><code>df.to_csv(&#39;Amazon_Unlocked_Mobile_small_Part_V.csv&#39;, index = False)</code></pre>
<p>Furthermore, we have created some frequency tables, which we will need again in the following post. Therefore they will be saved as well.</p>
<pre class="r"><code>df_most_common_words.to_csv(&#39;df_most_common_words.csv&#39;, index = False)
df_least_common_words.to_csv(&#39;df_least_common_words.csv&#39;, index = False)
df_most_common_words_text_corpus.to_csv(&#39;df_most_common_words_text_corpus.csv&#39;, index = False)
df_least_common_words_text_corpus.to_csv(&#39;df_least_common_words_text_corpus.csv&#39;, index = False)</code></pre>
</div>
