---
title: NN - Multi-layer Perceptron Classifier (MLPClassifier)
author: Michael Fuchs
date: '2021-02-03'
slug: nn-multi-layer-perceptron-classifier-mlpclassifier
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#loading-the-libraries">2 Loading the libraries</a></li>
<li><a href="#mlpclassifier-for-binary-classification">3 MLPClassifier for binary Classification</a>
<ul>
<li><a href="#loading-the-data">3.1 Loading the data</a></li>
<li><a href="#data-pre-processing">3.2 Data pre-processing</a></li>
<li><a href="#mlpclassifier">3.3 MLPClassifier</a></li>
<li><a href="#model-evaluation">3.4 Model Evaluation</a></li>
<li><a href="#hyper-parameter-tuning">3.5 Hyper Parameter Tuning</a></li>
</ul></li>
<li><a href="#mlpclassifier-for-multi-label-classification">4 MLPClassifier for Multi-Label Classification</a>
<ul>
<li><a href="#loading-the-data-1">4.1 Loading the data</a></li>
<li><a href="#data-pre-processing-1">4.2 Data pre-processing</a></li>
<li><a href="#mlpclassifier-1">4.3 MLPClassifier</a></li>
<li><a href="#model-evaluation-1">4.4 Model Evaluation</a></li>
<li><a href="#hyper-parameter-tuning-1">4.5 Hyper Parameter Tuning</a></li>
</ul></li>
<li><a href="#conclusion">5 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110s1.png" /></p>
<p>After I already got into the topic of Deep Learning (Computer Vision) with my past posts from January I would like to write about Neural Networks here with a more general post.</p>
<p>When one thinks of Deep Learning, the well-known libraries such as <a href="https://keras.io/">Keras</a>, <a href="https://pytorch.org/">PyTorch</a> or <a href="https://www.tensorflow.org/">TensorFlow</a> immediately come to mind.
Most of us may not know that the very popular machine learning library <a href="https://scikit-learn.org/stable/">Scikit-Learn</a> is also capable of basic deep learning modeling.</p>
<p>How to create a neural net with this library for classification I want to show in this post.</p>
<p>For this publication the datasets <em>Winequality</em> and <em>Iris</em> from the statistic platform <a href="https://www.kaggle.com/c/santander-customer-satisfaction/data">“Kaggle”</a> were used. You can download them from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets">“GitHub Repository”</a>.</p>
</div>
<div id="loading-the-libraries" class="section level1">
<h1>2 Loading the libraries</h1>
<pre class="r"><code>import pandas as pd

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import classification_report

from sklearn.model_selection import GridSearchCV</code></pre>
</div>
<div id="mlpclassifier-for-binary-classification" class="section level1">
<h1>3 MLPClassifier for binary Classification</h1>
<p>The <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">multilayer perceptron (MLP)</a> is a feedforward artificial neural network model that maps input data sets to a set of appropriate outputs. An MLP consists of multiple layers and each layer is fully connected to the following one. The nodes of the layers are neurons with nonlinear activation functions, except for the nodes of the input layer. Between the input and the output layer there may be one or more nonlinear hidden layers.</p>
<div id="loading-the-data" class="section level2">
<h2>3.1 Loading the data</h2>
<pre class="r"><code>df = pd.read_csv(&#39;winequality.csv&#39;).dropna()
df</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p1.png" /></p>
<p>Let’s have a look at the target variable:</p>
<pre class="r"><code>df[&#39;type&#39;].value_counts()</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p2.png" /></p>
</div>
<div id="data-pre-processing" class="section level2">
<h2>3.2 Data pre-processing</h2>
<pre class="r"><code>x = df.drop(&#39;type&#39;, axis=1)
y = df[&#39;type&#39;]

trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
<p>To train a MLP network, the data should always be <a href="https://michael-fuchs-python.netlify.app/2019/08/31/feature-scaling-with-scikit-learn/">scaled</a> because it is very sensitive to it.</p>
<pre class="r"><code>sc=StandardScaler()

scaler = sc.fit(trainX)
trainX_scaled = scaler.transform(trainX)
testX_scaled = scaler.transform(testX)</code></pre>
</div>
<div id="mlpclassifier" class="section level2">
<h2>3.3 MLPClassifier</h2>
<p>Before we train a first MLP, I’ll briefly explain something about the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">parameters</a>.</p>
<p>Suppose we have two predictor variables and want to do a binary classification.
For this I can enter the following parameters at the model:</p>
<pre class="r"><code>mlp_clf = MLPClassifier(hidden_layer_sizes=(5,2),
                        max_iter = 300,activation = &#39;relu&#39;,
                        solver = &#39;adam&#39;)</code></pre>
<ul>
<li>hidden_layer_sizes : With this parameter we can specify the number of layers and the number of nodes we want to have in the Neural Network Classifier. Each element in the tuple represents the number of nodes at the ith position, where i is the index of the tuple. Thus, the length of the tuple indicates the total number of hidden layers in the neural network.</li>
<li>max_iter: Indicates the number of epochs.</li>
<li>activation: The activation function for the hidden layers.</li>
<li>solver: This parameter specifies the algorithm for weight optimization over the nodes.</li>
</ul>
<p>The network structure created in the process would look like this:</p>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p3.png" /></p>
<p>So let’s train our first MLP (with a higher number of layers):</p>
<pre class="r"><code>mlp_clf = MLPClassifier(hidden_layer_sizes=(150,100,50),
                        max_iter = 300,activation = &#39;relu&#39;,
                        solver = &#39;adam&#39;)

mlp_clf.fit(trainX_scaled, trainY)</code></pre>
</div>
<div id="model-evaluation" class="section level2">
<h2>3.4 Model Evaluation</h2>
<p>The metrics that can be used to measure the performance of classification algorithms should be known. Otherwise, you can read about them <a href="https://michael-fuchs-python.netlify.app/2019/10/31/introduction-to-logistic-regression/#model-evaluation">here</a>.</p>
<pre class="r"><code>y_pred = mlp_clf.predict(testX_scaled)

print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred)))</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p4.png" /></p>
<pre class="r"><code>fig = plot_confusion_matrix(mlp_clf, testX_scaled, testY, display_labels=mlp_clf.classes_)
fig.figure_.suptitle(&quot;Confusion Matrix for Winequality Dataset&quot;)
plt.show()</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p5.png" /></p>
<pre class="r"><code>print(classification_report(testY, y_pred))</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p6.png" /></p>
<pre class="r"><code>plt.plot(mlp_clf.loss_curve_)
plt.title(&quot;Loss Curve&quot;, fontsize=14)
plt.xlabel(&#39;Iterations&#39;)
plt.ylabel(&#39;Cost&#39;)
plt.show()</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p7.png" /></p>
</div>
<div id="hyper-parameter-tuning" class="section level2">
<h2>3.5 Hyper Parameter Tuning</h2>
<pre class="r"><code>param_grid = {
    &#39;hidden_layer_sizes&#39;: [(150,100,50), (120,80,40), (100,50,30)],
    &#39;max_iter&#39;: [50, 100, 150],
    &#39;activation&#39;: [&#39;tanh&#39;, &#39;relu&#39;],
    &#39;solver&#39;: [&#39;sgd&#39;, &#39;adam&#39;],
    &#39;alpha&#39;: [0.0001, 0.05],
    &#39;learning_rate&#39;: [&#39;constant&#39;,&#39;adaptive&#39;],
}</code></pre>
<pre class="r"><code>grid = GridSearchCV(mlp_clf, param_grid, n_jobs= -1, cv=5)
grid.fit(trainX_scaled, trainY)

print(grid.best_params_) </code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110z1.png" /></p>
<pre class="r"><code>grid_predictions = grid.predict(testX_scaled) 

print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, grid_predictions)))</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110z2.png" /></p>
</div>
</div>
<div id="mlpclassifier-for-multi-label-classification" class="section level1">
<h1>4 MLPClassifier for Multi-Label Classification</h1>
<p>With an MLP, multi-label classifications can of course also be carried out.</p>
<div id="loading-the-data-1" class="section level2">
<h2>4.1 Loading the data</h2>
<pre class="r"><code>df = pd.read_csv(&#39;Iris_Data.csv&#39;)
df</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p8.png" /></p>
<pre class="r"><code>df[&#39;species&#39;].value_counts()</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p9.png" /></p>
</div>
<div id="data-pre-processing-1" class="section level2">
<h2>4.2 Data pre-processing</h2>
<pre class="r"><code>x = df.drop(&#39;species&#39;, axis=1)
y = df[&#39;species&#39;]

trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
<pre class="r"><code>sc=StandardScaler()

scaler = sc.fit(trainX)
trainX_scaled = scaler.transform(trainX)
testX_scaled = scaler.transform(testX)</code></pre>
</div>
<div id="mlpclassifier-1" class="section level2">
<h2>4.3 MLPClassifier</h2>
<pre class="r"><code>mlp_clf = MLPClassifier(hidden_layer_sizes=(150,100,50),
                        max_iter = 300,activation = &#39;relu&#39;,
                        solver = &#39;adam&#39;)

mlp_clf.fit(trainX_scaled, trainY)</code></pre>
</div>
<div id="model-evaluation-1" class="section level2">
<h2>4.4 Model Evaluation</h2>
<pre class="r"><code>y_pred = mlp_clf.predict(testX_scaled)

print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred)))</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p10.png" /></p>
<pre class="r"><code>fig = plot_confusion_matrix(mlp_clf, testX_scaled, testY, display_labels=mlp_clf.classes_)
fig.figure_.suptitle(&quot;Confusion Matrix for Iris Dataset&quot;)
plt.show()</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p11.png" /></p>
<pre class="r"><code>print(classification_report(testY, y_pred))</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p12.png" /></p>
<pre class="r"><code>plt.plot(mlp_clf.loss_curve_)
plt.title(&quot;Loss Curve&quot;, fontsize=14)
plt.xlabel(&#39;Iterations&#39;)
plt.ylabel(&#39;Cost&#39;)
plt.show()</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p13.png" /></p>
</div>
<div id="hyper-parameter-tuning-1" class="section level2">
<h2>4.5 Hyper Parameter Tuning</h2>
<pre class="r"><code>param_grid = {
    &#39;hidden_layer_sizes&#39;: [(150,100,50), (120,80,40), (100,50,30)],
    &#39;max_iter&#39;: [50, 100, 150],
    &#39;activation&#39;: [&#39;tanh&#39;, &#39;relu&#39;],
    &#39;solver&#39;: [&#39;sgd&#39;, &#39;adam&#39;],
    &#39;alpha&#39;: [0.0001, 0.05],
    &#39;learning_rate&#39;: [&#39;constant&#39;,&#39;adaptive&#39;],
}</code></pre>
<pre class="r"><code>grid = GridSearchCV(mlp_clf, param_grid, n_jobs= -1, cv=5)
grid.fit(trainX_scaled, trainY)

print(grid.best_params_) </code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110z3.png" /></p>
<pre class="r"><code>grid_predictions = grid.predict(testX_scaled) 

print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, grid_predictions)))</code></pre>
<p><img src="/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110z4.png" /></p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>5 Conclusion</h1>
<p>In this post, I showed how to build an MLP model to solve binary and multi-label classification problems.</p>
</div>
