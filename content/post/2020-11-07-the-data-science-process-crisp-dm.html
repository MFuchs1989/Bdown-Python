---
title: The Data Science Process (CRISP-DM)
author: Michael Fuchs
date: '2020-11-07'
slug: the-data-science-process-crisp-dm
categories:
  - R
tags:
  - R Markdown
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="table-of-content" class="section level1">
<h1>Table of Content</h1>
<ul>
<li>1 Introduction</li>
<li>2 Import the libraries</li>
<li>3 Import the data</li>
<li>4 Answering Research Questions - Descriptive Statistics</li>
<li>4.1 Data pre-processing</li>
<li>4.1.1 Check for Outliers</li>
<li>4.1.2 Check for Missing Values</li>
<li>4.1.3 Handling Categorical Variables</li>
<li>4.2 Research Question 1</li>
<li>4.3 Research Question 2</li>
<li>4.4 Research Question 3</li>
<li>5 Development of a Machine Learning Model End to End Process</li>
<li>5.1 Data pre-processing</li>
<li>5.1.1 Train Test Split</li>
<li>5.1.2 Removal of Outlier</li>
<li>5.1.3 Missing Values</li>
<li>5.1.4 Categorical Variables</li>
<li>5.1.5 Scaling</li>
<li>5.2 Filter Methods</li>
<li>5.2.1 Dealing with Highly Correlated Features</li>
<li>5.2.2 Dealing with Constant Features</li>
<li>5.2.3 Dealing with Duplicate Features</li>
<li>5.3 ML-Model Development</li>
<li>5.3.1 Training</li>
<li>5.3.2 Testing</li>
<li>5.3.2.1 Missing Values</li>
<li>5.3.2.2 Categorical Variables</li>
<li>5.3.2.3 Scaling</li>
<li>5.3.2.4 Test the ML-Model</li>
<li>5.4 Feature Selection</li>
<li>5.4.1 Recursive Feature Elimination</li>
<li>5.4.2 Training</li>
<li>5.4.2.1 Column Selection according RFE</li>
<li>5.4.2.2 New Scaling</li>
<li>5.4.2.3 Fit the new ML-Model</li>
<li>5.4.3 Testing</li>
<li>5.4.3.1 Column Selection according RFE</li>
<li>5.4.3.2 New Scaling</li>
<li>5.4.3.3 Test the new ML-Model</li>
<li>5.5 Out-Of-The-Box-Data</li>
<li>5.5.1 Missing Values</li>
<li>5.5.2 Cat Var</li>
<li>5.5.3 Column Selection according RFE</li>
<li>5.5.4 Scaling</li>
<li>5.5.5 Prediction of new Values</li>
<li>6 Conclusion</li>
<li>7 Data Science Best Practice Guidlines for ML-Model Development</li>
<li>8 Limitations</li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79s1.png" /></p>
<p>With reference to my project in the <a href="https://www.udacity.com/course/data-scientist-nanodegree--nd025">“Data Science Nanodegree”</a> of <a href="https://www.udacity.com/">“Udacity”</a> I would like to show the Data Science Process according to CRISP-DM.
If you are interested in the exact content of this course look <a href="https://github.com/MFuchs1989/DSND-Data-Science-Blog-Post/blob/master/Data%2BScientist%2BNanodegree%2BSyllabus.pdf">“here”</a>.</p>
<p>For this post the dataset <em>house-prices</em> from the statistic platform <a href="https://www.kaggle.com/c/santander-customer-satisfaction/data">“Kaggle”</a> was used. A copy of the record is available at my <a href="https://github.com/MFuchs1989/DSND-Data-Science-Blog-Post">“GitHub Repo”</a>.</p>
<p><strong>CRISP-DM</strong></p>
<p>CRISP-DM stands for Cross Industry Standard Process for Data Mining and describes the six phases in a data mining project.</p>
<ul>
<li>Phase 1 Business Understanding:</li>
</ul>
<p>In the business understanding phase, it is important to define the concrete goals and requirements for data mining. The result of this phase is the formulation of the task and the description of the planned rough procedure.</p>
<ul>
<li>Phase 2 Data Understanding:</li>
</ul>
<p>In the context of data understanding, an attempt is made to gain an initial overview of the available data and its quality. An analysis and evaluation of the data quality is carried out. Problems with the quality of the available data in relation to the tasks defined in the previous phase must be identified.</p>
<ul>
<li>Phase 3 Data Preparation:</li>
</ul>
<p>The purpose of data preparation is to create a final data set that forms the basis for the next phase of modeling.</p>
<ul>
<li>Phase 4 Modeling:</li>
</ul>
<p>Within the scope of modeling, the data mining methods suitable for the task are applied to the data set created in the data preparation. Typical for this phase are the optimization of parameters and the creation of several models.</p>
<ul>
<li>Phase 5 Evaluation:</li>
</ul>
<p>The evaluation ensures an exact comparison of the created data models with the task at hand and selects the most suitable model.</p>
<ul>
<li>Phase 6 Deployment:</li>
</ul>
<p>The final phase of the CRISP-DM is the evaluation. In this phase, the obtained results are processed in order to present them and feed them into the decision process of the client.</p>
<p>In the following I will complete these 6 phases using real data.
I will explain each of the steps in detail and explain why they are necessary and why I have done them this way.</p>
</div>
<div id="import-the-libraries" class="section level1">
<h1>2 Import the libraries</h1>
<pre class="r"><code>import pandas as pd
import numpy as np


from sklearn.preprocessing import LabelBinarizer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()


from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler

import statsmodels.formula.api as smf

from sklearn.model_selection import train_test_split

import pickle as pk

from sklearn.preprocessing import StandardScaler

from sklearn.feature_selection import VarianceThreshold

from sklearn import metrics

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression</code></pre>
</div>
<div id="import-the-data" class="section level1">
<h1>3 Import the data</h1>
<p>Here we get a first overview of the data available to us.</p>
<pre class="r"><code>house_prices = pd.read_csv(&quot;house_prices_dataframe.csv&quot;)
house_prices.head()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p1.png" /></p>
<pre class="r"><code>house_prices.shape</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p2.png" /></p>
<pre class="r"><code>house_prices.dtypes</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p3.png" /></p>
</div>
<div id="answering-research-questions---descriptive-statistics" class="section level1">
<h1>4 Answering Research Questions - Descriptive Statistics</h1>
<p>The further procedure follows the Best Practice approach for ML Model Development.
I have listed this approach at the end of this publication (chapter 7).
It allows you to track the naming convention used and to see when which models, data sets or stored metrics are saved.
You can also download these instructions here: <a href="https://github.com/MFuchs1989/DSND-Data-Science-Blog-Post/blob/master/DS%20Best%20Practice%20Guidlines%20for%20ML%20Model%20Development.pdf">“DS Best Practice Guidlines for ML Model Development”</a>.</p>
</div>
<div id="data-pre-processing" class="section level1">
<h1>4.1 Data pre-processing</h1>
</div>
<div id="check-for-outliers" class="section level1">
<h1>4.1.1 Check for Outliers</h1>
<p>First we will check the data frame for some outliers.
This step is necessary first, because otherwise replacing missing values would be negatively affected.
Categorical variables are not considered by this step.</p>
<p>For the identification of outliers the z-score method is used.</p>
<p>In statistics, if a data distribution is approximately normal then about 68%
of the data points lie within one standard deviation (sd) of the mean and about 95%
are within two standard deviations, and about 99.7% lie within three standard deviations.</p>
<p>Therefore, if you have any data point that is more than 3 times the standard deviation,
then those points are very likely to be outliers.</p>
<p>We are going to check observations above a sd of 3 and remove these as an outlier.</p>
<pre class="r"><code>&#39;&#39;&#39;
Here I defined a function do identificate observations with an sd above 3.
&#39;&#39;&#39;

def outliers_z_score(df):
    threshold = 3

    mean = np.mean(df)
    std = np.std(df)
    z_scores = [(y - mean) / std for y in df]
    return np.where(np.abs(z_scores) &gt; threshold)</code></pre>
<p>For the further proceeding we just need numerical colunns.</p>
<pre class="r"><code>my_list = [&#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;]
num_columns = list(house_prices.select_dtypes(include=my_list).columns)
numerical_columns = house_prices[num_columns]
numerical_columns.head(3)</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p4.png" /></p>
<p>Now we are going to apply our outlier-function to all numerical columns</p>
<pre class="r"><code>outlier_list = numerical_columns.apply(lambda x: outliers_z_score(x))
outlier_list</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p5.png" /></p>
<pre class="r"><code>df_of_outlier = outlier_list.iloc[0]
df_of_outlier = pd.DataFrame(df_of_outlier)
df_of_outlier.columns = [&#39;Rows_to_exclude&#39;]
df_of_outlier</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p6.png" /></p>
<pre class="r"><code># Convert all values from column Rows_to_exclude to a numpy array
outlier_list_final = df_of_outlier[&#39;Rows_to_exclude&#39;].to_numpy()

# Concatenate a whole sequence of arrays
outlier_list_final = np.concatenate( outlier_list_final, axis=0 )

# Drop dubplicate values
outlier_list_final_unique = set(outlier_list_final)
outlier_list_final_unique</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p7.png" /></p>
<p>Now we are going to exclude the identified outliers from original dataframe.</p>
<pre class="r"><code>filter_rows_to_exclude = house_prices.index.isin(outlier_list_final_unique)
df_without_outliers = house_prices[~filter_rows_to_exclude]</code></pre>
<pre class="r"><code>print(&#39;Length of original dataframe: &#39; + str(len(house_prices)))

print(&#39;Length of new dataframe without outliers: &#39; + str(len(df_without_outliers)))
print(&#39;----------------------------------------------------------------------------------------------------&#39;)
print(&#39;Difference between new and old dataframe: &#39; + str(len(house_prices) - len(df_without_outliers)))
print(&#39;----------------------------------------------------------------------------------------------------&#39;)
print(&#39;Length of unique outlier list: &#39; + str(len(outlier_list_final_unique)))</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p8.png" /></p>
<p>Important!</p>
<p>If you look at the index of the last two columns, you will notice that index 5 is missing.
This is because the values from row 5 for the variables bathrooms and sqft_living
were identified as outlier and removed accordingly.</p>
<p>The data set now has 1178 (the number of unique outliers) gaps at the corresponding position in the index.
This can lead to problems if you want to join other tables via the index.</p>
<p>Since I know that this will be the case as soon as I encode the categorical variables,
I proactively create a new index (via reset_index function) which will allow me to easily add new records later.</p>
<p>Below you can see the difference between index and new_index.</p>
<pre class="r"><code>df_without_outliers = df_without_outliers.reset_index()

df_without_outliers = df_without_outliers.rename(columns={&#39;index&#39;:&#39;old_index&#39;})

df_without_outliers.head(6)</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p9.png" /></p>
<p>Here is a comparison of the distribution of values from the target variable ‘house price’ before and after outlier removal.</p>
<pre class="r"><code># Distribution before outlier removal
sns.boxplot(x=&#39;price&#39;, data=house_prices)</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p10.png" /></p>
<pre class="r"><code># Distribution after outlier removal
sns.boxplot(x=&#39;price&#39;, data=df_without_outliers)</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p11.png" /></p>
</div>
<div id="check-for-missing-values" class="section level1">
<h1>4.1.2 Check for Missing Values</h1>
<p>Now we are going to check for missing values.</p>
<pre class="r"><code>def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()
        
        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        
        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : &#39;Missing Values&#39;, 1 : &#39;% of Total Values&#39;})
        
        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        &#39;% of Total Values&#39;, ascending=False).round(1)
        
        # Print some summary information
        print (&quot;Your selected dataframe has &quot; + str(df.shape[1]) + &quot; columns.\n&quot;      
            &quot;There are &quot; + str(mis_val_table_ren_columns.shape[0]) +
              &quot; columns that have missing values.&quot;)
        
        # Return the dataframe with missing information
        return mis_val_table_ren_columns</code></pre>
<pre class="r"><code>missing_values_table(df_without_outliers)</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p12.png" /></p>
<p>Due to the high number of missing values (&gt;75%) the column yr_renovated should not be considered in the further analysis.</p>
<pre class="r"><code>&#39;&#39;&#39;
Delete the column yr_renovated
&#39;&#39;&#39;

df_without_outliers = df_without_outliers.drop([&#39;yr_renovated&#39;], axis=1)
df_without_outliers.head()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p13.png" /></p>
<p>Due to the low number of Missing Values within the grade column, this column will not be deleted as valuable information could be lost. The missing values should be filled with the mean value of the column instead.</p>
<pre class="r"><code>&#39;&#39;&#39;
Impute missing values for column grade
&#39;&#39;&#39;

df_without_outliers[&#39;grade&#39;] = df_without_outliers[&#39;grade&#39;].fillna(df_without_outliers[&#39;grade&#39;].mean())</code></pre>
<p>Check if all missing values are removed or replaced.</p>
<pre class="r"><code>missing_values_table(df_without_outliers)</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p14.png" /></p>
<p>According to our best practice approach we will assign a new name to the data set.</p>
<pre class="r"><code>df_without_MV = df_without_outliers</code></pre>
</div>
<div id="handling-categorical-variables" class="section level1">
<h1>4.1.3 Handling Categorical Variables</h1>
<p>Now we are going to handle the categorical variables.</p>
<pre class="r"><code>obj_col = [&#39;object&#39;]
object_columns = list(df_without_MV.select_dtypes(include=obj_col).columns)
house_prices_categorical = df_without_MV[object_columns]

print()
print(&#39;There are &#39; + str(house_prices_categorical.shape[1]) + &#39; categorical columns within dataframe:&#39;)

house_prices_categorical.head()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p15.png" /></p>
<pre class="r"><code>print(&#39;Values of the variable waterfront:&#39;)
print()
print(df_without_MV[&#39;waterfront&#39;].value_counts())

print(&#39;--------------------------------------------&#39;)

print(&#39;Values of the variable view:&#39;)
print()
print(df_without_MV[&#39;view&#39;].value_counts())

print(&#39;--------------------------------------------&#39;)

print(&#39;Values of the variable property_typ:&#39;)
print()
print(df_without_MV[&#39;property_typ&#39;].value_counts())</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p16.png" /></p>
<p>Based on the output shown above, the following scale level can be determined for the three categorical variables:</p>
<ul>
<li>waterfront: binary</li>
<li>view: ordinal</li>
<li>property_typ: nominal</li>
</ul>
<p>The variables will be coded accordingly in the following.</p>
<pre class="r"><code>&#39;&#39;&#39;
In the following the column waterfront is coded.
Then the newly generated values are inserted into the original dataframe 
and the old column will be deleted. 
&#39;&#39;&#39;

encoder_waterfront = LabelBinarizer()

# Application of the LabelBinarizer
waterfront_encoded = encoder_waterfront.fit_transform(df_without_MV.waterfront.values.reshape(-1,1))

# Insertion of the coded values into the original data set
df_without_MV[&#39;waterfront_encoded&#39;] = waterfront_encoded

# Delete the original column to avoid duplication
df_without_MV = df_without_MV.drop([&#39;waterfront&#39;], axis=1)

# Getting the exact coding and show new dataframe
print(encoder_waterfront.classes_)
print(&#39;Codierung: no=0, yes=1&#39;)
print(&#39;-----------------------------&#39;)
print()
print(&#39;New Data Frame:&#39;)
df_without_MV.head()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p17.png" /></p>
<pre class="r"><code>&#39;&#39;&#39;
In the following the column view is coded.
Then the newly generated values are inserted into the original dataframe 
and the old column will be deleted. 
&#39;&#39;&#39;

# Create a dictionary how the observations should be coded
view_dict = {&#39;bad&#39; : 0,
             &#39;medium&#39; : 1,
             &#39;good&#39; : 2,
             &#39;very_good&#39; : 3,
             &#39;excellent&#39; : 4}

# Map the dictionary on the column view and store the results in a new column
df_without_MV[&#39;view_encoded&#39;] = df_without_MV.view.map(view_dict)

# Delete the original column to avoid duplication
df_without_MV = df_without_MV.drop([&#39;view&#39;], axis=1)


print(&#39;New Data Frame:&#39;)
df_without_MV.head()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p18.png" /></p>
<pre class="r"><code>&#39;&#39;&#39;
In the following the column property_typ is coded.
Then the newly generated values are inserted into the original dataframe 
and the old column will be deleted. 
&#39;&#39;&#39;

encoder_property_typ = OneHotEncoder()

# Application of the OneHotEncoder
OHE = encoder_property_typ.fit_transform(df_without_MV.property_typ.values.reshape(-1,1)).toarray()

# Conversion of the newly generated data to a dataframe
df_OHE = pd.DataFrame(OHE, columns = [&quot;property_typ_&quot; + str(encoder_property_typ.categories_[0][i]) 
                                     for i in range(len(encoder_property_typ.categories_[0]))])




# Insertion of the coded values into the original data set
df_without_MV = pd.concat([df_without_MV, df_OHE], axis=1)


# Delete the original column to avoid duplication
df_without_MV = df_without_MV.drop([&#39;property_typ&#39;], axis=1)


print(&#39;New Data Frame:&#39;)
df_without_MV.head()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p19.png" /></p>
<p>Now that the record is clean we can rename it accordingly.
It is also recommended to save the clean data set at this point.</p>
<pre class="r"><code>final_df_house_prices = df_without_MV

final_df_house_prices.to_csv(&#39;dataframes/final_df_house_prices.csv&#39;, index=False)</code></pre>
</div>
<div id="research-question-1" class="section level1">
<h1>4.2 Research Question 1</h1>
<p>The first research question is:</p>
<p>Is there a difference in the characteristics of the properties, if they are located on a waterfront?</p>
<pre class="r"><code>print(&#39;Absolute distribution: &#39;)
print()
print(final_df_house_prices[&#39;waterfront_encoded&#39;].value_counts())

print(&#39;-------------------------------------------------------&#39;)

print(&#39;Percentage distribution: &#39;)
print()
print(pd.DataFrame({&#39;Percentage&#39;: final_df_house_prices.groupby((&#39;waterfront_encoded&#39;)).size() / len(final_df_house_prices)}))</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p20.png" /></p>
<pre class="r"><code>final_df_house_prices.groupby((&#39;waterfront_encoded&#39;)).mean()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p21.png" /></p>
<p>Answer the first research question:</p>
<p>The average price for waterfront properties is almost twice as high as for other properties.</p>
<p>In contrast, however, plots without a water front have on average more square meters of living space and usable area.
Also in terms of condition and degree they are a little better off.</p>
<p>If one regards the year of construction of the real estates then one can state that the houses at a waterfront are average 18 years older.</p>
</div>
<div id="research-question-2" class="section level1">
<h1>4.3 Research Question 2</h1>
<p>The second research question is:</p>
<p>What clusters are there in terms of real estate?</p>
<pre class="r"><code># Here we drop the column old_index because we do not want to cluster these features

house_prices_cluster = final_df_house_prices.drop([&#39;old_index&#39;], axis=1)
house_prices_cluster</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p22.png" /></p>
<p>Let’s have another look at the distribution of the variable ‘house prices’. This time not with a box plot but with a histogram.</p>
<pre class="r"><code>plt.hist(house_prices_cluster[&#39;price&#39;], bins=&#39;auto&#39;)
plt.title(&quot;Histogram for house prices&quot;)
plt.xlim(xmin=0, xmax = 1200000)
plt.show()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p23.png" /></p>
<pre class="r"><code>mms = MinMaxScaler()
mms.fit(house_prices_cluster)
data_transformed = mms.transform(house_prices_cluster)</code></pre>
<pre class="r"><code>Sum_of_squared_distances = []
K = range(1,15)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(data_transformed)
    Sum_of_squared_distances.append(km.inertia_)</code></pre>
<pre class="r"><code>plt.plot(K, Sum_of_squared_distances, &#39;bx-&#39;)
plt.xlabel(&#39;k&#39;)
plt.ylabel(&#39;Sum_of_squared_distances&#39;)
plt.title(&#39;Elbow Method For Optimal k&#39;)
plt.show()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p24.png" /></p>
<pre class="r"><code>km = KMeans(n_clusters=4, random_state=1)
km.fit(house_prices_cluster)</code></pre>
<pre class="r"><code>predict=km.predict(house_prices_cluster)
house_prices_cluster[&#39;clusters&#39;] = pd.Series(predict, index=house_prices_cluster.index)</code></pre>
<pre class="r"><code>df_sub = house_prices_cluster[[&#39;sqft_living&#39;, &#39;price&#39;]].values

plt.scatter(df_sub[predict==0, 0], df_sub[predict==0, 1], s=100, c=&#39;red&#39;, label =&#39;Cluster 1&#39;)
plt.scatter(df_sub[predict==1, 0], df_sub[predict==1, 1], s=100, c=&#39;blue&#39;, label =&#39;Cluster 2&#39;)
plt.scatter(df_sub[predict==2, 0], df_sub[predict==2, 1], s=100, c=&#39;green&#39;, label =&#39;Cluster 3&#39;)
plt.scatter(df_sub[predict==3, 0], df_sub[predict==3, 1], s=100, c=&#39;cyan&#39;, label =&#39;Cluster 4&#39;)

plt.title(&#39;Cluster of Houses&#39;)
plt.xlim((0, 5000))
plt.ylim((0,2000000))
plt.xlabel(&#39;sqft_living \n\n Cluster1(Red), Cluster2 (Blue), Cluster3(Green), Cluster4(Cyan)&#39;)
plt.ylabel(&#39;Price&#39;)
plt.show()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p25.png" /></p>
<p>Answer the second research question:</p>
<p>Within the data set 4 clusters could be identified.
We can see in the graphic above that the target buyer groups of the houses can be clearly defined.</p>
</div>
<div id="research-question-3" class="section level1">
<h1>4.4 Research Question 3</h1>
<p>The third research question is:</p>
<p>Does the number of square meters have a significant influence on the price of the property?</p>
<p>To answer the research question we filter the data set for the variable price and sqft_living.</p>
<pre class="r"><code>HousePrices_SimplReg = final_df_house_prices[[&#39;price&#39;, &#39;sqft_living&#39;]]
HousePrices_SimplReg.head()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p26.png" /></p>
<pre class="r"><code>x = HousePrices_SimplReg[&#39;sqft_living&#39;]
y = HousePrices_SimplReg[&#39;price&#39;]

plt.scatter(x, y)
plt.title(&#39;Scatter plot: sqft_living vs. price&#39;)
plt.xlabel(&#39;sqft_living&#39;)
plt.ylabel(&#39;price&#39;)
plt.show()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p27.png" /></p>
<pre class="r"><code>model1 = smf.ols(formula=&#39;price~sqft_living&#39;, data=HousePrices_SimplReg).fit()

model1.summary()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p28.png" /></p>
<p>Answer the third research question:</p>
<p>As we can see from the p value, it is highly significant.
It can therefore be assumed that the number of square meters is a significant predictor of the price of a property.</p>
</div>
<div id="development-of-a-machine-learning-model-end-to-end-process" class="section level1">
<h1>5 Development of a Machine Learning Model End to End Process</h1>
<p>Now that the descriptive part has been completed and the research questions posed at the beginning have been answered, we now come to the second main part of this publication.</p>
<p>The training of a machine learning model for predicting house prices.</p>
<p><strong>A golden rule in Data Science</strong></p>
<p>For the development of a machine learning algorithm it is imperative that all pre-processing steps
are applied to the training part only! Anything else would negatively affect the later performance of the algorithm. Outliers are the only exceptions to this rule. Here it always depends on the respective data set and its size.</p>
<p>Often I have found that this rule is not followed. This has been one of the motivations for me to describe a complete end-to-end process for the development of machine learning models in this post.</p>
<p>The further procedure follows the Best Practice approach for ML Model Development. I have listed this approach at the end of this publication (chapter 7). You can also download these instructions here: <a href="https://github.com/MFuchs1989/DSND-Data-Science-Blog-Post/blob/master/DS%20Best%20Practice%20Guidlines%20for%20ML%20Model%20Development.pdf">“DS Best Practice Guidlines for ML Model Development”</a>. It allows you to track the naming convention used and to see when which models, data sets or stored metrics are saved.</p>
</div>
<div id="data-pre-processing-1" class="section level1">
<h1>5.1 Data pre-processing</h1>
<p>Due to the fact that we have to approach the pre-processing steps a bit differently to create an ML algorithm,
we reload the original data set.</p>
<pre class="r"><code>house_prices = pd.read_csv(&quot;house_prices_dataframe.csv&quot;)
house_prices.head()</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p29.png" /></p>
</div>
<div id="train-test-split" class="section level1">
<h1>5.1.1 Train Test Split</h1>
<p>Here we are going to split the data into a training part (80%) and a test part (20%).</p>
<pre class="r"><code>x = house_prices.drop([&#39;price&#39;], axis=1)
y = house_prices[&#39;price&#39;]

trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
<p>At this point it is recommended to reset the index of the 4 created ‘records’ and to delete
the old index (which is automatically generated as new column).</p>
<pre class="r"><code>trainX = trainX.reset_index().drop([&#39;index&#39;], axis=1)
testX = testX.reset_index().drop([&#39;index&#39;], axis=1)
trainY = trainY.reset_index().drop([&#39;index&#39;], axis=1)
testY = testY.reset_index().drop([&#39;index&#39;], axis=1)</code></pre>
</div>
<div id="removal-of-outlier" class="section level1">
<h1>5.1.2 Removal of Outlier</h1>
<pre class="r"><code># Define the outlier-detect function
def outliers_z_score(df):
    threshold = 3

    mean = np.mean(df)
    std = np.std(df)
    z_scores = [(y - mean) / std for y in df]
    return np.where(np.abs(z_scores) &gt; threshold)

# For the further proceeding we just need numerical colunns
my_list = [&#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;]
num_columns = list(trainX.select_dtypes(include=my_list).columns)
numerical_columns = trainX[num_columns]

# Now we are going to apply our outlier-function to all numerical columns
outlier_list = numerical_columns.apply(lambda x: outliers_z_score(x))
outlier_list

# Store the results within a dataframe
df_of_outlier = outlier_list.iloc[0]
df_of_outlier = pd.DataFrame(df_of_outlier)
df_of_outlier.columns = [&#39;Rows_to_exclude&#39;]


# Convert all values from column Rows_to_exclude to a numpy array
outlier_list_final = df_of_outlier[&#39;Rows_to_exclude&#39;].to_numpy()

# Concatenate a whole sequence of arrays
outlier_list_final = np.concatenate( outlier_list_final, axis=0 )

# Drop dubplicate values
outlier_list_final_unique = set(outlier_list_final)

# Create a new dataframe without the identified outlier
filter_rows_to_exclude = trainX.index.isin(outlier_list_final_unique)
trainX_wo_outlier = trainX[~filter_rows_to_exclude]

# Reset the index again 
trainX_wo_outlier = trainX_wo_outlier.reset_index().drop([&#39;index&#39;], axis=1)</code></pre>
<pre class="r"><code>print(&#39;Length of original dataframe: &#39; + str(len(trainX)))

print(&#39;Length of new dataframe without outliers: &#39; + str(len(trainX_wo_outlier)))
print(&#39;----------------------------------------------------------------------------------------------------&#39;)
print(&#39;Difference between new and old dataframe: &#39; + str(len(trainX) - len(trainX_wo_outlier)))
print(&#39;----------------------------------------------------------------------------------------------------&#39;)
print(&#39;Length of unique outlier list: &#39; + str(len(outlier_list_final_unique)))</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p30.png" /></p>
<p>Of course it is now also necessary to remove the corresponding Y values of the outlier.</p>
<pre class="r"><code>filter_rows_to_exclude = trainY.index.isin(outlier_list_final_unique)
trainY_wo_outlier = trainY[~filter_rows_to_exclude]

trainY_wo_outlier = trainY_wo_outlier.reset_index().drop([&#39;index&#39;], axis=1)</code></pre>
<pre class="r"><code>print(&#39;Length of original dataframe: &#39; + str(len(trainY)))

print(&#39;Length of new dataframe without outliers: &#39; + str(len(trainY_wo_outlier)))
print(&#39;----------------------------------------------------------------------------------------------------&#39;)
print(&#39;Difference between new and old dataframe: &#39; + str(len(trainY) - len(trainY_wo_outlier)))
print(&#39;----------------------------------------------------------------------------------------------------&#39;)
print(&#39;Length of unique outlier list: &#39; + str(len(outlier_list_final_unique)))</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p31.png" /></p>
</div>
<div id="missing-values" class="section level1">
<h1>5.1.3 Missing Values</h1>
<pre class="r"><code>def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()
        
        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        
        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : &#39;Missing Values&#39;, 1 : &#39;% of Total Values&#39;})
        
        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        &#39;% of Total Values&#39;, ascending=False).round(1)
        
        # Print some summary information
        print (&quot;Your selected dataframe has &quot; + str(df.shape[1]) + &quot; columns.\n&quot;      
            &quot;There are &quot; + str(mis_val_table_ren_columns.shape[0]) +
              &quot; columns that have missing values.&quot;)
        
        # Return the dataframe with missing information
        return mis_val_table_ren_columns

    
missing_values_table(trainX_wo_outlier)</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p32.png" /></p>
<pre class="r"><code># Deletion of the column yr_renovated because too many missing values.

trainX_wo_outlier = trainX_wo_outlier.drop([&#39;yr_renovated&#39;], axis=1)</code></pre>
<pre class="r"><code># Impute missing values for column grade with mean values of this columns

trainX_wo_outlier[&#39;grade&#39;] = trainX_wo_outlier[&#39;grade&#39;].fillna(trainX_wo_outlier[&#39;grade&#39;].mean())</code></pre>
<pre class="r"><code># Check if all missing values are removed or replaced.

missing_values_table(trainX_wo_outlier)</code></pre>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p33.png" /></p>
</div>
<div id="categorical-variables" class="section level1">
<h1>5.1.4 Categorical Variables</h1>
</div>
<div id="scaling" class="section level1">
<h1>5.1.5 Scaling</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="filter-methods" class="section level1">
<h1>5.2 Filter Methods</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="dealing-with-highly-correlated-features" class="section level1">
<h1>5.2.1 Dealing with Highly Correlated Features</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="dealing-with-constant-features" class="section level1">
<h1>5.2.2 Dealing with Constant Features</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="dealing-with-duplicate-features" class="section level1">
<h1>5.2.3 Dealing with Duplicate Features</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="ml-model-development" class="section level1">
<h1>5.3 ML-Model Development</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="training" class="section level1">
<h1>5.3.1 Training</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="testing" class="section level1">
<h1>5.3.2 Testing</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="missing-values-1" class="section level1">
<h1>5.3.2.1 Missing Values</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="categorical-variables-1" class="section level1">
<h1>5.3.2.2 Categorical Variables</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="scaling-1" class="section level1">
<h1>5.3.2.3 Scaling</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="test-the-ml-model" class="section level1">
<h1>5.3.2.4 Test the ML-Model</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
<div id="feature-selection" class="section level1">
<h1>5.4 Feature Selection</h1>
</div>
<div id="recursive-feature-elimination" class="section level1">
<h1>5.4.1 Recursive Feature Elimination</h1>
</div>
<div id="training-1" class="section level1">
<h1>5.4.2 Training</h1>
</div>
<div id="column-selection-according-rfe" class="section level1">
<h1>5.4.2.1 Column Selection according RFE</h1>
</div>
<div id="new-scaling" class="section level1">
<h1>5.4.2.2 New Scaling</h1>
</div>
<div id="fit-the-new-ml-model" class="section level1">
<h1>5.4.2.3 Fit the new ML-Model</h1>
</div>
<div id="testing-1" class="section level1">
<h1>5.4.3 Testing</h1>
</div>
<div id="column-selection-according-rfe-1" class="section level1">
<h1>5.4.3.1 Column Selection according RFE</h1>
</div>
<div id="new-scaling-1" class="section level1">
<h1>5.4.3.2 New Scaling</h1>
</div>
<div id="test-the-new-ml-model" class="section level1">
<h1>5.4.3.3 Test the new ML-Model</h1>
</div>
<div id="out-of-the-box-data" class="section level1">
<h1>5.5 Out-Of-The-Box-Data</h1>
</div>
<div id="missing-values-2" class="section level1">
<h1>5.5.1 Missing Values</h1>
</div>
<div id="cat-var" class="section level1">
<h1>5.5.2 Cat Var</h1>
</div>
<div id="column-selection-according-rfe-2" class="section level1">
<h1>5.5.3 Column Selection according RFE</h1>
</div>
<div id="scaling-2" class="section level1">
<h1>5.5.4 Scaling</h1>
</div>
<div id="prediction-of-new-values" class="section level1">
<h1>5.5.5 Prediction of new Values</h1>
</div>
<div id="conclusion" class="section level1">
<h1>6 Conclusion</h1>
</div>
<div id="data-science-best-practice-guidlines-for-ml-model-development" class="section level1">
<h1>7 Data Science Best Practice Guidlines for ML-Model Development</h1>
</div>
<div id="limitations" class="section level1">
<h1>8 Limitations</h1>
<p><img src="/post/2020-11-07-the-data-science-process-crisp-dm/p79p.png" /></p>
</div>
