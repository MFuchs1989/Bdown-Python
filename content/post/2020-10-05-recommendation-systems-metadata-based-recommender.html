---
title: Recommendation Systems - Metadata-based Recommender
author: Michael Fuchs
date: '2020-10-05'
slug: recommendation-systems-metadata-based-recommender
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#import-the-libraries-and-the-data">2 Import the libraries and the data</a></li>
<li><a href="#data-pre-processing">3 Data pre-processing</a>
<ul>
<li><a href="#clean-id-column-of-df">3.1 Clean id column of df</a></li>
<li><a href="#join-the-dataframes">3.2 Join the dataframes</a></li>
<li><a href="#wrangling-crew-cast-keywords-and-genres">3.3 Wrangling crew, cast, keywords and genres</a></li>
</ul></li>
<li><a href="#sanitize-data">3.4 Sanitize data</a>
<ul>
<li><a href="#create-a-soup-of-desired-metadata">3.5 Create a soup of desired metadata</a></li>
<li><a href="#create-vectors-with-countvectorizer">3.6 Create vectors with CountVectorizer</a></li>
<li><a href="#compute-the-pairwise-similarity">3.7 Compute the pairwise similarity</a></li>
</ul></li>
<li><a href="#build-the-metadata-based-recommender">4 Build the Metadata-based Recommender</a></li>
<li><a href="#test-the-recommender">5 Test the recommender</a></li>
<li><a href="#conclusion">6 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>Now that we have developed a <a href="https://michael-fuchs-python.netlify.app/2020/10/03/recommendation-systems-plot-description-based-recommender/">“recommender”</a> based on the film descriptions, we will go a step further in this post and add more metadata.</p>
<p>For this post the datasets <em>movies_metadata</em>, <em>keywords</em> and <em>credits</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used. You can download it from my <a href="https://github.com/MFuchs1989/Bdown-Python/tree/master/datasets/Recommendation%20Systems/Knowledged-based%20Recommender">“GitHub Repository”</a> and here:</p>
<ul>
<li><a href="https://drive.google.com/file/d/130LECn3_7LoBL6nMHVIGG0zw6DqitPib/view?usp=sharing">“keywords dataframe”</a></li>
<li><a href="https://drive.google.com/file/d/1ReopPjfDglnCiKeJs_TfnMP0T5wM36zx/view?usp=sharing">“credits dataframe”</a></li>
</ul>
</div>
<div id="import-the-libraries-and-the-data" class="section level1">
<h1>2 Import the libraries and the data</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

import preprocessing_recommender_systems as prs

from ast import literal_eval

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity</code></pre>
<p>As in the <a href="https://michael-fuchs-python.netlify.app/2020/10/03/recommendation-systems-plot-description-based-recommender/">“Plot Description-based Recommender”</a> post I will execute the first pre-processing steps for the main dataset using my stored function in the preprocessing_recommender_systems.py file.
This file is also stored on my <a href="https://github.com/MFuchs1989/Bdown-Python/tree/master/datasets/Recommendation%20Systems/Metadata-based%20Recommender">“GitHub Account”</a> and can be downloaded from there.</p>
<pre class="r"><code>df = pd.read_csv(&#39;movies_metadata.csv&#39;)
df = prs.clean_data(df)
df</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p1.png" /></p>
<p>Let’s load the other two datasets as well:</p>
<pre class="r"><code>cred_df = pd.read_csv(&#39;credits.csv&#39;)
cred_df.head()</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p2.png" /></p>
<pre class="r"><code>key_df = pd.read_csv(&#39;keywords.csv&#39;)
key_df.head()</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p3.png" /></p>
</div>
<div id="data-pre-processing" class="section level1">
<h1>3 Data pre-processing</h1>
<p>For the preparation of the data we have to take some steps.</p>
<div id="clean-id-column-of-df" class="section level2">
<h2>3.1 Clean id column of df</h2>
<pre class="r"><code># Function to convert all non-integer IDs to NaN
def clean_ids(x):
    try:
        return int(x)
    except:
        return np.nan

    
#Clean the ids of df
df[&#39;id&#39;] = df[&#39;id&#39;].apply(clean_ids)

#Filter all rows where ID is not null
df = df[df[&#39;id&#39;].notnull()] </code></pre>
</div>
<div id="join-the-dataframes" class="section level2">
<h2>3.2 Join the dataframes</h2>
<p>Here we get a final dataset with the columns cast, crew and keywords from the last loaded records.</p>
<pre class="r"><code># Convert IDs into integer
df[&#39;id&#39;] = df[&#39;id&#39;].astype(&#39;int&#39;)
key_df[&#39;id&#39;] = key_df[&#39;id&#39;].astype(&#39;int&#39;)
cred_df[&#39;id&#39;] = cred_df[&#39;id&#39;].astype(&#39;int&#39;)

# Merge keywords and credits into our updated metadata dataframe
df = df.merge(cred_df, on=&#39;id&#39;)
df = df.merge(key_df, on=&#39;id&#39;)

#Display the head of df
df.head()</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p4.png" /></p>
</div>
<div id="wrangling-crew-cast-keywords-and-genres" class="section level2">
<h2>3.3 Wrangling crew, cast, keywords and genres</h2>
<p>In the following we will work on the columns crew, cast, keywords and genres so that they are usable for the recommender.</p>
<pre class="r"><code># Convert the stringified objects into the native python objects
from ast import literal_eval

features = [&#39;cast&#39;, &#39;crew&#39;, &#39;keywords&#39;]
for feature in features:
    df[feature] = df[feature].apply(literal_eval)</code></pre>
<pre class="r"><code>df.head()</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p5.png" /></p>
<p>In the pre-processing step via preprocessing_recommender_systems.py literal_eval has already been applied to the column genres. For this reason it is not listed in the features here.</p>
<p><strong>Column ‘crew’</strong></p>
<p>Here we are only interested in the director.</p>
<pre class="r"><code>#Print the first crew member of the first movie in df
df.iloc[0][&#39;crew&#39;][0]</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p6.png" /></p>
<pre class="r"><code># Extract the director&#39;s name. If director is not listed, return NaN
def get_director(x):
    for crew_member in x:
        if crew_member[&#39;job&#39;] == &#39;Director&#39;:
            return crew_member[&#39;name&#39;]
    return np.nan</code></pre>
<pre class="r"><code>#Define the new director feature
df[&#39;director&#39;] = df[&#39;crew&#39;].apply(get_director)

#Print the directors of the first five movies
df[&#39;director&#39;].head()</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p7.png" /></p>
<p><strong>Column ‘cast’</strong></p>
<p>Here we are interested in the first three cast members.</p>
<pre class="r"><code>#Print the first cast member of the first movie in df
df.iloc[0][&#39;cast&#39;][0]</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p8.png" /></p>
<pre class="r"><code># Returns the list top 3 elements or entire list; whichever is more.
def generate_list(x):
    if isinstance(x, list):
        names = [i[&#39;name&#39;] for i in x]
        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.
        if len(names) &gt; 3:
            names = names[:3]
        return names

    #Return empty list in case of missing/malformed data
    return []</code></pre>
<pre class="r"><code>#Apply the generate_list function to cast
df[&#39;cast&#39;] = df[&#39;cast&#39;].apply(generate_list)</code></pre>
<p><strong>Column ‘keywords’</strong></p>
<p>Here we are interested in the first three keywords.</p>
<pre class="r"><code>#Print the first keyword of the first movie in df
df.iloc[0][&#39;keywords&#39;][3]</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p9.png" /></p>
<p>We can again use the generate_list function here.</p>
<pre class="r"><code>#Apply the generate_list function to keywords
df[&#39;keywords&#39;] = df[&#39;keywords&#39;].apply(generate_list)</code></pre>
<p><strong>Column ‘genres’</strong></p>
<pre class="r"><code>#Only consider a maximum of 3 genres
df[&#39;genres&#39;] = df[&#39;genres&#39;].apply(lambda x: x[:3])</code></pre>
</div>
</div>
<div id="sanitize-data" class="section level1">
<h1>3.4 Sanitize data</h1>
<p>Our current data set looks like this:</p>
<pre class="r"><code># Print the new features of the first 5 movies along with title
df[[&#39;title&#39;, &#39;cast&#39;, &#39;director&#39;, &#39;keywords&#39;, &#39;genres&#39;]].head()</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p10.png" /></p>
<p>We will again apply a vectorizer to the text columns, as we did with the Plot Description-based Recommender.
In order for it to work correctly, we need to remove the spaces between all the names and keywords (if there are any) and convert them into lowercase.</p>
<pre class="r"><code># Function to sanitize data to prevent ambiguity. It removes spaces and converts to lowercase
def sanitize(x):
    if isinstance(x, list):
        #Strip spaces and convert to lowercase
        return [str.lower(i.replace(&quot; &quot;, &quot;&quot;)) for i in x]
    else:
        #Check if director exists. If not, return empty string
        if isinstance(x, str):
            return str.lower(x.replace(&quot; &quot;, &quot;&quot;))
        else:
            return &#39;&#39;</code></pre>
<pre class="r"><code>#Apply the sanitize function to cast, keywords, director and genres
for feature in [&#39;cast&#39;, &#39;director&#39;, &#39;genres&#39;, &#39;keywords&#39;]:
    df[feature] = df[feature].apply(sanitize)</code></pre>
<pre class="r"><code>df[[&#39;title&#39;, &#39;cast&#39;, &#39;director&#39;, &#39;keywords&#39;, &#39;genres&#39;]].head()</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p11.png" /></p>
<div id="create-a-soup-of-desired-metadata" class="section level2">
<h2>3.5 Create a soup of desired metadata</h2>
<pre class="r"><code>#Function that creates a soup out of the desired metadata
def create_soup(x):
    return &#39; &#39;.join(x[&#39;keywords&#39;]) + &#39; &#39; + &#39; &#39;.join(x[&#39;cast&#39;]) + &#39; &#39; + x[&#39;director&#39;] + &#39; &#39; + &#39; &#39;.join(x[&#39;genres&#39;])</code></pre>
<pre class="r"><code># Create the new soup feature
df[&#39;soup&#39;] = df.apply(create_soup, axis=1)</code></pre>
<pre class="r"><code>#Display the soup of the first movie
df.iloc[0][&#39;soup&#39;]</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p12.png" /></p>
</div>
<div id="create-vectors-with-countvectorizer" class="section level2">
<h2>3.6 Create vectors with CountVectorizer</h2>
<pre class="r"><code># Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

#Define a new CountVectorizer object and create vectors for the soup
count = CountVectorizer(stop_words=&#39;english&#39;)
count_matrix = count.fit_transform(df[&#39;soup&#39;])</code></pre>
</div>
<div id="compute-the-pairwise-similarity" class="section level2">
<h2>3.7 Compute the pairwise similarity</h2>
<pre class="r"><code>#Import cosine_similarity function
from sklearn.metrics.pairwise import cosine_similarity

#Compute the cosine similarity score (equivalent to dot product for tf-idf vectors)
cosine_sim = cosine_similarity(count_matrix, count_matrix)</code></pre>
</div>
</div>
<div id="build-the-metadata-based-recommender" class="section level1">
<h1>4 Build the Metadata-based Recommender</h1>
<pre class="r"><code># Reset index of your df and construct reverse mapping again
df = df.reset_index()
indices = pd.Series(df.index, index=df[&#39;title&#39;])</code></pre>
<pre class="r"><code># Function that takes in movie title as input and gives recommendations 
def content_recommender(title, cosine_sim=cosine_sim, df=df, indices=indices):
    # Obtain the index of the movie that matches the title
    idx = indices[title]

    # Get the pairwsie similarity scores of all movies with that movie
    # And convert it into a list of tuples as described above
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the movies based on the cosine similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar movies. Ignore the first movie.
    sim_scores = sim_scores[1:11]

    # Get the movie indices
    movie_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar movies
    return df[&#39;title&#39;].iloc[movie_indices]</code></pre>
</div>
<div id="test-the-recommender" class="section level1">
<h1>5 Test the recommender</h1>
<pre class="r"><code>content_recommender(&#39;Toy Story&#39;, cosine_sim, df, indices)</code></pre>
<p><img src="/post/2020-10-05-recommendation-systems-metadata-based-recommender_files/p93p13.png" /></p>
</div>
<div id="conclusion" class="section level1">
<h1>6 Conclusion</h1>
<p>Done.
Now we have designed a recommender that takes into account any amount of additional data such as the participating actors or the director in charge.</p>
<p><strong>References</strong></p>
<p>Alfarhood, M., &amp; Cheng, J. (2018, December). DeepHCF: a deep learning based hybrid collaborative filtering approach for recommendation systems. In 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA) (pp. 89-96). IEEE.</p>
<p>Banik, R. (2018). Hands-On Recommendation Systems with Python: Start building powerful and personalized, recommendation engines with Python. Packt Publishing Ltd.</p>
<p>Hug, N. (2020). Surprise: A Python library for recommender systems. Journal of Open Source Software, 5(52), 2174.</p>
<p>Richert, W. (2013). Building machine learning systems with Python. Packt Publishing Ltd.</p>
</div>
