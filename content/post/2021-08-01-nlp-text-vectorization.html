---
title: NLP - Text Vectorization
author: Michael Fuchs
date: '2021-08-01'
slug: nlp-text-vectorization
categories: []
tags: []
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 5
---


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">1 Introduction</a></li>
<li><a href="#import-the-libraries-and-the-data" id="toc-import-the-libraries-and-the-data">2 Import the Libraries and the Data</a></li>
<li><a href="#text-vectorization" id="toc-text-vectorization">3 Text Vectorization</a>
<ul>
<li><a href="#bag-of-wordsbow" id="toc-bag-of-wordsbow">3.1 Bag-of-Words(BoW)</a>
<ul>
<li><a href="#functionality" id="toc-functionality">3.1.2 Functionality</a></li>
<li><a href="#creation-of-the-final-data-set" id="toc-creation-of-the-final-data-set">3.1.3 Creation of the final Data Set</a></li>
<li><a href="#test-of-a-sample-record" id="toc-test-of-a-sample-record">3.1.4 Test of a Sample Record</a></li>
</ul></li>
<li><a href="#n-grams" id="toc-n-grams">3.2 N-grams</a>
<ul>
<li><a href="#explanation" id="toc-explanation">3.2.1 Explanation</a></li>
<li><a href="#functionality-1" id="toc-functionality-1">3.2.2 Functionality</a>
<ul>
<li><a href="#defining-ngram_range" id="toc-defining-ngram_range">3.2.2.1 Defining ngram_range</a></li>
<li><a href="#defining-max_features" id="toc-defining-max_features">3.2.2.2 Defining max_features</a></li>
</ul></li>
<li><a href="#creation-of-the-final-data-set-1" id="toc-creation-of-the-final-data-set-1">3.2.3 Creation of the final Data Set</a></li>
</ul></li>
<li><a href="#tf-idf" id="toc-tf-idf">3.3 TF-IDF</a>
<ul>
<li><a href="#explanation-1" id="toc-explanation-1">3.3.1 Explanation</a>
<ul>
<li><a href="#mathematical-formulas" id="toc-mathematical-formulas">3.3.1.1 Mathematical Formulas</a></li>
<li><a href="#example-calculation" id="toc-example-calculation">3.3.1.2 Example Calculation</a></li>
<li><a href="#tf-idf-using-scikit-learn" id="toc-tf-idf-using-scikit-learn">3.3.1.3 TF-IDF using scikit-learn</a></li>
</ul></li>
<li><a href="#functionality-2" id="toc-functionality-2">3.3.2 Functionality</a></li>
<li><a href="#creation-of-the-final-data-set-2" id="toc-creation-of-the-final-data-set-2">3.3.3 Creation of the final Data Set</a></li>
</ul></li>
</ul></li>
<li><a href="#best-practice---application-to-the-amazon-data-set" id="toc-best-practice---application-to-the-amazon-data-set">4 Best Practice - Application to the Amazon Data Set</a>
<ul>
<li><a href="#import-the-dataframe" id="toc-import-the-dataframe">4.1 Import the Dataframe</a></li>
<li><a href="#tf-idf-vectorizer" id="toc-tf-idf-vectorizer">4.2 TF-IDF Vectorizer</a></li>
<li><a href="#model-training" id="toc-model-training">4.3 Model Training</a></li>
<li><a href="#tf-idf-vectorizer-with-ngram_range" id="toc-tf-idf-vectorizer-with-ngram_range">4.4 TF-IDF Vectorizer with ngram_range</a></li>
<li><a href="#model-training-ii" id="toc-model-training-ii">4.5 Model Training II</a></li>
<li><a href="#out-of-the-box-data" id="toc-out-of-the-box-data">4.6 Out-Of-The-Box-Data</a></li>
</ul></li>
<li><a href="#conclusion" id="toc-conclusion">5 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>Now that we have cleaned up and prepared our text dataset in the previous posts, we come to the next topic: <strong>Text Vectorization</strong></p>
<p>Most machine learning algorithms cannot handle string variables. We have to convert them into a format that is readable for machine learning algorithms. Text vectorization is the process of converting text into real numbers. These numbers can be used as input to machine learning models.</p>
<p>In the following, I will use a simple example to show several ways in which vectorization can be done.</p>
<p>Finally, I will apply a vectorization method to the dataset (<a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets/NLP/Text%20Pre-Processing%20-%20All%20in%20One">‘Amazon_Unlocked_Mobile_small_pre_processed.csv’</a>) created and processed in the <a href="https://michael-fuchs-python.netlify.app/2021/06/23/nlp-text-pre-processing-all-in-one/">last post</a> and train a machine learning model on it.</p>
</div>
<div id="import-the-libraries-and-the-data" class="section level1">
<h1>2 Import the Libraries and the Data</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score</code></pre>
<pre class="r"><code>df = pd.DataFrame({&#39;Rating&#39;: [2,5,3],
                   &#39;Text&#39;: [&quot;This is a brown horse&quot;,
                            &quot;This horse likes to play&quot;,
                            &quot;The horse is in the stable&quot;]})
df</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p1.png" /></p>
</div>
<div id="text-vectorization" class="section level1">
<h1>3 Text Vectorization</h1>
<div id="bag-of-wordsbow" class="section level2">
<h2>3.1 Bag-of-Words(BoW)</h2>
<p>CountVectorizer() is one of the simplest methods of text vectorization.</p>
<p>It creates a sparse matrix consisting of a set of dummy variables. These indicate whether a certain word occurs in the document or not. The CountVectorizer function matches the word vocabulary, learns it, and creates a document term matrix where the individual cells indicate the frequency of that word in a given document.
This is also called term frequency where the columns are dedicated to each word in the corpus.</p>
<div id="functionality" class="section level3">
<h3>3.1.2 Functionality</h3>
<pre class="r"><code>cv = CountVectorizer()

cv_vectorizer = cv.fit(df[&#39;Text&#39;])
text_cv_vectorized = cv_vectorizer.transform(df[&#39;Text&#39;])

text_cv_vectorized_array = text_cv_vectorized.toarray()

print(text_cv_vectorized_array)
print()
print(text_cv_vectorized_array.shape)</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p2.png" /></p>
<p>10 different words were found in the text corpus. These can also be output as follows:</p>
<pre class="r"><code>cv_vectorizer.get_feature_names_out()</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p3.png" /></p>
<pre class="r"><code>cv_vectorizer.vocabulary_</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p4.png" /></p>
<p>To make the output a bit more readable we can have it displayed as a dataframe:</p>
<pre class="r"><code>cv_vectorized_matrix = pd.DataFrame(text_cv_vectorized.toarray(), 
                                    columns=cv_vectorizer.get_feature_names_out())
cv_vectorized_matrix</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p5.png" /></p>
<p>How are the rows and columns in the matrix shown above to be read?</p>
<ul>
<li>The rows indicate the documents in the corpus and</li>
<li>The columns indicate the tokens in the dictionary</li>
</ul>
</div>
<div id="creation-of-the-final-data-set" class="section level3">
<h3>3.1.3 Creation of the final Data Set</h3>
<p>Finally, I create a new data set on which to train machine learning algorithms. This time I use the generated array directly to create the final data frame:</p>
<pre class="r"><code>cv_df = pd.DataFrame(text_cv_vectorized_array, 
                     columns = cv_vectorizer.get_feature_names_out()).add_prefix(&#39;Counts_&#39;)

df_new_cv = pd.concat([df, cv_df], axis=1, sort=False)
df_new_cv</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p6.png" /></p>
<p>However, the bag-of-words method also has two crucial disadvantages:</p>
<ul>
<li>BoW does not preserve the order of words and</li>
<li>It does not allow to draw useful conclusions for downstream NLP tasks</li>
</ul>
</div>
<div id="test-of-a-sample-record" class="section level3">
<h3>3.1.4 Test of a Sample Record</h3>
<p>Let’s test a sample record:</p>
<pre class="r"><code>new_input = [&quot;Hi this is Mikel.&quot;]
new_input</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p7.png" /></p>
<pre class="r"><code>new_input_cv_vectorized = cv_vectorizer.transform(new_input)
new_input_cv_vectorized_array = new_input_cv_vectorized.toarray()
new_input_cv_vectorized_array</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p8.png" /></p>
<pre class="r"><code>new_input_matrix = pd.DataFrame(new_input_cv_vectorized_array, 
                                columns = cv_vectorizer.get_feature_names_out())

new_input_matrix</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p9.png" /></p>
<p>The words ‘is’ and ‘this’ have been learned by the CountVectorizer and thus get a count here.</p>
<pre class="r"><code>new_input = [&quot;You say goodbye and I say hello&quot;, &quot;hello world&quot;]
new_input</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p10.png" /></p>
<pre class="r"><code>new_input_cv_vectorized = cv_vectorizer.transform(new_input)
new_input_cv_vectorized_array = new_input_cv_vectorized.toarray()
new_input_cv_vectorized_array</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p11.png" /></p>
<pre class="r"><code>new_input_matrix = pd.DataFrame(new_input_cv_vectorized_array, 
                                columns = cv_vectorizer.get_feature_names_out())

new_input_matrix</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p12.png" /></p>
<p>In our second example, I did not use any of the words CountVectorizer learned. Therefore all values are 0.</p>
</div>
</div>
<div id="n-grams" class="section level2">
<h2>3.2 N-grams</h2>
<div id="explanation" class="section level3">
<h3>3.2.1 Explanation</h3>
<p>First of all, what are n-grams?
In a nutshell: an N-gram means a sequence of N words.
So for example, “Hi there” is a 2-gram (a bigram), “Hello sunny world” is a 3-gram (trigram) and “Hi this is Mikel” is a 4-gram.</p>
<p>How would this look when vectorizing a text corpus?</p>
<p><em>Example</em>: “A horse rides on the beach.”</p>
<ul>
<li>Unigram (1-gram): A, horse, rides, on, the, beach</li>
<li>Bigram (2-gram): A horse, horse rides, rides on, …</li>
<li>Trigram (3-gram): A horse rides, horse rides on, …</li>
</ul>
<p>Unlike BoW, n-gram maintains word order.
They can also be created with the CountVectorizer() function. For this only the ngram_range parameter must be adjusted.</p>
<p>An ngram_range of:</p>
<ul>
<li>(1, 1) means only unigrams</li>
<li>(1, 2) means unigrams and bigrams</li>
<li>(2, 2) means only bigrams</li>
<li>(1, 3) means unigrams, bigrams and trigrams …</li>
</ul>
<p>Here a short example of this:</p>
<pre class="r"><code>example_sentence = [&quot;A horse rides on the beach.&quot;]
example_sentence</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p13.png" /></p>
<pre class="r"><code>cv_ngram = CountVectorizer(ngram_range=(1, 3))

cv_ngram_vectorizer = cv_ngram.fit(example_sentence)
cv_ngram_vectorizer.get_feature_names_out()</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p14.png" /></p>
<pre class="r"><code>cv_ngram = CountVectorizer(ngram_range=(2, 3))

cv_ngram_vectorizer = cv_ngram.fit(example_sentence)
cv_ngram_vectorizer.get_feature_names_out()</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p15.png" /></p>
</div>
<div id="functionality-1" class="section level3">
<h3>3.2.2 Functionality</h3>
<div id="defining-ngram_range" class="section level4">
<h4>3.2.2.1 Defining ngram_range</h4>
<p>Now that we know how the CountVectorizer works with the ngram_range parameter, we will apply it to our sample dataset:</p>
<pre class="r"><code>cv_ngram = CountVectorizer(ngram_range=(1, 3))

cv_ngram_vectorizer = cv_ngram.fit(df[&#39;Text&#39;])
text_cv_ngram_vectorized = cv_ngram_vectorizer.transform(df[&#39;Text&#39;])

text_cv_ngram_vectorized_array = text_cv_ngram_vectorized.toarray()

print(cv_ngram_vectorizer.get_feature_names_out())</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p16.png" /></p>
<p>The disadvantage of n-gram is that it usually generates too many features and is therefore very computationally expensive. One way to counteract this is to limit the maximum number of features. This can be done with the max_features parameter.</p>
</div>
<div id="defining-max_features" class="section level4">
<h4>3.2.2.2 Defining max_features</h4>
<pre class="r"><code>cv_ngram = CountVectorizer(ngram_range=(1, 3),
                           max_features=15)

cv_ngram_vectorizer = cv_ngram.fit(df[&#39;Text&#39;])
text_cv_ngram_vectorized = cv_ngram_vectorizer.transform(df[&#39;Text&#39;])

text_cv_ngram_vectorized_array = text_cv_ngram_vectorized.toarray()

print(cv_ngram_vectorizer.get_feature_names_out())</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p17.png" /></p>
<pre class="r"><code>cv_ngram_vectorized_matrix = pd.DataFrame(text_cv_ngram_vectorized.toarray(), 
                                          columns=cv_ngram_vectorizer.get_feature_names_out())
cv_ngram_vectorized_matrix</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p18.png" /></p>
<p>That worked out well. But what I always try to avoid with column names are spaces between the words. But this can be easily corrected:</p>
<pre class="r"><code>cv_ngram_vectorized_matrix_columns_list = cv_ngram_vectorized_matrix.columns.to_list()

k = []

for i in cv_ngram_vectorized_matrix_columns_list:
    j = i.replace(&#39; &#39;,&#39;_&#39;)
    k.append(j)

cv_ngram_vectorized_matrix.columns = [k]
cv_ngram_vectorized_matrix</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p19.png" /></p>
</div>
</div>
<div id="creation-of-the-final-data-set-1" class="section level3">
<h3>3.2.3 Creation of the final Data Set</h3>
<pre class="r"><code>cv_ngram_df = pd.DataFrame(text_cv_ngram_vectorized_array, 
                           columns = cv_ngram_vectorizer.get_feature_names_out()).add_prefix(&#39;Counts_&#39;)

df_new_cv_ngram = pd.concat([df, cv_ngram_df], axis=1, sort=False)
df_new_cv_ngram.T</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p20.png" /></p>
</div>
</div>
<div id="tf-idf" class="section level2">
<h2>3.3 TF-IDF</h2>
<div id="explanation-1" class="section level3">
<h3>3.3.1 Explanation</h3>
<p>TF-IDF stands for Term Frequency - Inverse Document Frequency . It’s a statistical measure of how relevant a word is with respect to a document in a collection of documents.</p>
<p>TF-IDF consists of two components:</p>
<ul>
<li>Term frequency (TF): The number of times the word occurs in the document</li>
<li>Inverse Document Frequency (IDF): A weighting that indicates how common or rare a word is in the overall document set.</li>
</ul>
<p>Multiplying TF and IDF results in the TF-IDF score of a word in a document. The higher the score, the more relevant that word is in that particular document.</p>
<div id="mathematical-formulas" class="section level4">
<h4>3.3.1.1 Mathematical Formulas</h4>
<p>TF-IDF is therefore the product of TF and IDF:</p>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135s1.png" /></p>
<p>where TF computes the term frequency:</p>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135s2.png" /></p>
<p>and IDF computes the inverse document frequency:</p>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135s3.png" /></p>
</div>
<div id="example-calculation" class="section level4">
<h4>3.3.1.2 Example Calculation</h4>
<p>Here is a simple example. Let’s assume we have the following collection of documents D:</p>
<ul>
<li>Doc1: “I said please and you said thanks”</li>
<li>Doc2: “please darling please”</li>
<li>Doc3: “please thanks”</li>
</ul>
<p>The calculation of TF, IDF and TF-IDF is shown in the table below:</p>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135s4.png" /></p>
<p>Let’s take a closer look at the values to understand the calculation. The word ‘said’ appears twice in the first document, and the total number of words in Doc1 is 7.</p>
<p>The TF value is therefore 2/7.</p>
<p>In the other two documents ‘said’ is not present at all. This results in an IDF value of log(3/1), since there are a total of three documents in the collection and the word ‘said’ appears in one of the three.</p>
<p>The calculation of the TF-IDF value is therefore as follows:</p>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135s5.png" /></p>
<p>If you look at the values for ‘please’, you will see that this word appears (sometimes several times) in all documents. It is therefore considered common and receives a TF-IDF value of 0.</p>
</div>
<div id="tf-idf-using-scikit-learn" class="section level4">
<h4>3.3.1.3 TF-IDF using scikit-learn</h4>
<p>Below I will use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer">TF-IDF vectorizer from scikit-learn</a>, which has two small modifications to the original formula.</p>
<p>The calculation of IDF is as follows:</p>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135s6.png" /></p>
<p>Here, 1 is added to the numerator and to the denominator. This is to avoid the computational problem of dividing by 0. We also need to add a 1 to the numerator to balance the effect of adding 1 to the denominator.</p>
<p>The second modification is in the calculation of TF-IDF values:</p>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135s7.png" /></p>
<p>Here, a 1 is again added to IDF so that a zero value of IDF does not result in a complete suppression of TF-IDF.
Using the TfidfVectorizer() function on our sample collection clearly shows this effect:</p>
<pre class="r"><code>documents = [&#39;I said please and you said thanks&#39;,
             &#39;please darling please&#39;,
             &#39;please thanks&#39;]

tf_idf = TfidfVectorizer()
tf_idf_vectorizer = tf_idf.fit(documents)
documents_tf_idf_vectorized = tf_idf_vectorizer.transform(documents)

documents_tf_idf_vectorized_array = documents_tf_idf_vectorized.toarray()


tf_idf_vectorized_matrix = pd.DataFrame(documents_tf_idf_vectorized.toarray(), 
                                        columns=tf_idf_vectorizer.get_feature_names_out())
tf_idf_vectorized_matrix = tf_idf_vectorized_matrix[[&#39;said&#39;, &#39;please&#39;, &#39;and&#39;, &#39;you&#39;, &#39;thanks&#39;, &#39;darling&#39;]]
tf_idf_vectorized_matrix.T</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p21.png" /></p>
<p>Here again for comparison the values calculated using the original formula:</p>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135s4.png" /></p>
<p>As we can see here very nicely the values for the word ‘please’ were not completely suppressed during the calculation by TfidfVectorizer().</p>
<p>However, the interpretation of TF-IDF remain exactly the same despite these minor adjustments.</p>
<p>Furthermore, the word ‘I’ was not included, because scikit-learn’s vectorizer automatically disregards words with a length of one letter.</p>
<p><strong>Hint:</strong></p>
<p>Scikit-learn also provides the TfidfTransformer() function.
But it needs the customized output of CountVectorize as input to calculate the TF-IDF values, see <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer">here</a>. In almost all cases you can use TfidfVectorizer directly.</p>
</div>
</div>
<div id="functionality-2" class="section level3">
<h3>3.3.2 Functionality</h3>
<pre class="r"><code>tf_idf = TfidfVectorizer()
tf_idf_vectorizer = tf_idf.fit(df[&#39;Text&#39;])
text_tf_idf_vectorized = tf_idf_vectorizer.transform(df[&#39;Text&#39;])

text_tf_idf_vectorized_array = text_tf_idf_vectorized.toarray()


tf_idf_vectorized_matrix = pd.DataFrame(text_tf_idf_vectorized.toarray(), 
                                        columns=tf_idf_vectorizer.get_feature_names_out())

tf_idf_vectorized_matrix</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p22.png" /></p>
</div>
<div id="creation-of-the-final-data-set-2" class="section level3">
<h3>3.3.3 Creation of the final Data Set</h3>
<pre class="r"><code>tf_idf_df = pd.DataFrame(text_tf_idf_vectorized_array, 
                         columns = tf_idf_vectorizer.get_feature_names_out()).add_prefix(&#39;TF-IDF_&#39;)

df_new_tf_idf = pd.concat([df, tf_idf_df], axis=1, sort=False)
df_new_tf_idf.T</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p23.png" /></p>
</div>
</div>
</div>
<div id="best-practice---application-to-the-amazon-data-set" class="section level1">
<h1>4 Best Practice - Application to the Amazon Data Set</h1>
<p>As mentioned in the introduction, I will now apply a vectorizer to the dataset <em>Amazon_Unlocked_Mobile_small_pre_processed.csv</em> that I prepared in the <a href="https://michael-fuchs-python.netlify.app/2021/06/23/nlp-text-pre-processing-all-in-one/">last post</a>. Afterwards, I will train a machine learning model on it.</p>
<p>Feel free to download the dataset from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets/NLP/Text%20Pre-Processing%20-%20All%20in%20One">GitHub Repository</a>.</p>
<div id="import-the-dataframe" class="section level2">
<h2>4.1 Import the Dataframe</h2>
<pre class="r"><code>url = &quot;https://raw.githubusercontent.com/MFuchs1989/Datasets-and-Miscellaneous/main/datasets/NLP/Text%20Pre-Processing%20-%20All%20in%20One/Amazon_Unlocked_Mobile_small_pre_processed.csv&quot;

df_amazon = pd.read_csv(url, error_bad_lines=False)
# Conversion of the desired column to the correct data type
df_amazon[&#39;Reviews_cleaned_wo_rare_words&#39;] = df_amazon[&#39;Reviews_cleaned_wo_rare_words&#39;].astype(&#39;str&#39;)
df_amazon.head(3).T</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p24.png" /></p>
<p>I have already prepared the data set in various ways. You can read about the exact steps here: <a href="https://michael-fuchs-python.netlify.app/2021/06/23/nlp-text-pre-processing-all-in-one/">NLP - Text Pre-Processing - All in One</a></p>
<p>I will apply the TF-IDF vectorizer to the ‘Reviews_cleaned_wo_rare_words’ column. For this I will create a subset of the original dataframe. Feel free to try the TF-IDF (or any other vectorizer) on the other processed columns and compare the performance of the ML algorithms.</p>
<pre class="r"><code>df_amazon_subset = df_amazon[[&#39;Label&#39;, &#39;Reviews_cleaned_wo_rare_words&#39;]]
df_amazon_subset</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p25.png" /></p>
<pre class="r"><code>x = df_amazon_subset.drop([&#39;Label&#39;], axis=1)
y = df_amazon_subset[&#39;Label&#39;]

trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
</div>
<div id="tf-idf-vectorizer" class="section level2">
<h2>4.2 TF-IDF Vectorizer</h2>
<p>As with scaling or encoding, the .fit command is applied only to the training part. Using these stored metrics, trainX as well as testX is then vectorized.</p>
<p>I still used the additional function <code>.values.astype('U')</code> in the code below. This would not have been necessary at this point, because I already assigned the correct data type to the column ‘Reviews_cleaned_wo_rare_words’ when loading the dataset.
To be on the safe side that TfidfVectorizer works, this code part can be kept.</p>
<pre class="r"><code>tf_idf = TfidfVectorizer()
tf_idf_vectorizer = tf_idf.fit(trainX[&#39;Reviews_cleaned_wo_rare_words&#39;].values.astype(&#39;U&#39;))

trainX_tf_idf_vectorized = tf_idf_vectorizer.transform(trainX[&#39;Reviews_cleaned_wo_rare_words&#39;].values.astype(&#39;U&#39;))
testX_tf_idf_vectorized = tf_idf_vectorizer.transform(testX[&#39;Reviews_cleaned_wo_rare_words&#39;].values.astype(&#39;U&#39;))


trainX_tf_idf_vectorized_array = trainX_tf_idf_vectorized.toarray()
testX_tf_idf_vectorized_array = testX_tf_idf_vectorized.toarray()

print(&#39;Number of features generated: &#39; + str(len(tf_idf_vectorizer.get_feature_names_out())))</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p26.png" /></p>
<p>The next step is actually not necessary, since the machine learning models can handle arrays wonderfully.</p>
<pre class="r"><code>trainX_tf_idf_vectorized_final = pd.DataFrame(trainX_tf_idf_vectorized_array, 
                                              columns = tf_idf_vectorizer.get_feature_names_out()).add_prefix(&#39;TF-IDF_&#39;)

testX_tf_idf_vectorized_final = pd.DataFrame(testX_tf_idf_vectorized_array, 
                                             columns = tf_idf_vectorizer.get_feature_names_out()).add_prefix(&#39;TF-IDF_&#39;)</code></pre>
</div>
<div id="model-training" class="section level2">
<h2>4.3 Model Training</h2>
<p>In the following I will use the Support Vector Machine classifier. Of course you can also try any other one.</p>
<pre class="r"><code>clf = SVC(kernel=&#39;linear&#39;)
clf.fit(trainX_tf_idf_vectorized_final, trainY)

y_pred = clf.predict(testX_tf_idf_vectorized_final)</code></pre>
<pre class="r"><code>confusion_matrix = confusion_matrix(testY, y_pred)
print(confusion_matrix)</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p27.png" /></p>
<pre class="r"><code>print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred)))</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p28.png" /></p>
</div>
<div id="tf-idf-vectorizer-with-ngram_range" class="section level2">
<h2>4.4 TF-IDF Vectorizer with ngram_range</h2>
<p>The TF-IDF Vectorizer can also be used in combination with n-grams. It has been shown in practice that the use of the parameter <code>analyser='char'</code> in combination with <code>ngram_range</code> not only generates fewer features, which is less computationally intensive, but also often provides the better result.</p>
<pre class="r"><code>tf_idf_ngram = TfidfVectorizer(analyzer=&#39;char&#39;,
                               ngram_range=(2, 3))

tf_idf_ngram_vectorizer = tf_idf_ngram.fit(trainX[&#39;Reviews_cleaned_wo_rare_words&#39;].values.astype(&#39;U&#39;))

trainX_tf_idf_ngram_vectorized = tf_idf_ngram_vectorizer.transform(trainX[&#39;Reviews_cleaned_wo_rare_words&#39;].values.astype(&#39;U&#39;))
testX_tf_idf_ngram_vectorized = tf_idf_ngram_vectorizer.transform(testX[&#39;Reviews_cleaned_wo_rare_words&#39;].values.astype(&#39;U&#39;))


trainX_tf_idf_ngram_vectorized_array = trainX_tf_idf_ngram_vectorized.toarray()
testX_tf_idf_ngram_vectorized_array = testX_tf_idf_ngram_vectorized.toarray()

print(&#39;Number of features generated: &#39; + str(len(tf_idf_ngram_vectorizer.get_feature_names_out())))</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p29.png" /></p>
<pre class="r"><code>trainX_tf_idf_ngram_vectorized_final = pd.DataFrame(trainX_tf_idf_ngram_vectorized_array, 
                                              columns = tf_idf_ngram_vectorizer.get_feature_names_out()).add_prefix(&#39;TF-IDF_ngram_&#39;)

testX_tf_idf_ngram_vectorized_final = pd.DataFrame(testX_tf_idf_ngram_vectorized_array, 
                                              columns = tf_idf_ngram_vectorizer.get_feature_names_out()).add_prefix(&#39;TF-IDF_ngram_&#39;)</code></pre>
</div>
<div id="model-training-ii" class="section level2">
<h2>4.5 Model Training II</h2>
<pre class="r"><code>clf2 = SVC(kernel=&#39;linear&#39;)
clf2.fit(trainX_tf_idf_ngram_vectorized_final, trainY)

y_pred2 = clf2.predict(testX_tf_idf_ngram_vectorized_final)</code></pre>
<pre class="r"><code>y_pred2 = clf2.predict(testX_tf_idf_ngram_vectorized_final)</code></pre>
<pre class="r"><code>confusion_matrix2 = confusion_matrix(testY, y_pred2)
print(confusion_matrix2)</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p30.png" /></p>
<pre class="r"><code>print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred2)))</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p31.png" /></p>
<p>Unfortunately, the performance did not increase. Therefore, we use the first TF-IDF Vectorizer as well as the first ML model.</p>
</div>
<div id="out-of-the-box-data" class="section level2">
<h2>4.6 Out-Of-The-Box-Data</h2>
<p>Finally, I’d like to test some self-generated evaluation comments and see what the model predicts.</p>
<p>Normally, all pre-processing steps that took place during model training should also be applied to new data. These were (to be read in the post <a href="https://michael-fuchs-python.netlify.app/2021/06/23/nlp-text-pre-processing-all-in-one/">NLP - Text Pre-Processing - All in One</a>):</p>
<ul>
<li>Text Cleaning
<ul>
<li>Conversion to Lower Case</li>
<li>Removing HTML-Tags</li>
<li>Removing URLs</li>
<li>Removing Accented Characters</li>
<li>Removing Punctuation</li>
<li>Removing irrelevant Characters (Numbers and Punctuation)</li>
<li>Removing extra Whitespaces</li>
</ul></li>
<li>Tokenization</li>
<li>Removing Stop Words</li>
<li>Normalization</li>
<li>Removing Single Characters</li>
<li>Removing specific Words</li>
<li>Removing Rare words</li>
</ul>
<p>For simplicity, I’ll omit these steps for this example, since I used simple words without punctuation or special characters.</p>
<pre class="r"><code>my_rating_comment = [&quot;a great device anytime again&quot;, 
                     &quot;has poor reception and a too small display&quot;, 
                     &quot;goes like this to some extent has a lot of good but also negative&quot;]</code></pre>
<p>Here is the vectorized data set:</p>
<pre class="r"><code>my_rating_comment_vectorized = tf_idf_vectorizer.transform(my_rating_comment)
my_rating_comment_vectorized_array = my_rating_comment_vectorized.toarray()
my_rating_comment_df = pd.DataFrame(my_rating_comment_vectorized_array, 
                                    columns = tf_idf_vectorizer.get_feature_names_out())
my_rating_comment_df</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p32.png" /></p>
<p>To be able to see which words from my_rating_comment were in the learned vocabulary of the vectorizer (and consequently received a TF-IDF score) I filter the dataset:</p>
<pre class="r"><code>my_rating_comment_df_filtered = my_rating_comment_df.loc[:, (my_rating_comment_df != 0).any(axis=0)]
my_rating_comment_df_filtered</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p33.png" /></p>
<p>Ok let’s predict:</p>
<pre class="r"><code>y_pred_my_rating = clf.predict(my_rating_comment_df)</code></pre>
<p>Here is the final result:</p>
<pre class="r"><code>my_rating_comment_df_final = pd.DataFrame (my_rating_comment, columns = [&#39;My_Rating&#39;])
my_rating_comment_df_final[&#39;Prediction&#39;] = y_pred_my_rating
my_rating_comment_df_final</code></pre>
<p><img src="/post/2021-08-01-nlp-text-vectorization_files/p135p34.png" /></p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>5 Conclusion</h1>
<p>In this post I showed how to generate readable input from text data for machine learning algorithms. Furthermore, I applied a vectorizer to the previously created and cleaned dataset and trained a machine learning model on it. Finally, I showed how to make new predictions using the trained model.</p>
</div>
