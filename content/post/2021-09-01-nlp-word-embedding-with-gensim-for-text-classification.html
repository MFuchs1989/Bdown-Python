---
title: NLP - Word Embedding with GENSIM for Text-Classification
author: Michael Fuchs
date: '2021-09-01'
slug: nlp-word-embedding-with-gensim-for-text-classification
categories: []
tags: []
output:
  blogdown::html_page:
    toc: yes
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#import-the-libraries-and-the-data">2 Import the Libraries and the Data</a></li>
<li><a href="#gensim---word2vec">3 Gensim - Word2Vec</a>
<ul>
<li><a href="#instantiation">3.1 Instantiation</a></li>
<li><a href="#exploration-of-the-calculated-values">3.2 Exploration of the calculated Values</a></li>
<li><a href="#generation-of-aggregated-sentence-vectors">3.3 Generation of aggregated Sentence Vectors</a></li>
<li><a href="#generation-of-averaged-sentence-vectors">3.4 Generation of averaged Sentence Vectors</a></li>
<li><a href="#model-training">3.5 Model Training</a></li>
<li><a href="#processing-of-new-input">3.6 Processing of new Input</a>
<ul>
<li><a href="#load-the-word2vec-model">3.6.1 Load the Word2Vec Model</a></li>
<li><a href="#load-the-new-input">3.6.2 Load the new Input</a></li>
<li><a href="#pre-processing-of-the-new-input">3.6.3 Pre-Processing of the new Input</a></li>
<li><a href="#model-predictions">3.6.4 Model Predictions</a></li>
</ul></li>
<li><a href="#updating-the-word2vec-model">3.7 Updating the Word2Vec Model</a>
<ul>
<li><a href="#updating-the-weights">3.7.1 Updating the Weights</a></li>
<li><a href="#updating-the-weights-and-the-vocabulary">3.7.2 Updating the Weights and the Vocabulary</a></li>
</ul></li>
<li><a href="#final-saving-of-the-word2vec-model">3.8 Final saving of the Word2Vec Model</a>
<ul>
<li><a href="#saving-the-entire-model">3.8.1 Saving the entire Model</a></li>
<li><a href="#saving-the-keyedvectors">3.8.2 Saving the KeyedVectors</a></li>
</ul></li>
</ul></li>
<li><a href="#gensim---doc2vec">4 Gensim - Doc2Vec</a>
<ul>
<li><a href="#instantiation-1">4.1 Instantiation</a></li>
<li><a href="#updating-the-doc2vec-model">4.2 Updating the Doc2Vec Model</a></li>
<li><a href="#generation-of-aggregated-sentence-vectors-1">4.3 Generation of aggregated Sentence Vectors</a></li>
<li><a href="#generation-of-averaged-sentence-vectors-1">4.4 Generation of averaged Sentence Vectors</a></li>
<li><a href="#final-saving-of-the-doc2vec-model">4.5 Final saving of the Doc2Vec Model</a></li>
</ul></li>
<li><a href="#gensim---fasttext">5 Gensim - FastText</a></li>
<li><a href="#phrases-phraser">6 Phrases &amp; Phraser</a>
<ul>
<li><a href="#detecting-phrases">6.1 Detecting Phrases</a></li>
<li><a href="#updating-the-phrases-model">6.2 Updating the Phrases Model</a></li>
<li><a href="#safe-load">6.3 Safe &amp; Load</a></li>
<li><a href="#creation-of-the-final-dataframe">6.4 Creation of the final dataframe</a></li>
</ul></li>
<li><a href="#use-pre-trained-models">7 Use Pre-Trained Models</a></li>
<li><a href="#conclusion">8 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>In my last post (<a href="https://michael-fuchs-python.netlify.app/2021/08/01/nlp-text-vectorization/">NLP - Text Vectorization</a>) I showed how to convert words from a text corpus into real numbers to make them readable for machine learning algorithms.
The problem with BoW or TF-IDF vectorization is that the semantics of the word are not encoded in its vector representation. For example, the words “mobile phone” and “cell phone” have similar meanings, but BoW or TF-IDF does not take this into account and ignores the meaning of the words.</p>
<p>And that’s where Word Embedding comes in.</p>
<p>Word embedding is one of the most popular language modeling techniques used to map words onto vectors of real numbers. It is able to represent words or phrases in a vector space with multiple dimensions that have semantic and syntactic similarity. Word embeddings can be created using various methods.</p>
<p><strong>In contrast to BoW or TF-IDF, the word embedding approach vectorizes a word, placing words that have similar meanings closer together</strong>. For example, the words “mobile phone” and “cell phone” would have a similar vector representation. This means that when the word is embedded, the meaning of the word is encoded in the vector.</p>
<p>The motivation behind converting text into semantic vectors is that the word embedding method is not only able to extract the semantic relations, but it should also preserve most of the relevant information about a text corpus.</p>
</div>
<div id="import-the-libraries-and-the-data" class="section level1">
<h1>2 Import the Libraries and the Data</h1>
<pre class="r"><code>import pandas as pd
import numpy as np
import pickle as pk

from nltk.tokenize import word_tokenize
from sklearn.svm import SVC
import statistics

from gensim.models import Word2Vec
from gensim.models import KeyedVectors

from gensim.models.doc2vec import Doc2Vec
from gensim.models.doc2vec import TaggedDocument

from gensim.models import Phrases
from gensim.models.phrases import Phraser

import gensim.downloader as api</code></pre>
<pre class="r"><code>df = pd.DataFrame({&#39;Rating&#39;: [3,5,1,2],
                   &#39;Text&#39;: [&quot;I love sunflowers&quot;,
                            &quot;Sunflowers fill my heart with joy&quot;,
                            &quot;I love to look into the garden and see the flowers&quot;,
                            &quot;Flowers especially sunflowers are the most beautiful&quot;]})
df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p1.png" /></p>
<p>Here I will quickly create tokens from our sample dataset, since Word2Vec from gensim can only do well with them.</p>
<pre class="r"><code>df[&#39;Text_Tokenized&#39;] = df[&#39;Text&#39;].str.lower().apply(word_tokenize)
df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p2.png" /></p>
</div>
<div id="gensim---word2vec" class="section level1">
<h1>3 Gensim - Word2Vec</h1>
<p>Word2Vec from gensim is one of the most popular techniques for learning word embeddings using a flat neural network. It can be used with two methods:</p>
<ul>
<li>CBOW (Common Bag Of Words): Using the context to predict a target word</li>
<li>Skip Gram: Using a word to predict a target context</li>
</ul>
<p>The corresponding layer structure looks like this:</p>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136s1.png" /></p>
<p>Source: <a href="https://arxiv.org/pdf/1301.3781.pdf">Mikolov T., Chen K., Corrado G. &amp; Dean J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</a></p>
<p>When should which method be used?</p>
<p><strong>CBOW is faster</strong> and can therefore be used well for large data sets. In addition, the representation of more frequent words is better than with Skip Gram.</p>
<p>The <strong>Skip Gram</strong> method works well for smaller data sets and can <strong>represent rare words well</strong>.</p>
<div id="instantiation" class="section level2">
<h2>3.1 Instantiation</h2>
<p>First of all I instantiate the Word2Vec model.
I use the following parameters:</p>
<ul>
<li><code>vector_size</code>: Determines the size of the vectors we want</li>
<li><code>window</code>: Determines the number of words before and after the target word to be considered as context for the word</li>
<li><code>min_count</code>: Determines the number of times a word must occur in the text corpus for a word vector to be created</li>
</ul>
<p>The exact meaning of these and other parameters can be read here: <a href="https://radimrehurek.com/gensim/models/word2vec.html">GENSIM - Word2vec embeddings</a></p>
<p>I deliberately assign the vector size (vector_size) to its own variable here, since I will need it again at a later time.</p>
<pre class="r"><code>vector_size_n_w2v = 5

w2v_model = Word2Vec(vector_size=vector_size_n_w2v,
                     window=3,
                     min_count=1,
                     sg=0) # 0=CBOW, 1=Skip-gram

print(w2v_model)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p3.png" /></p>
<p>Now the creation of the vocabulary, which is to be learned by Word2Vec, takes place.</p>
<pre class="r"><code>w2v_model.build_vocab(df[&#39;Text_Tokenized&#39;])
print(w2v_model)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p4.png" /></p>
<p>Finally, the neural network is trained (here with the CBOW method) over 5 epochs.</p>
<pre class="r"><code>w2v_model.train(df[&#39;Text_Tokenized&#39;], 
                total_examples=w2v_model.corpus_count, 
                epochs=5)</code></pre>
<p>These steps do not necessarily have to be performed individually. If the following syntax is used (with the specification of the text corpus), the creation of the vocabulary as well as the training is carried out automatically.</p>
<p>However, it is recommended to perform the steps separately, as some parameter settings can be made in each case. You can also read all about it here: <a href="https://radimrehurek.com/gensim/models/word2vec.html">GENSIM - Word2vec embeddings</a></p>
<pre class="r"><code>vector_size_n_w2v = 5

w2v_model = Word2Vec(df[&#39;Text_Tokenized&#39;],
                     vector_size=vector_size_n_w2v,
                     window=3,
                     min_count=1,
                     sg=0, # 0=CBOW, 1=Skip-gram
                     epochs=5)

print(w2v_model)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p5.png" /></p>
<p>By supplying sentences directly in the original instantiation call, you essentially asked that one call to also do the build_vocab() &amp; train() steps automatically using those sentences.</p>
<p>It is recommended to save the Word2Vec model immediately after a training session, as this is usually a very time-consuming task. Furthermore, I also save the metric that was used for the <code>vector_size</code> parameter.</p>
<pre class="r"><code>w2v_model.save(&quot;word2vec/word2vec_model&quot;)

pk.dump(vector_size_n_w2v, open(&#39;word2vec/vector_size_w2v_metric.pkl&#39;, &#39;wb&#39;))</code></pre>
</div>
<div id="exploration-of-the-calculated-values" class="section level2">
<h2>3.2 Exploration of the calculated Values</h2>
<p>In the following I show a few commands how to display the calculated and stored values.</p>
<pre class="r"><code># The learned vocabulary:
w2v_model.wv.index_to_key</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p6.png" /></p>
<pre class="r"><code># Length of the learned vocabulary:
len(w2v_model.wv.index_to_key)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p7.png" /></p>
<pre class="r"><code># Output of the calculated vector for a given word from the vocabulary:
w2v_model.wv[&#39;sunflowers&#39;]</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p8.png" /></p>
<pre class="r"><code># Length of the calculated vector:
len(w2v_model.wv[&#39;sunflowers&#39;])</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p9.png" /></p>
<pre class="r"><code># Display the words that are most similar to a given word from the vocabulary:
w2v_model.wv.most_similar(&#39;sunflowers&#39;)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p10.png" /></p>
</div>
<div id="generation-of-aggregated-sentence-vectors" class="section level2">
<h2>3.3 Generation of aggregated Sentence Vectors</h2>
<p>Now we will generate aggregate sentence vectors based on the word vectors for each word in the given sentence.</p>
<pre class="r"><code>words = set(w2v_model.wv.index_to_key )
df[&#39;Text_vect&#39;] = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])
                         for ls in df[&#39;Text_Tokenized&#39;]])


df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p11.png" /></p>
<p>Since we used <code>min_count=1</code> in the model training, each word was learned by Word2Vec and got a vector (with length of 5).</p>
<p>Unfortunately, our example sentences have different numbers of words, so the database is correspondingly heterogeneous:</p>
<pre class="r"><code>for i, v in enumerate(df[&#39;Text_vect&#39;]):
    print(len(df[&#39;Text_Tokenized&#39;].iloc[i]), len(v))</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p12.png" /></p>
<p>As we can see from the output shown above, the first sentence from the dataset is assigned three vectors (each with a length of 5). This would result in a count of 15 features.</p>
<p>The second sentence would be assigned 30 features (6*5).</p>
<p>However, a machine learning model wants to see a consistent set of features for each example. Currently, a model training error would be thrown out.</p>
<p>To get the number of features equal, the next step is to calculate an element-wise average of the different vectors assigned to a sentence. To explain in more detail how this should work, let’s look at the three vectors of the first sentence:</p>
<pre class="r"><code>content_sentence1_Text_vect = list(df[&#39;Text_vect&#39;].loc[0:0])
content_sentence1_Text_vect</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p13.png" /></p>
<p>Each of these word vectors has a size of 5 because we set it that way when we trained our Word2Vec model. Now we will average the first element of these three word vectors and store it as the first entry in our final vector. We do this for all further elements so that we finally get a final vector with length 5 which describes the sentence.</p>
<p>I have listed this calculation manually once for our present example:</p>
<pre class="r"><code>element1 = [0.1476101,
            -0.03632087,
            -0.01072454]

element2 = [-0.03066945,
            0.05751216,
            0.0047286]

element3 = [-0.09073229,
            0.01985285,
            0.10206699]

element4 = [0.13108101,
            -0.16571797,
            0.18018547]

element5 = [-0.09720321,
            -0.18894958,
            -0.186059]

element1_mean = statistics.mean(element1)
element2_mean = statistics.mean(element2)
element3_mean = statistics.mean(element3)
element4_mean = statistics.mean(element4)
element5_mean = statistics.mean(element5)


manually_calculated_mean_values = [[element1_mean, element2_mean, element3_mean, element4_mean, element5_mean]]

manually_calculated_mean_values_df = pd.DataFrame(manually_calculated_mean_values)
manually_calculated_mean_values_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p14.png" /></p>
<p>This is now the expected output for our first record.</p>
<p>Since I don’t want to do this manual calculation for each record separately (a vector size of 5 is very small and in real life you have more than 4 entries in the data set) I use a for-loop for this.</p>
</div>
<div id="generation-of-averaged-sentence-vectors" class="section level2">
<h2>3.4 Generation of averaged Sentence Vectors</h2>
<p>As described above, I will now generate sentence vectors based on the averaging of the word vectors for the words contained in the sentence. If you can remember I assigned the vector size to a separate variable (‘vector_size_n’) during instantiation. This is needed again in the else statement of the following for-loop.</p>
<pre class="r"><code>text_vect_avg = []
for v in df[&#39;Text_vect&#39;]:
    if v.size:
        text_vect_avg.append(v.mean(axis=0))
    else:
        text_vect_avg.append(np.zeros(vector_size_n, dtype=float)) # the same vector size must be used here as for model training
        
        
df[&#39;Text_vect_avg&#39;] = text_vect_avg
df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p15.png" /></p>
<p>Let’s check again if the vector lengths are now consistent:</p>
<pre class="r"><code># Are our sentence vector lengths consistent?
for i, v in enumerate(df[&#39;Text_vect_avg&#39;]):
    print(len(df[&#39;Text_Tokenized&#39;].iloc[i]), len(v))</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p16.png" /></p>
<p>A machine learning algorithm cannot work directly with the column generated above (‘Text_vect_avg’). I have added it to the dataset for completeness. However, I continue to work with the created dictionary ‘text_vect_avg’.</p>
<pre class="r"><code>df_Machine_Learning = pd.DataFrame(text_vect_avg)
df_Machine_Learning</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p17.png" /></p>
<p>To make this dataset a little prettier, I add names to the columns:</p>
<pre class="r"><code>df_Machine_Learning.columns = [&#39;Element_&#39; + str(i+1) for i in range(0, df_Machine_Learning.shape[1])]
df_Machine_Learning</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p18.png" /></p>
<p>Let’s do a plausibility check again at this point.</p>
<p>In an earlier step, I manually calculated the final vector for the first sentence. Let’s briefly compare the result of the for-loop with the manually calculated result:</p>
<pre class="r"><code>df_Machine_Learning.iloc[:1]</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p19.png" /></p>
<pre class="r"><code>manually_calculated_mean_values_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p20.png" /></p>
<p>Fits perfectly.</p>
<p>Now I create my final data set with which I can train a machine learning model in a meaningful way:</p>
<pre class="r"><code>final_df = pd.concat([df[[&#39;Rating&#39;, &#39;Text&#39;]], df_Machine_Learning], axis=1, sort=False)
final_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p21.png" /></p>
</div>
<div id="model-training" class="section level2">
<h2>3.5 Model Training</h2>
<pre class="r"><code>clf = SVC(kernel=&#39;linear&#39;)
clf.fit(df_Machine_Learning, final_df[&#39;Rating&#39;])</code></pre>
<pre class="r"><code>pk.dump(clf, open(&#39;clf_model.pkl&#39;, &#39;wb&#39;))</code></pre>
</div>
<div id="processing-of-new-input" class="section level2">
<h2>3.6 Processing of new Input</h2>
<p>What I would like to show is how to deal with new input, i.e. if you want to use a trained model, so that the predictions can be run.</p>
<div id="load-the-word2vec-model" class="section level3">
<h3>3.6.1 Load the Word2Vec Model</h3>
<pre class="r"><code>w2v_model_reloaded = Word2Vec.load(&quot;word2vec/word2vec_model&quot;)
vector_size_n_reloaded = pk.load(open(&quot;word2vec/vector_size_w2v_metric.pkl&quot;,&#39;rb&#39;))

print(w2v_model_reloaded)
print(vector_size_n_reloaded)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p22.png" /></p>
</div>
<div id="load-the-new-input" class="section level3">
<h3>3.6.2 Load the new Input</h3>
<pre class="r"><code>new_input = [&quot;Flowers I like to see in the park especially sunflowers&quot;, 
             &quot;I like flowers&quot;]

print(new_input[0])
print(new_input[1])</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p23.png" /></p>
<p>It makes sense to transfer it to a dataframe if you don’t get the data that way anyway.</p>
<pre class="r"><code>new_input_df = pd.DataFrame(new_input, columns=[&#39;New_Input&#39;])
new_input_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p24.png" /></p>
</div>
<div id="pre-processing-of-the-new-input" class="section level3">
<h3>3.6.3 Pre-Processing of the new Input</h3>
<p>Let’s apply the steps shown earlier to the new dataset as well so we can use the ML model.</p>
<pre class="r"><code>new_input_df[&#39;New_Input_Tokenized&#39;] = new_input_df[&#39;New_Input&#39;].str.lower().apply(word_tokenize)
new_input_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p25.png" /></p>
<pre class="r"><code>words = set(w2v_model_reloaded.wv.index_to_key )
new_input_df[&#39;New_Input_vect&#39;] = np.array([np.array([w2v_model_reloaded.wv[i] for i in ls if i in words])
                                           for ls in new_input_df[&#39;New_Input_Tokenized&#39;]])


new_input_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p26.png" /></p>
<pre class="r"><code>for i, v in enumerate(new_input_df[&#39;New_Input_vect&#39;]):
    print(len(new_input_df[&#39;New_Input_Tokenized&#39;].iloc[i]), len(v))</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p27.png" /></p>
<p>Here we see now that only 7 out of 10 (or 2 out of 3 in the second sentence) words were already learned by Word2Vec and only accordingly many word vectors were merged in the column ‘New_Input_vect’.</p>
<pre class="r"><code>text_vect_avg = []
for v in new_input_df[&#39;New_Input_vect&#39;]:
    if v.size:
        text_vect_avg.append(v.mean(axis=0))
    else:
        text_vect_avg.append(np.zeros(vector_size_n_reloaded, dtype=float)) # the same vector size must be used here as for model training
        
        
new_input_df[&#39;New_Input_vect_avg&#39;] = text_vect_avg
new_input_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p28.png" /></p>
<pre class="r"><code>for i, v in enumerate(new_input_df[&#39;New_Input_vect_avg&#39;]):
    print(len(new_input_df[&#39;New_Input_Tokenized&#39;].iloc[i]), len(v))</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p29.png" /></p>
<p>Fits, the final vector length is uniform again.</p>
<pre class="r"><code>new_input_Machine_Learning_df = pd.DataFrame(text_vect_avg)
new_input_Machine_Learning_df.columns = [&#39;Element_&#39; + str(i+1) for i in range(0, new_input_Machine_Learning_df.shape[1])]
new_input_Machine_Learning_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p30.png" /></p>
<pre class="r"><code>final_new_input_df = pd.concat([new_input_df[[&#39;New_Input&#39;]], new_input_Machine_Learning_df], axis=1, sort=False)
final_new_input_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p31.png" /></p>
</div>
<div id="model-predictions" class="section level3">
<h3>3.6.4 Model Predictions</h3>
<p>Okay, now we are ready to receive the predictions.</p>
<pre class="r"><code>clf_reloaded = pk.load(open(&quot;clf_model.pkl&quot;,&#39;rb&#39;))

y_pred = clf_reloaded.predict(new_input_Machine_Learning_df)
y_pred</code></pre>
<pre class="r"><code>final_new_input_df[&#39;Prediction&#39;] = y_pred
final_new_input_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p32.png" /></p>
</div>
</div>
<div id="updating-the-word2vec-model" class="section level2">
<h2>3.7 Updating the Word2Vec Model</h2>
<p>After all, it is not uncommon to want to regularly improve your existing model. Word2Vec offers a simple solution for this.</p>
<p>Let’s assume that the content of the new record (‘new_input_df’) has pleased me quite well and I want to train my existing w2v_model the contained vocabulary.</p>
<pre class="r"><code>new_input_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p33.png" /></p>
<p>In the following, I have compared what Word2Vec have already learned with the new vocabulary and <strong>found three words that the model does not yet know</strong>.</p>
<pre class="r"><code>list_new_input = []

for value in new_input_df.New_Input.str.lower().str.split(&#39; &#39;):
    list_new_input.extend(value)

set(list_new_input) - set(list(w2v_model_reloaded.wv.index_to_key))</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p34.png" /></p>
<p>Just a reminder, the current existing vocabulary contains 20 words:</p>
<pre class="r"><code>len(w2v_model_reloaded.wv.index_to_key)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p35.png" /></p>
<div id="updating-the-weights" class="section level3">
<h3>3.7.1 Updating the Weights</h3>
<p>First I load the model I want to update.</p>
<pre class="r"><code>w2v_model_reloaded = Word2Vec.load(&quot;word2vec/word2vec_model&quot;)</code></pre>
<p>Then I train the Word2Vec model on the new data set (the tokens, since Word2Vec can only handle this).</p>
<p>Make sure that you <strong>specify the correct length / the length of the correct dataset</strong>, because this step was not necessary before during instantiation.</p>
<pre class="r"><code>w2v_model_reloaded.train(new_input_df[&#39;New_Input_Tokenized&#39;], 
                         total_examples=len(new_input_df),
                         epochs=10)</code></pre>
<p>Let’s look at the length of the new vocabulary:</p>
<pre class="r"><code>len(w2v_model_reloaded.wv.index_to_key)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p36.png" /></p>
<p>Mhhh… strange. My vocabulary has not changed. But what about the weights of the features and consequently the word vectors?</p>
<p>For this I compare the calculated vector (for the word ‘flowers’) of the model w2v_model and w2v_model_reloaded (which we just updated).</p>
<pre class="r"><code>print(w2v_model.wv[&#39;flowers&#39;])
print(w2v_model_reloaded.wv[&#39;flowers&#39;])</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p37.png" /></p>
<p>Ok the weights have changed at least.</p>
<p>I also assumed this, since the word ‘flowers’ was present in the learned vocabulary as well as in the newly learned text corpus. What about the vectors for a word that was not included in the new text corpus?</p>
<pre class="r"><code>print(w2v_model.wv[&#39;sunflowers&#39;])
print(w2v_model_reloaded.wv[&#39;sunflowers&#39;])</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p38.png" /></p>
<p>Here the vectors remained the same. However, this <strong>should not have been mandatory</strong>, because finally the semantic relation of several words is considered in Word2Vec. Even if a word is not necessarily relearned, its vector(s) can still be influenced by the newly added words.</p>
<p>But how do I now also adapt the vocabulary?</p>
</div>
<div id="updating-the-weights-and-the-vocabulary" class="section level3">
<h3>3.7.2 Updating the Weights and the Vocabulary</h3>
<p>Also for this I reload the Word2Vec model, because each execution of a learning process (which the weights updated) is kept.</p>
<pre class="r"><code>w2v_model_reloaded = Word2Vec.load(&quot;word2vec/word2vec_model&quot;)</code></pre>
<p>In order to also include the new vocabulary to the existing one, you need to apply the .build_vocab() function before retraining.</p>
<pre class="r"><code>w2v_model_reloaded.build_vocab(new_input_df[&#39;New_Input_Tokenized&#39;], update=True)
print(w2v_model_reloaded)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p39.png" /></p>
<p>Perfect, that worked (see ‘vocab=23’).</p>
<p>Now the training can be started to adjust the weights of the features as well.</p>
<p>It is no longer necessary to pay explicit attention to the <code>total_examples</code> parameter as in the previous chapter, since this information is already given by the instantiation and can be retrieved using .corpus_count.</p>
<pre class="r"><code>w2v_model_reloaded.train(new_input_df[&#39;New_Input_Tokenized&#39;], 
                         total_examples=w2v_model_reloaded.corpus_count, 
                         epochs=20)</code></pre>
<p>Let’s compare again the calculated vectors of the models w2v_model and w2v_model_reloaded for the word ‘flowers’:</p>
<pre class="r"><code>print(w2v_model.wv[&#39;flowers&#39;])
print(w2v_model_reloaded.wv[&#39;flowers&#39;])</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p40.png" /></p>
<p>Great, the vectors have adjusted again.</p>
<p>And what about the vectors for ‘sunflowers’?</p>
<pre class="r"><code>print(w2v_model.wv[&#39;sunflowers&#39;])
print(w2v_model_reloaded.wv[&#39;sunflowers&#39;])</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p41.png" /></p>
<p>This time there was also an adjustment, because the newly learned vocabulary has created new word contexts and these have influenced the new calculation of the vectors.</p>
</div>
</div>
<div id="final-saving-of-the-word2vec-model" class="section level2">
<h2>3.8 Final saving of the Word2Vec Model</h2>
<div id="saving-the-entire-model" class="section level3">
<h3>3.8.1 Saving the entire Model</h3>
<p>If you want to continue the model training at a later time, you have to save the complete model (as shown before):</p>
<pre class="r"><code>w2v_model_reloaded.save(&quot;word2vec/w2v_model_updated&quot;)</code></pre>
</div>
<div id="saving-the-keyedvectors" class="section level3">
<h3>3.8.2 Saving the KeyedVectors</h3>
<p>If the model training is completely finished and should not be repeated / updated, it is recommended to save only the KeyedVectors, which contain the calculated vectors of the learned vocabulary.</p>
<p>A trained Word2Vec model can reach a very high memory capacity very quickly. KeyedVectors, on the other hand, are much smaller and faster objects that can be used to load the required vectors very quickly while conserving memory.</p>
<pre class="r"><code>w2v_model_reloaded_vectors = w2v_model_reloaded.wv

w2v_model_reloaded_vectors.save(&quot;word2vec/w2v_model_vectors_updated&quot;)</code></pre>
<p>Ok, let’s reload it once:</p>
<pre class="r"><code>w2v_model_vectors_reloaded = KeyedVectors.load(&quot;word2vec/w2v_model_vectors_updated&quot;, mmap=&#39;r&#39;)
print(w2v_model_vectors_reloaded)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p42.png" /></p>
<p>Be aware that from now on <strong>the additional argument .wv is no longer needed</strong>.</p>
<pre class="r"><code>w2v_model_vectors_reloaded[&#39;sunflowers&#39;]</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p43.png" /></p>
<p>Here again for comparison that the same values have been saved:</p>
<pre class="r"><code>w2v_model_reloaded.wv[&#39;sunflowers&#39;]</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p44.png" /></p>
</div>
</div>
</div>
<div id="gensim---doc2vec" class="section level1">
<h1>4 Gensim - Doc2Vec</h1>
<p>Ok now we are familiar with Word2Vec and how it works…. but what is Doc2Vec??</p>
<p>Once you understand what Word2Vec is, the concept of Doc2Vec is very easy to understand.
A Doc2Vec model is based on Word2Vec, with only one more vector added to the neural network for input, the paragraph ID.
Accordingly, Doc2vec also uses an unsupervised learning approach to learn document representation like Word2Vec.
Look at the following diagram of the layer structure:</p>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136s2.png" /></p>
<p>Source: <a href="http://148.204.64.1/~sidorov/MICAI_2016_helena.pdf">Markov, I, Gómez-Adorno, H, Posadas-Durán, J P, Sidorov, G, &amp; Gelbukh, A (2016, October). Author profiling with doc2vec neural network-based document embeddings. In Mexican International Conference on Artificial Intelligence (pp. 117-131). Springer, Cham.</a></p>
<p>The diagram shown above is <strong>based on the layer structure of the CBOW</strong> model and is called <strong>distributed Memory version of Paragraph Vector (PV-DM)</strong>.</p>
<p>But instead of using only nearby words to predict the target word, another feature vector is added which is document specific. So when training the word vectors W, the document vector D is trained as well, and in the end of training, it holds a numeric representation of the document. Thus, when the word vectors W are trained, the document vector D is also trained, which contains a numerical representation of the document at the end of the training.</p>
<p>The <strong>counterpart of the PV-DM is called Distributed Bag of Words Version of Paragraph Vector (PV-DBOW)</strong> and is based on the Skip-Gram approach.</p>
<p>Since PV-DM is most commonly used in practice, I will limit myself to this method in this post. Furthermore, the way it works is very similar to the Word2Vec model I described in great detail above, so I won’t go into each step again in the following.</p>
<p>I load the same data set as for Word2Vec as the data basis:</p>
<pre class="r"><code>df_doc2vec = pd.DataFrame({&#39;Rating&#39;: [3,5,1,2],
                           &#39;Text&#39;: [&quot;I love sunflowers&quot;,
                            &quot;Sunflowers fill my heart with joy&quot;,
                            &quot;I love to look into the garden and see the flowers&quot;,
                            &quot;Flowers especially sunflowers are the most beautiful&quot;]})
df_doc2vec</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p45.png" /></p>
<pre class="r"><code>df_doc2vec[&#39;Text_Tokenized&#39;] = df_doc2vec[&#39;Text&#39;].str.lower().apply(word_tokenize)
df_doc2vec</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p46.png" /></p>
<div id="instantiation-1" class="section level2">
<h2>4.1 Instantiation</h2>
<p>The TaggedDocument function helps us to create an input format suitable for the Doc2Vec algorithm.
This results in a list of words (tokens) associated with a particular tag (paragraph ID). This results in the additional feature vector mentioned earlier that benefits model training.</p>
<p>Again I assign the <code>vector_size</code> to a separate variable.</p>
<pre class="r"><code>vector_size_n_d2v = 5
documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_doc2vec[&#39;Text_Tokenized&#39;])]

d2v_model = Doc2Vec(documents,
                    vector_size=vector_size_n_d2v,
                    window=3,
                    min_count=1,
                    dm=1, # 0=PV-DBOW, 1=PV-DM
                    epochs=5)</code></pre>
<pre class="r"><code># Length of the learned vocabulary:
len(d2v_model.wv.index_to_key)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p47.png" /></p>
<pre class="r"><code># Saving the model as well as the required parameters 
d2v_model.save(&quot;doc2vec/doc2vec_model&quot;)

pk.dump(vector_size_n_d2v, open(&#39;doc2vec/vector_size_d2v_metric.pkl&#39;, &#39;wb&#39;))</code></pre>
</div>
<div id="updating-the-doc2vec-model" class="section level2">
<h2>4.2 Updating the Doc2Vec Model</h2>
<p>Again, I use the same example data set as for Word2Vec.</p>
<pre class="r"><code>new_input_doc2vec = [&quot;Flowers I like to see in the park especially sunflowers&quot;, &quot;I like flowers&quot;]
new_input_doc2vec = pd.DataFrame(new_input_doc2vec, columns=[&#39;New_Input&#39;])
new_input_doc2vec[&#39;New_Input_Tokenized&#39;] = new_input_doc2vec[&#39;New_Input&#39;].str.lower().apply(word_tokenize)
new_input_doc2vec</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p48.png" /></p>
<pre class="r"><code>d2v_model_reloaded = Doc2Vec.load(&quot;doc2vec/doc2vec_model&quot;)
vector_size_n_d2v_reloaded = pk.load(open(&quot;doc2vec/vector_size_d2v_metric.pkl&quot;,&#39;rb&#39;))</code></pre>
<pre class="r"><code># Learning the vocabulary
documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(new_input_doc2vec[&#39;New_Input_Tokenized&#39;])]
d2v_model_reloaded.build_vocab(documents, update=True)</code></pre>
<pre class="r"><code>len(d2v_model_reloaded.wv.index_to_key)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p49.png" /></p>
<pre class="r"><code>#Training the model again
d2v_model_reloaded.train(documents, 
                         total_examples=d2v_model_reloaded.corpus_count, 
                         epochs=20)</code></pre>
<p>Here are the calculated vectors of the different models for the word ‘sunflowers’ for comparison:</p>
<pre class="r"><code>print(w2v_model.wv[&#39;sunflowers&#39;])
print(d2v_model.wv[&#39;sunflowers&#39;])
print(w2v_model_reloaded.wv[&#39;sunflowers&#39;])
print(d2v_model_reloaded.wv[&#39;sunflowers&#39;])</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p50.png" /></p>
<p>The Word2Vec model as well as the Doc2Vec model obtained the same vectors during the first training (5 epochs). Now you might think that the two models work exactly the same. However, this is not the case, since the document tag is included in Doc2Vec. However, this effect is not yet noticeable due to the short training time of 5 epochs.</p>
<p>The two relaoded models were not only updated with the additional sample data set but also trained over more epochs (20). A small difference in the vectors can already be seen here.</p>
</div>
<div id="generation-of-aggregated-sentence-vectors-1" class="section level2">
<h2>4.3 Generation of aggregated Sentence Vectors</h2>
<p>The principle how to combine the generated word vectors for a sentence and bring them into the same length I have already described above. Therefore here only the adapted syntax for Doc2Vec.</p>
<pre class="r"><code>words = set(d2v_model_reloaded.wv.index_to_key )
new_input_doc2vec[&#39;New_Input_vect&#39;] = np.array([np.array([d2v_model_reloaded.wv[i] for i in ls if i in words])
                         for ls in new_input_doc2vec[&#39;New_Input_Tokenized&#39;]])

new_input_doc2vec</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p51.png" /></p>
<pre class="r"><code>for i, v in enumerate(new_input_doc2vec[&#39;New_Input_vect&#39;]):
    print(len(new_input_doc2vec[&#39;New_Input_Tokenized&#39;].iloc[i]), len(v))</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p52.png" /></p>
</div>
<div id="generation-of-averaged-sentence-vectors-1" class="section level2">
<h2>4.4 Generation of averaged Sentence Vectors</h2>
<pre class="r"><code>text_vect_avg = []
for v in new_input_doc2vec[&#39;New_Input_vect&#39;]:
    if v.size:
        text_vect_avg.append(v.mean(axis=0))
    else:
        text_vect_avg.append(np.zeros(vector_size_n_d2v_reloaded, dtype=float)) # the same vector size must be used here as for model training
        
        
new_input_doc2vec[&#39;New_Input_vect_avg&#39;] = text_vect_avg
new_input_doc2vec</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p53.png" /></p>
<pre class="r"><code>for i, v in enumerate(new_input_doc2vec[&#39;New_Input_vect_avg&#39;]):
    print(len(new_input_doc2vec[&#39;New_Input_Tokenized&#39;].iloc[i]), len(v))</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p54.png" /></p>
<pre class="r"><code>new_input_w2v_Machine_Learning_df = pd.DataFrame(text_vect_avg)
new_input_w2v_Machine_Learning_df.columns = [&#39;Element_&#39; + str(i+1) for i in range(0, new_input_w2v_Machine_Learning_df.shape[1])]
new_input_w2v_Machine_Learning_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p55.png" /></p>
<pre class="r"><code>final_new_input_d2v_df = pd.concat([new_input_doc2vec[[&#39;New_Input&#39;]], 
                                    new_input_w2v_Machine_Learning_df], 
                                    axis=1, sort=False)
final_new_input_d2v_df</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p56.png" /></p>
</div>
<div id="final-saving-of-the-doc2vec-model" class="section level2">
<h2>4.5 Final saving of the Doc2Vec Model</h2>
<p>Again, with Doc2Vec I have the two options of either saving the entire model (if I want to continue with the training at another time) or saving only the calculated vectors:</p>
<p><strong>Saving the entire Model</strong></p>
<pre class="r"><code>d2v_model_reloaded.save(&quot;doc2vec/d2v_model_updated&quot;)</code></pre>
<p><strong>Saving the KeyedVectors</strong></p>
<pre class="r"><code>d2v_model_reloaded_vectors = d2v_model_reloaded.wv

d2v_model_reloaded_vectors.save(&quot;doc2vec/d2v_model_vectors_updated&quot;)</code></pre>
<p>That’s it.</p>
</div>
</div>
<div id="gensim---fasttext" class="section level1">
<h1>5 Gensim - FastText</h1>
<p>You are also encouraged to read through the documentation of how <a href="https://radimrehurek.com/gensim/models/fasttext.html#module-gensim.models.fasttext">FastText</a> works. <a href="https://radimrehurek.com/gensim/models/fasttext.html#module-gensim.models.fasttext">FastText</a> is another extremely useful word embedding and text classification module developed by Facebook that has already achieved excellent results on many NLP problems and it works pretty much the same way as Word2Vec and Doc2Vec.</p>
</div>
<div id="phrases-phraser" class="section level1">
<h1>6 Phrases &amp; Phraser</h1>
<div id="detecting-phrases" class="section level2">
<h2>6.1 Detecting Phrases</h2>
<p><a href="https://radimrehurek.com/gensim/index.html">GENSIM</a> includes some useful features that can simplify / improve a model training of Word2Vec or Doc2Vec.
One of them is <a href="https://radimrehurek.com/gensim/models/phrases.html">Phrase detection</a></p>
<p>It can be used to easily find and replace bigrams or trigrams in a text corpus. This can be an incredible booster for the Word2Vec model when creating vectors.</p>
<p>Let’s have a look at the following example dataframe:</p>
<pre class="r"><code>df_phrases = pd.DataFrame({&#39;Text&#39;: [&quot;I love machine learning it is state of the art&quot;,
                                    &quot;Machine learning is pretty famous in new york&quot;,
                                    &quot;New york is the place to be for machine learning&quot;,
                                    &quot;New york for machine learning&quot;]})
df_phrases</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p57.png" /></p>
<pre class="r"><code>df_phrases[&#39;Text_Tokenized&#39;] = df_phrases[&#39;Text&#39;].str.lower().apply(word_tokenize)
df_phrases</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p58.png" /></p>
<p>After tokenization of the data set, the phrase model can now be trained. The two parameters I use are:</p>
<ul>
<li><code>min_count</code>: Ignore all words whose total number of words found in the text corpus is less than this value</li>
<li><code>threshold</code>: Determines the formation of the phrases based on their achieved score. A higher value means fewer phrases.</li>
</ul>
<p>You can read these and other parameters here: <a href="https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases">gensim.models.phrases.Phrases</a></p>
<pre class="r"><code>bigram_phrases = Phrases(df_phrases[&#39;Text_Tokenized&#39;], 
                         min_count=2, 
                         threshold=2)</code></pre>
<p><strong>If you want</strong> to detect not only bigrams but <strong>also trigrams</strong> in your text corpus use the following code:</p>
<p><code>trigram_phrases = Phrases(bigram_phrases)</code></p>
<p>Let’s see what bigrams were found and learned:</p>
<pre class="r"><code>print(&#39;Number of bigrams learned: &#39; + str(len(bigram_phrases.export_phrases())))
print()
print(&#39;Learned bigrams:&#39;)
print(bigram_phrases.export_phrases())</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p59.png" /></p>
<p>You can transfer the found bigrams into a dataframe with the following code:</p>
<pre class="r"><code>dct_bigrams = {k:[v] for k,v in bigram_phrases.export_phrases().items()}
df_bigrams = pd.DataFrame(dct_bigrams).T.reset_index()
df_bigrams.columns = [&#39;bigram&#39;, &#39;score&#39;]
df_bigrams</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p60.png" /></p>
<p>Why am I taking this step?</p>
<p>It is not so easy to determine <code>min_count</code> and <code>threshold</code> at the beginning.
This way you can see by filtering and sorting which scoring the bigrams (or trigrams) have received and adjust these parameters if necessary.</p>
<p>Let’s briefly preview what the new sentences from our sample dataset would look like if we applied the trained phrases model:</p>
<pre class="r"><code># Preview:
for i in range(0,len(df_phrases)):
    print(bigram_phrases[df_phrases[&#39;Text_Tokenized&#39;][i]])</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p61.png" /></p>
<p>Note: I will show the saving of the model(s) and the creation of a final dataset at a later time.</p>
</div>
<div id="updating-the-phrases-model" class="section level2">
<h2>6.2 Updating the Phrases Model</h2>
<p>Like the Word2Vec/Doc2Vec model, the Phrases model can be updated with new input. I will show you how to do this in this chapter. Here we have a new data set:</p>
<pre class="r"><code>df_phrases_new_input = pd.DataFrame({&#39;Text&#39;: [&quot;Data Science becomes more and more popular&quot;,
                                              &quot;For Data Science task you need machnine learning algorithms&quot;,
                                              &quot;The buzzword 2020 is Data Science&quot;]})
df_phrases_new_input</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p62.png" /></p>
<pre class="r"><code>df_phrases_new_input[&#39;Text_Tokenized&#39;] = df_phrases_new_input[&#39;Text&#39;].str.lower().apply(word_tokenize)
df_phrases_new_input</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p63.png" /></p>
<p>The add_vocab function can be used to update the existing model:</p>
<pre class="r"><code>bigram_phrases.add_vocab(df_phrases_new_input[&#39;Text_Tokenized&#39;])</code></pre>
<p>Now let’s look again at what bigrams were learned from the model:</p>
<pre class="r"><code>print(&#39;Number of bigrams learned: &#39; + str(len(bigram_phrases.export_phrases())))
print()
print(&#39;Learned bigrams:&#39;)
print(bigram_phrases.export_phrases())</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p64.png" /></p>
<p>Perfect, a new bigram (‘machine_learning’) was learned.</p>
</div>
<div id="safe-load" class="section level2">
<h2>6.3 Safe &amp; Load</h2>
<p>As with Word2Vec/Doc2Vec, we can save the Phrases model and reload it at a later time as well as re-train /update it.</p>
<p><strong>Phrases:</strong></p>
<pre class="r"><code>bigram_phrases.save(&quot;bigram_phrases&quot;)</code></pre>
<pre class="r"><code>bigram_phrases_relaod = Phrases.load(&quot;bigram_phrases&quot;)</code></pre>
<pre class="r"><code>print(&#39;Number of bigrams learned: &#39; + str(len(bigram_phrases_relaod.export_phrases())))
print()
print(&#39;Learned bigrams:&#39;)
print(bigram_phrases_relaod.export_phrases())</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p65.png" /></p>
<p><strong>Phraser:</strong></p>
<p>If we don’t want to train the model further, we can use Phraser to save memory and increase speed.</p>
<pre class="r"><code>bigram_phraser = Phraser(bigram_phrases)
bigram_phraser.save(&quot;bigram_phraser&quot;)</code></pre>
<pre class="r"><code>bigram_phraser_reload = Phraser.load(&quot;bigram_phraser&quot;)</code></pre>
<pre class="r"><code># Check if relaoded phraser still work
sentence = [&quot;I live in new york&quot;]
sentence_stream = [doc.split(&quot; &quot;) for doc in sentence]

bigram_phraser_reload.find_phrases(sentence_stream)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p66.png" /></p>
</div>
<div id="creation-of-the-final-dataframe" class="section level2">
<h2>6.4 Creation of the final dataframe</h2>
<p>As soon as I am done with the Phrases model training, <strong>I will exclusively use the created Phraser model</strong>.</p>
<pre class="r"><code>test_new_input = pd.DataFrame({&#39;Text&#39;: [&quot;Data Science especially machine learning in New York is a total hype&quot;]})

test_new_input</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p67.png" /></p>
<pre class="r"><code>test_new_input[&#39;Text&#39;][0]</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p68.png" /></p>
<pre class="r"><code>test_new_input[&#39;Text_Tokenized&#39;] = test_new_input[&#39;Text&#39;].str.lower().apply(word_tokenize)
test_new_input</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p69.png" /></p>
<pre class="r"><code>bigram_phraser.find_phrases(test_new_input[&#39;Text_Tokenized&#39;])</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p70.png" /></p>
<p>All three learned bigrams also appear in our example here and are correctly detected by the model.</p>
<p><strong>Now I want to create a final dataset that contains the bigrams instead of the individual words in the text corpus.</strong></p>
<p>I use the following function for this purpose:</p>
<pre class="r"><code>def make_bigrams_func(text):
    &#39;&#39;&#39;
    Replaces single words by found bigrams
    The bigram model &#39;bigram_phraser&#39; must be loaded
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with inserted bigrams
    &#39;&#39;&#39;  
    return [bigram_phraser[doc] for doc in text]</code></pre>
<pre class="r"><code>test_new_input[&#39;Text_Tokenized_bigrams&#39;] = make_bigrams_func(test_new_input[&#39;Text_Tokenized&#39;])
test_new_input</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p71.png" /></p>
<pre class="r"><code>test_new_input[&#39;Text_Tokenized_bigrams&#39;][0]</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p72.png" /></p>
<p>Perfect, our new text corpus now contains the bigrams found instead of the individual words.</p>
<p>Now I could train a Word2Vec or Doc2Vec model on this data basis, which would probably generate better values of the vectors.</p>
</div>
</div>
<div id="use-pre-trained-models" class="section level1">
<h1>7 Use Pre-Trained Models</h1>
<p>Especially in the field of NLP it is very difficult to get a sufficiently large data base in the beginning to be able to do a meaningful model training.</p>
<p>What has proven to be extremely useful in solving many NLP problems is the use of pre-trained models.</p>
<p>Gensim offers a wide range of pre-trained models:</p>
<pre class="r"><code>list(api.info()[&#39;models&#39;].keys())</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p73.png" /></p>
<p>Let’s load one of them:</p>
<pre class="r"><code>glove_vectors = api.load(&#39;glove-wiki-gigaword-50&#39;)</code></pre>
<p>Ok let’s see if our vocabulary is also already included in it:</p>
<pre class="r"><code>glove_vectors[&#39;sunflowers&#39;]</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p74.png" /></p>
<pre class="r"><code>glove_vectors.most_similar(&#39;sunflowers&#39;, topn=5)</code></pre>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136p75.png" /></p>
<p>Yes it is. So we can make life easy for ourselves and use the already calculated vectors of ‘glove-wiki-gigaword-50’.</p>
</div>
<div id="conclusion" class="section level1">
<h1>8 Conclusion</h1>
<p>In this post I went into detail about using Word2Vec and Doc2Vec from the python library gensim to solve text classification problems.</p>
<p>Furthermore, I have shown how the Phrases module can be used to further improve the data basis.</p>
<p>The final folder structure should now look like this:</p>
<p><img src="/post/2021-09-01-nlp-word-embedding-with-gensim-for-text-classification_files/p136s3.png" /></p>
</div>
