---
title: NLP - Text Pre-Processing III (POS, NER and Normalization)
author: Michael Fuchs
date: '2021-05-31'
slug: nlp-text-pre-processing-iii-pos-ner-and-normalization
categories: []
tags: []
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#import-the-libraries-and-the-data">2 Import the Libraries and the Data</a></li>
<li><a href="#definition-of-required-functions">3 Definition of required Functions</a></li>
<li><a href="#text-pre-processing">4 Text Pre-Processing</a>
<ul>
<li><a href="#text-cleaning">4.1 (Text Cleaning)</a></li>
<li><a href="#tokenization">4.2 (Tokenization)</a></li>
<li><a href="#stop-words">4.3 (Stop Words)</a></li>
<li><a href="#digression-pos-ner">4.4 Digression: POS &amp; NER</a>
<ul>
<li><a href="#part-of-speech-tagging-pos">4.4.1 Part of Speech Tagging (POS)</a></li>
<li><a href="#named-entity-recognition-ner">4.4.2 Named Entity Recognition (NER)</a></li>
</ul></li>
<li><a href="#normalization">4.5 Normalization</a>
<ul>
<li><a href="#stemming">4.5.1 Stemming</a></li>
<li><a href="#lemmatization">4.5.2 Lemmatization</a>
<ul>
<li><a href="#wordnet-lemmatizer-with-specific-pos-tag">4.5.2.1 Wordnet Lemmatizer with specific POS tag</a></li>
<li><a href="#wordnet-lemmatizer-with-appropriate-pos-tag">4.5.2.2 Wordnet Lemmatizer with appropriate POS tag</a></li>
<li><a href="#multiple-specific-wordnet-lemmatizer">4.5.2.3 Multiple specific Wordnet Lemmatizer</a></li>
</ul></li>
<li><a href="#application-to-the-example-string">4.5.3 <strong>Application</strong> to the Example String</a></li>
<li><a href="#application-to-the-dataframe">4.5.4 <strong>Application</strong> to the DataFrame</a></li>
</ul></li>
</ul></li>
<li><a href="#conclusion">5 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>Let’s continue in our post series about Text Pre-Processing.</p>
<p>In this post I will cover the following topics:</p>
<ul>
<li>POS</li>
<li>NER and</li>
<li>Normalization</li>
</ul>
<p>For this publication the processed dataset <em>Amazon Unlocked Mobile</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used as well as the created Example String. You can download both files from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets/NLP/Text%20Pre-Processing%20III%20(POS%2C%20NER%20and%20Normalization)">“GitHub Repository”</a>.</p>
</div>
<div id="import-the-libraries-and-the-data" class="section level1">
<h1>2 Import the Libraries and the Data</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

import pickle as pk

import warnings
warnings.filterwarnings(&quot;ignore&quot;)


from bs4 import BeautifulSoup
import unicodedata
import re

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize

from nltk.corpus import stopwords


from nltk.corpus import wordnet
from nltk import pos_tag
from nltk import ne_chunk

from nltk.stem.porter import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

from nltk.probability import FreqDist
import matplotlib.pyplot as plt
from wordcloud import WordCloud</code></pre>
<pre class="r"><code>pd.set_option(&#39;display.max_colwidth&#39;, 30)</code></pre>
<pre class="r"><code>df = pd.read_csv(&#39;Amazon_Unlocked_Mobile_small_Part_II.csv&#39;)
df.head()</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p1.png" /></p>
<pre class="r"><code>df[&#39;Reviews_wo_Stop_Words&#39;] = df[&#39;Reviews_wo_Stop_Words&#39;].astype(str)</code></pre>
<pre class="r"><code>clean_text_wo_stop_words = pk.load(open(&quot;clean_text_wo_stop_words.pkl&quot;,&#39;rb&#39;))
clean_text_wo_stop_words</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p2.png" /></p>
</div>
<div id="definition-of-required-functions" class="section level1">
<h1>3 Definition of required Functions</h1>
<p>All functions are summarized here. I will show them again where they are used during this post if they are new and have not been explained yet.</p>
<pre class="r"><code>def word_count_func(text):
    &#39;&#39;&#39;
    Counts words within a string
    
    Args:
        text (str): String to which the function is to be applied, string
    
    Returns:
        Number of words within a string, integer
    &#39;&#39;&#39; 
    return len(text.split())</code></pre>
<pre class="r"><code>def norm_stemming_func(text):
    &#39;&#39;&#39;
    Stemming tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use PorterStemmer() to stem the created tokens
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with stemmed words
    &#39;&#39;&#39;  
    words = word_tokenize(text)
    text = &#39; &#39;.join([PorterStemmer().stem(word) for word in words])
    return text</code></pre>
<pre class="r"><code>def norm_lemm_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() to lemmatize the created tokens
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39;  
    words = word_tokenize(text)
    text = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word) for word in words])
    return text</code></pre>
<pre class="r"><code>def norm_lemm_v_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string 
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() to lemmatize the created tokens
            POS tag is set to &#39;v&#39; for verb
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39;  
    words = word_tokenize(text)
    text = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, pos=&#39;v&#39;) for word in words])
    return text</code></pre>
<pre class="r"><code>def norm_lemm_a_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() to lemmatize the created tokens
            POS tag is set to &#39;a&#39; for adjective
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    text = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, pos=&#39;a&#39;) for word in words])
    return text</code></pre>
<pre class="r"><code>def get_wordnet_pos_func(word):
    &#39;&#39;&#39;
    Maps the respective POS tag of a word to the format accepted by the lemmatizer of wordnet
    
    Args:
        word (str): Word to which the function is to be applied, string
    
    Returns:
        POS tag, readable for the lemmatizer of wordnet
    &#39;&#39;&#39;     
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {&quot;J&quot;: wordnet.ADJ,
                &quot;N&quot;: wordnet.NOUN,
                &quot;V&quot;: wordnet.VERB,
                &quot;R&quot;: wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)</code></pre>
<pre class="r"><code>def norm_lemm_POS_tag_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() to lemmatize the created tokens
            POS tag is determined with the help of function get_wordnet_pos()
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    text = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, get_wordnet_pos_func(word)) for word in words])
    return text</code></pre>
<pre class="r"><code>def norm_lemm_v_a_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() with POS tag &#39;v&#39; to lemmatize the created tokens
    Step 3: Use word_tokenize() to get tokens from generated string        
    Step 4: Use WordNetLemmatizer() with POS tag &#39;a&#39; to lemmatize the created tokens
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39;
    words1 = word_tokenize(text)
    text1 = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, pos=&#39;v&#39;) for word in words1])
    words2 = word_tokenize(text1)
    text2 = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, pos=&#39;a&#39;) for word in words2])
    return text2</code></pre>
</div>
<div id="text-pre-processing" class="section level1">
<h1>4 Text Pre-Processing</h1>
<div id="text-cleaning" class="section level2">
<h2>4.1 (Text Cleaning)</h2>
<p>I have already described this part in an earlier post. See here: <a href="https://michael-fuchs-python.netlify.app/2021/05/22/nlp-text-pre-processing-i-text-cleaning/#text-cleaning">Text Cleaning</a></p>
</div>
<div id="tokenization" class="section level2">
<h2>4.2 (Tokenization)</h2>
<p>I have already described this part in the previous post. See here: <a href="https://michael-fuchs-python.netlify.app/2021/05/25/nlp-text-pre-processing-ii-tokenization-and-stop-words/#tokenization">Text Pre-Processing II-Tokenization</a></p>
</div>
<div id="stop-words" class="section level2">
<h2>4.3 (Stop Words)</h2>
<p>I have already described this part in the previous post. See here: <a href="https://michael-fuchs-python.netlify.app/2021/05/25/nlp-text-pre-processing-ii-tokenization-and-stop-words/#stop-words">Text Pre-Processing II-Stop Words</a></p>
</div>
<div id="digression-pos-ner" class="section level2">
<h2>4.4 Digression: POS &amp; NER</h2>
<p>In the following I will briefly explain what Part of Speech Tagging (POS) and Named Entity Recognition (NER) is and what we will need it for in the context of text pre-processing.</p>
<pre class="r"><code>pos_ner_text = &quot;Bill Gates founded Microsoft Corp. together with Paul Allen in 1975.&quot;
pos_ner_text</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p3.png" /></p>
<div id="part-of-speech-tagging-pos" class="section level3">
<h3>4.4.1 Part of Speech Tagging (POS)</h3>
<p>Part-of-speech tagging (POS tagging) aims to identify which grammatical group a word belongs to, i.e. whether it is a noun, adjective, verb, adverb, etc., based on the context.</p>
<p>Relationships within the sentence are searched for and each word in a sentence is tagged with the appropriate tag.</p>
<p>Here is a <a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">list of Part of Speech tags</a>.</p>
<pre class="r"><code>POS_tag = pos_tag(word_tokenize(pos_ner_text))
POS_tag</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p4.png" /></p>
<p>As we can see, Bill Gates and Paul Allen are correctly recognized as NNP (Proper noun, singular) and tagged accordingly. The same applies to Microsoft. Now let’s see what comes out of NER.</p>
</div>
<div id="named-entity-recognition-ner" class="section level3">
<h3>4.4.2 Named Entity Recognition (NER)</h3>
<p>Named Entity Recognition (NER) tries to find out whether a word is a named entity or not. Named entities are places, organisations, people, time expressions, etc.</p>
<p>POS is more of a global problem, as there can be relationships between the first and last word of a sentence.</p>
<p>In contrast, NER is more of a local problem, since named entities are not distributed in a sentence and mostly consist of uni-, bi- or trigrams.</p>
<pre class="r"><code>NER_tree = ne_chunk(pos_tag(word_tokenize(pos_ner_text)))
print(NER_tree)</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p5.png" /></p>
<p>Again, Bill Gates and Paul Allen are recognised as NNP. In addition, however, we still receive the information here that they are (correctly) persons. If we look at Microsoft, we see that it was not only tagged as an NNP but also recognised as an organisation.</p>
<p>But what do we need all this for in our project?</p>
<p>There are many other things you can do with POS and NER and I will explain these two topics in more detail in separate posts, but for now it is sufficient to know what these two methods basically do.</p>
<p>The following chapter will be about normalising texts. The aim here is to bring the words into their basic form in order to make them even more meaningful for further analysis. The algorithms used for this partly use POS and NER, so it is useful to know roughly what is happening here.</p>
</div>
</div>
<div id="normalization" class="section level2">
<h2>4.5 Normalization</h2>
<p>Text normalisation tries to reduce the randomness in text and bring it closer to a predefined standard.
This has the effect of reducing the amount of different information (that the further algorithms have to work with) and thus improving efficiency.</p>
<p>There are two methods for this:</p>
<ul>
<li>Stemming and</li>
<li>Lemmatization</li>
</ul>
<p>The aim of these normalisation techniques is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.</p>
<p><strong>Stemming</strong> is the process of reducing words to their root or root form.
Here, stemming algorithms work by cutting off the beginning or end of a word, taking into account a list of common prefixes and suffixes.
However, this random cutting does not always work. Therefore, this approach has some limitations.</p>
<p>In <strong>lemmatization</strong>, words are reduced to their base word.
The lemmatization algorithms try to reduce the inflected words correctly so that the affiliation of the base word to the language is guaranteed.</p>
<p>The <strong>difference between stemming and lemmatization</strong> is that a stemming algorithm works with a single word without knowing the context of the whole sentence and therefore cannot distinguish between words that have different meanings depending on the type of word. One advantage of stemming algorithms is that they are easier to implement and run faster. If accuracy is not so important for the application, stemming algorithms are the right choice. What increases the working time of lemmatization algorithms is that the part of speech of a word has to be determined first and in this process the normalisation rules will be different for different parts of speech.</p>
<pre class="r"><code>stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

print(&quot;Original Word: &#39;studies&#39; &quot;)
print()
print(&#39;With Stemming: &#39; + str(stemmer.stem(&quot;studies&quot;)))
print(&#39;with Lemmatization: &#39; + str(lemmatizer.lemmatize(&quot;studies&quot;)))</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p6.png" /></p>
<pre class="r"><code>text_for_normalization = &quot;\
I saw an amazing thing and ran. \
It took too long. \
We are eating and swimming. \
I want better dog.\
&quot;
text_for_normalization</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p7.png" /></p>
<div id="stemming" class="section level3">
<h3>4.5.1 Stemming</h3>
<p><a href="https://www.nltk.org/_modules/nltk/stem/porter.html">PorterStemmer</a> (which we will also use in the following for this task) is probably the best known stemming algorithm.</p>
<p>But there are several others that can be used:</p>
<ul>
<li><a href="https://www.hashcollision.org/hkn/python/py_lovins/">Lovin’s Stemmer</a></li>
<li><a href="http://algorithmtraining.com/stemming-words/">Dawson’s Stemmer</a></li>
<li><a href="https://pypi.org/project/krovetz/">Krovetz Stemmer</a></li>
<li><a href="http://algorithmtraining.com/stemming-words/">Xerox Stemmer</a></li>
<li><a href="https://pypi.org/project/snowballstemmer/">Snowball Stemmer</a></li>
</ul>
<pre class="r"><code>def norm_stemming_func(text):
    &#39;&#39;&#39;
    Stemming tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use PorterStemmer() to stem the created tokens
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with stemmed words
    &#39;&#39;&#39;  
    words = word_tokenize(text)
    text = &#39; &#39;.join([PorterStemmer().stem(word) for word in words])
    return text</code></pre>
<pre class="r"><code>norm_stemming_func(text_for_normalization)</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p8.png" /></p>
</div>
<div id="lemmatization" class="section level3">
<h3>4.5.2 Lemmatization</h3>
<p>As with stemming, there are several algorithms that can be used for lemmatization:</p>
<ul>
<li><a href="https://spacy.io/api/lemmatizer">spaCy Lemmatizer</a></li>
<li><a href="https://textblob.readthedocs.io/en/dev/">TextBlob Lemmatizer</a></li>
<li><a href="https://stanfordnlp.github.io/CoreNLP/lemma.html">Stanford CoreNLP Lemmatization</a></li>
<li><a href="https://tedboy.github.io/nlps/generated/generated/gensim.utils.lemmatize.html">Gensim Lemmatize</a></li>
</ul>
<p>We use the <a href="https://www.nltk.org/_modules/nltk/stem/wordnet.html">WordNetLemmatizer</a> from <a href="https://www.nltk.org/">nltk</a> for the following examples.</p>
<pre class="r"><code>def norm_lemm_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() to lemmatize the created tokens
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39;  
    words = word_tokenize(text)
    text = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word) for word in words])
    return text</code></pre>
<pre class="r"><code>norm_lemm_func(text_for_normalization)</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p9.png" /></p>
<p>Notice the WordNetLemmatizer function didn’t do a good job. Because, ‘are’ is not converted to ‘be’ and ‘swimming’ is not converted to ‘swim’ as expected.</p>
<div id="wordnet-lemmatizer-with-specific-pos-tag" class="section level4">
<h4>4.5.2.1 Wordnet Lemmatizer with specific POS tag</h4>
<p>With the Wordnet Lemmatizer you have the possibility to set a specific POS tag. Let’s set this to pos=‘v’ where ‘v’ stands for ‘verb’. Usually this POS tag is used.</p>
<pre class="r"><code>def norm_lemm_v_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string 
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() to lemmatize the created tokens
            POS tag is set to &#39;v&#39; for verb
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39;  
    words = word_tokenize(text)
    text = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, pos=&#39;v&#39;) for word in words])
    return text</code></pre>
<pre class="r"><code>norm_lemm_v_func(text_for_normalization)</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p10.png" /></p>
<p>Perfect, now it’s almost the way we want it (‘are’ became ‘be’ and ‘eating’ and ‘swimming’ became ‘eat’ and ‘swim’). One exception is ‘saw’. One would have expected that this word would be changed to ‘see’. Why this is a strange exception can be read <a href="https://stackoverflow.com/questions/33594721/why-nltk-lemmatization-has-wrong-output-even-if-verb-exc-has-added-right-value">here</a>.</p>
<p>Let’s take a look at the phrase ‘I want better dog’. Instead of ‘better’ I would like to have ‘good’ or ‘well’. But since this is not a verb but an adjective we would have to use the WordNetLemmatizer with the POS tag = ‘a’ for adjective. Also for this I wrote a function that does exactly that:</p>
<pre class="r"><code>def norm_lemm_a_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() to lemmatize the created tokens
            POS tag is set to &#39;a&#39; for adjective
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    text = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, pos=&#39;a&#39;) for word in words])
    return text</code></pre>
<pre class="r"><code>norm_lemm_a_func(text_for_normalization)</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p11.png" /></p>
<p>Has worked but unfortunately now again the verbs were not taken into account.</p>
<p>For this problem there are two ways which I would like to introduce in the following two chapters.</p>
</div>
<div id="wordnet-lemmatizer-with-appropriate-pos-tag" class="section level4">
<h4>4.5.2.2 Wordnet Lemmatizer with appropriate POS tag</h4>
<p>One possibility would be to write a function that determines the respective POS tag of a word and passes it to the lemmatization function for the respective token.</p>
<p>Here are the Part-of-speech constants:</p>
<ul>
<li>ADJ = ‘a’</li>
<li>ADJ_SAT = ‘s’</li>
<li>ADV = ‘r’</li>
<li>NOUN = ‘n’</li>
<li>VERB = ‘v’</li>
</ul>
<pre class="r"><code>def get_wordnet_pos_func(word):
    &#39;&#39;&#39;
    Maps the respective POS tag of a word to the format accepted by the lemmatizer of wordnet
    
    Args:
        word (str): Word to which the function is to be applied, string
    
    Returns:
        POS tag, readable for the lemmatizer of wordnet
    &#39;&#39;&#39;     
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {&quot;J&quot;: wordnet.ADJ,
                &quot;N&quot;: wordnet.NOUN,
                &quot;V&quot;: wordnet.VERB,
                &quot;R&quot;: wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)</code></pre>
<pre class="r"><code>def norm_lemm_POS_tag_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() to lemmatize the created tokens
            POS tag is determined with the help of function get_wordnet_pos()
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39; 
    words = word_tokenize(text)
    text = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, get_wordnet_pos_func(word)) for word in words])
    return text</code></pre>
<pre class="r"><code>print(&#39;POS tag for the word &quot;dog&quot;: &#39; + str(get_wordnet_pos_func(&quot;dog&quot;)))
print(&#39;POS tag for the word &quot;going&quot;: &#39; + str(get_wordnet_pos_func(&quot;going&quot;)))
print(&#39;POS tag for the word &quot;good&quot;: &#39; + str(get_wordnet_pos_func(&quot;good&quot;)))</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p12.png" /></p>
<pre class="r"><code>norm_lemm_POS_tag_func(text_for_normalization)</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p13.png" /></p>
</div>
<div id="multiple-specific-wordnet-lemmatizer" class="section level4">
<h4>4.5.2.3 Multiple specific Wordnet Lemmatizer</h4>
<p>Another alternative is to run two lemmatization algorithms with different specific POS tags one after the other:</p>
<pre class="r"><code>text_for_norm_v_lemmatized = norm_lemm_v_func(text_for_normalization)
text_for_norm_n_lemmatized = norm_lemm_a_func(text_for_norm_v_lemmatized)
text_for_norm_n_lemmatized</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p14.png" /></p>
<p>Personally, I find it easier to write this back into a function:</p>
<pre class="r"><code>def norm_lemm_v_a_func(text):
    &#39;&#39;&#39;
    Lemmatize tokens from string
    
    Step 1: Use word_tokenize() to get tokens from string
    Step 2: Use WordNetLemmatizer() with POS tag &#39;v&#39; to lemmatize the created tokens
    Step 3: Use word_tokenize() to get tokens from generated string        
    Step 4: Use WordNetLemmatizer() with POS tag &#39;a&#39; to lemmatize the created tokens
    
    Args:
        text (str): String to which the functions are to be applied, string
    
    Returns:
        String with lemmatized words
    &#39;&#39;&#39;
    words1 = word_tokenize(text)
    text1 = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, pos=&#39;v&#39;) for word in words1])
    words2 = word_tokenize(text1)
    text2 = &#39; &#39;.join([WordNetLemmatizer().lemmatize(word, pos=&#39;a&#39;) for word in words2])
    return text2</code></pre>
<pre class="r"><code>norm_lemm_v_a_func(text_for_normalization)</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p15.png" /></p>
</div>
</div>
<div id="application-to-the-example-string" class="section level3">
<h3>4.5.3 <strong>Application</strong> to the Example String</h3>
<pre class="r"><code>clean_text_wo_stop_words</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p16.png" /></p>
<p><strong>with norm_lemm_v_a_func</strong></p>
<pre class="r"><code>clean_text_lemmatized_v_a = norm_lemm_v_a_func(clean_text_wo_stop_words)
clean_text_lemmatized_v_a</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p17.png" /></p>
<p><strong>with norm_lemm_POS_tag_func</strong></p>
<pre class="r"><code>clean_text_lemmatized_pos_tag = norm_lemm_POS_tag_func(clean_text_wo_stop_words)
clean_text_lemmatized_pos_tag</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p18.png" /></p>
<p>This example shows the advantages and disadvantages of the two variants.
With the v_a_func, ‘bought’ became ‘buy’ and ‘paid’ became ‘pay’. On the other hand, ‘rating’ became ‘rat’.</p>
<p>With the pos_tag function, on the other hand, the past tense remained, but ‘rating’ was not reformatted to the word rat.</p>
<p>This is the well-known <a href="https://www.kdnuggets.com/2019/09/no-free-lunch-data-science.html">No Free Lunch Theorem</a> where we have to weigh up which variant with which advantages and disadvantages we want to take.</p>
<p><strong>In this example, I will continue to work with the norm_lemm_v_a function for the Example String and the DataFrame.</strong></p>
</div>
<div id="application-to-the-dataframe" class="section level3">
<h3>4.5.4 <strong>Application</strong> to the DataFrame</h3>
<pre class="r"><code>df.head(3).T</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p19.png" /></p>
<pre class="r"><code>df[&#39;Reviews_lemmatized&#39;] = df[&#39;Reviews_wo_Stop_Words&#39;].apply(norm_lemm_v_a_func)

df[&#39;Word_Count_lemmatized_Reviews&#39;] = df[&#39;Reviews_lemmatized&#39;].apply(word_count_func)

df.head(3).T</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p20.png" /></p>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>5 Conclusion</h1>
<p>In this part of the Text Pre-Processing series, I have given a brief example explanation of what POS and NER are and what these two techniques are to be used for.</p>
<p>Furthermore, I went into detail about normalization techniques for text data.</p>
<p>I save the edited DataFrame and Example String again for subsequent use.</p>
<pre class="r"><code>pk.dump(clean_text_lemmatized_v_a, open(&#39;clean_text_lemmatized_v_a.pkl&#39;, &#39;wb&#39;))

df.to_csv(&#39;Amazon_Unlocked_Mobile_small_Part_III.csv&#39;, index = False)</code></pre>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p.png" /></p>
<p><img src="/post/2021-05-31-nlp-text-pre-processing-iii-pos-ner-and-normalization_files/p125p.png" /></p>
</div>
