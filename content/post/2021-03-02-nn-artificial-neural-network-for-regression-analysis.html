---
title: NN - Artificial Neural Network for Regression Analysis
author: Michael Fuchs
date: '2021-03-02'
slug: nn-artificial-neural-network-for-regression-analysis
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#loading-the-libraries">2 Loading the libraries</a></li>
<li><a href="#loading-the-data">3 Loading the data</a></li>
<li><a href="#data-pre-processing">4 Data pre-processing</a>
<ul>
<li><a href="#determination-of-the-predictors-and-the-criterion">4.1 Determination of the predictors and the criterion</a></li>
<li><a href="#train-validation-test-split">4.2 Train-Validation-Test Split</a></li>
<li><a href="#scaling">4.3 Scaling</a></li>
</ul></li>
<li><a href="#ann-for-regression">5 ANN for Regression</a>
<ul>
<li><a href="#name-definitions">5.1 Name Definitions</a></li>
<li><a href="#parameter-settings">5.2 Parameter Settings</a></li>
<li><a href="#layer-structure">5.3 Layer Structure</a></li>
<li><a href="#configuring-the-model-for-training">5.4 Configuring the model for training</a></li>
<li><a href="#callbacks">5.5 Callbacks</a></li>
<li><a href="#fitting-the-model">5.6 Fitting the model</a></li>
<li><a href="#obtaining-the-best-model-values">5.7 Obtaining the best model values</a></li>
<li><a href="#storing-all-necessary-metrics">5.8 Storing all necessary metrics</a></li>
<li><a href="#validation">5.9 Validation</a>
<ul>
<li><a href="#metrics-from-model-training-history">5.9.1 Metrics from model training (history)</a></li>
<li><a href="#k-fold-cross-validation">5.9.2 K-fold cross validation</a>
<ul>
<li><a href="#determination-of-the-layer-structure-as-well-as-the-number-of-cross-validations">5.9.2.1 Determination of the layer structure as well as the number of cross-validations</a></li>
<li><a href="#obtaining-the-mae-for-each-fold">5.9.2.2 Obtaining the MAE for each fold</a></li>
<li><a href="#obtaining-the-mae-for-each-epoch">5.9.2.3 Obtaining the MAE for each epoch</a></li>
</ul></li>
</ul></li>
<li><a href="#load-best-model">5.10 Load best model</a></li>
<li><a href="#model-testing">5.11 Model Testing</a></li>
<li><a href="#predictions">5.12 Predictions</a></li>
<li><a href="#evaluation">5.13 Evaluation</a></li>
</ul></li>
<li><a href="#prevent-overfitting">6 Prevent Overfitting</a></li>
<li><a href="#conclusion">7 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>Now that I have shown how to solve classification problems (<a href="https://michael-fuchs-python.netlify.app/2021/02/16/nn-artificial-neural-network-for-binary-classification/">binary</a> and <a href="https://michael-fuchs-python.netlify.app/2021/02/23/nn-artificial-neural-network-for-multi-class-classfication/">multi-class</a>) with <a href="https://keras.io/">Keras</a>, I would like to show how to solve regression problems as well.</p>
<p>For this publication the dataset <em>House Sales in King County, USA</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used. You can download it from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets">“GitHub Repository”</a>.</p>
</div>
<div id="loading-the-libraries" class="section level1">
<h1>2 Loading the libraries</h1>
<pre class="r"><code>import pandas as pd
import numpy as np
import os
import shutil
import pickle as pk
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from keras import models
from keras import layers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model

from sklearn import metrics</code></pre>
</div>
<div id="loading-the-data" class="section level1">
<h1>3 Loading the data</h1>
<pre class="r"><code>df = pd.read_csv(&#39;house_prices.csv&#39;)
df = df.drop([&#39;id&#39;, &#39;date&#39;, &#39;yr_built&#39;, &#39;yr_renovated&#39;, &#39;zipcode&#39;, &#39;lat&#39;, &#39;long&#39;], axis=1)
df</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p1.png" /></p>
</div>
<div id="data-pre-processing" class="section level1">
<h1>4 Data pre-processing</h1>
<div id="determination-of-the-predictors-and-the-criterion" class="section level2">
<h2>4.1 Determination of the predictors and the criterion</h2>
<pre class="r"><code>x = df.drop(&#39;price&#39;, axis=1)
y = df[&#39;price&#39;]</code></pre>
</div>
<div id="train-validation-test-split" class="section level2">
<h2>4.2 Train-Validation-Test Split</h2>
<p>In the following, I will randomly assign 70% of the data to the training part and 15% each to the validation and test part.</p>
<pre class="r"><code>train_ratio = 0.70
validation_ratio = 0.15
test_ratio = 0.15

# Generate TrainX and TrainY
trainX, testX, trainY, testY = train_test_split(x, y, test_size= 1 - train_ratio)
# Genearate ValX, TestX, ValY and TestY
valX, testX, valY, testY = train_test_split(testX, testY, test_size=test_ratio/(test_ratio + validation_ratio))</code></pre>
<pre class="r"><code>print(trainX.shape)
print(valX.shape)
print(testX.shape)</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p2.png" /></p>
</div>
<div id="scaling" class="section level2">
<h2>4.3 Scaling</h2>
<pre class="r"><code>sc=StandardScaler()

scaler = sc.fit(trainX)

trainX_scaled = scaler.transform(trainX)
valX_scaled = scaler.transform(valX)
testX_scaled = scaler.transform(testX)</code></pre>
</div>
</div>
<div id="ann-for-regression" class="section level1">
<h1>5 ANN for Regression</h1>
<div id="name-definitions" class="section level2">
<h2>5.1 Name Definitions</h2>
<pre class="r"><code>checkpoint_no = &#39;ckpt_1_ANN&#39;
model_name = &#39;House_ANN_2FC_F64_64_epoch_120&#39;</code></pre>
</div>
<div id="parameter-settings" class="section level2">
<h2>5.2 Parameter Settings</h2>
<pre class="r"><code>input_shape = trainX.shape[1]

n_batch_size = 128

n_steps_per_epoch = int(trainX.shape[0] / n_batch_size)
n_validation_steps = int(valX.shape[0] / n_batch_size)
n_test_steps = int(testX.shape[0] / n_batch_size)

n_epochs = 120


print(&#39;Input Shape: &#39; + str(input_shape))
print(&#39;Batch Size: &#39; + str(n_batch_size))
print()
print(&#39;Steps per Epoch: &#39; + str(n_steps_per_epoch))
print()
print(&#39;Validation Steps: &#39; + str(n_validation_steps))
print(&#39;Test Steps: &#39; + str(n_test_steps))
print()
print(&#39;Number of Epochs: &#39; + str(n_epochs))</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p3.png" /></p>
</div>
<div id="layer-structure" class="section level2">
<h2>5.3 Layer Structure</h2>
<pre class="r"><code>model = models.Sequential()
model.add(layers.Dense(64, activation=&#39;relu&#39;, input_shape=(input_shape,)))
model.add(layers.Dense(64, activation=&#39;relu&#39;))
model.add(layers.Dense(1))</code></pre>
<pre class="r"><code>model.summary()</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p4.png" /></p>
</div>
<div id="configuring-the-model-for-training" class="section level2">
<h2>5.4 Configuring the model for training</h2>
<pre class="r"><code>model.compile(loss=&#39;mse&#39;,
              optimizer=&#39;rmsprop&#39;,
              metrics=[&#39;mae&#39;])</code></pre>
</div>
<div id="callbacks" class="section level2">
<h2>5.5 Callbacks</h2>
<p>If you want to know more about callbacks you can read about it here at <a href="https://keras.io/api/callbacks/">Keras</a> or also in my post about <a href="https://michael-fuchs-python.netlify.app/2021/01/08/computer-vision-convolutional-neural-network/#callbacks">Convolutional Neural Networks</a>.</p>
<pre class="r"><code># Prepare a directory to store all the checkpoints.
checkpoint_dir = &#39;./&#39;+ checkpoint_no
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)</code></pre>
<pre class="r"><code>keras_callbacks = [ModelCheckpoint(filepath = checkpoint_dir + &#39;/&#39; + model_name, 
                                   monitor=&#39;val_loss&#39;, save_best_only=True, mode=&#39;auto&#39;)]</code></pre>
</div>
<div id="fitting-the-model" class="section level2">
<h2>5.6 Fitting the model</h2>
<pre class="r"><code>history = model.fit(trainX_scaled,
                    trainY,
                    steps_per_epoch=n_steps_per_epoch,
                    epochs=n_epochs,
                    batch_size=n_batch_size,
                    validation_data=(valX_scaled, valY),
                    validation_steps=n_validation_steps,
                    callbacks=[keras_callbacks])</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p5.png" /></p>
</div>
<div id="obtaining-the-best-model-values" class="section level2">
<h2>5.7 Obtaining the best model values</h2>
<pre class="r"><code>hist_df = pd.DataFrame(history.history)
hist_df[&#39;epoch&#39;] = hist_df.index + 1
cols = list(hist_df.columns)
cols = [cols[-1]] + cols[:-1]
hist_df = hist_df[cols]
hist_df.to_csv(checkpoint_no + &#39;/&#39; + &#39;history_df_&#39; + model_name + &#39;.csv&#39;)
hist_df.head()</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p6.png" /></p>
<pre class="r"><code>values_of_best_model = hist_df[hist_df.val_loss == hist_df.val_loss.min()]
values_of_best_model</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p7.png" /></p>
</div>
<div id="storing-all-necessary-metrics" class="section level2">
<h2>5.8 Storing all necessary metrics</h2>
<p>After we have used the StandardScaler in <a href="https://michael-fuchs-python.netlify.app/2021/03/02/nn-artificial-neural-network-for-regression-analysis/#scaling">chapter 4.3</a>, we should also save it for later use.</p>
<pre class="r"><code>pk.dump(scaler, open(checkpoint_no + &#39;/&#39; + &#39;scaler.pkl&#39;, &#39;wb&#39;))</code></pre>
</div>
<div id="validation" class="section level2">
<h2>5.9 Validation</h2>
<div id="metrics-from-model-training-history" class="section level3">
<h3>5.9.1 Metrics from model training (history)</h3>
<pre class="r"><code>mae = history.history[&#39;mae&#39;]
val_mae = history.history[&#39;val_mae&#39;]
loss = history.history[&#39;loss&#39;]
val_loss = history.history[&#39;val_loss&#39;]

epochs = range(1, len(mae) + 1)

plt.plot(epochs, mae, &#39;bo&#39;, label=&#39;Training MAE&#39;)
plt.plot(epochs, val_mae, &#39;b&#39;, label=&#39;Validation MAE&#39;)
plt.title(&#39;Training and validation MAE&#39;)
plt.legend()

plt.figure()

plt.plot(epochs, loss, &#39;bo&#39;, label=&#39;Training loss&#39;)
plt.plot(epochs, val_loss, &#39;b&#39;, label=&#39;Validation loss&#39;)
plt.title(&#39;Training and validation loss&#39;)
plt.legend()

plt.show()</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p8.png" /></p>
</div>
<div id="k-fold-cross-validation" class="section level3">
<h3>5.9.2 K-fold cross validation</h3>
<p>In the following, I will perform cross-validation for the selected layer structure (<a href="https://michael-fuchs-python.netlify.app/2021/03/02/nn-artificial-neural-network-for-regression-analysis/#layer-structure">chapter 5.3</a>) and the specified parameter (<a href="https://michael-fuchs-python.netlify.app/2021/03/02/nn-artificial-neural-network-for-regression-analysis/#parameter-settings">chapter 5.2</a>). The cross-validation is performed on the trainX_scaled and trainY parts, since the metrics from the model training (<a href="https://michael-fuchs-python.netlify.app/2021/03/02/nn-artificial-neural-network-for-regression-analysis/#metrics-from-model-training-history">chapter 5.9.1</a>) were also created based only on these data and the test part remains untouched until the end.</p>
<div id="determination-of-the-layer-structure-as-well-as-the-number-of-cross-validations" class="section level4">
<h4>5.9.2.1 Determination of the layer structure as well as the number of cross-validations</h4>
<p>In order to be able to validate the layered structure defined in <a href="https://michael-fuchs-python.netlify.app/2021/03/02/nn-artificial-neural-network-for-regression-analysis/#layer-structure">chapter 5.3</a> in a meaningful way, the same structure must of course also be used here. The same applies to the parameters defined in <a href="https://michael-fuchs-python.netlify.app/2021/03/02/nn-artificial-neural-network-for-regression-analysis/#parameter-settings">chapter 5.2</a>.</p>
<pre class="r"><code>def build_model():
    model = models.Sequential()
    model.add(layers.Dense(64, activation=&#39;relu&#39;,
                           input_shape=(input_shape,)))
    model.add(layers.Dense(64, activation=&#39;relu&#39;))
    model.add(layers.Dense(1))
    model.compile(loss=&#39;mse&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;mae&#39;])
    return model</code></pre>
<p>Here we only define the number of cross validations that should be performed.</p>
<pre class="r"><code>k = 5
num_val_samples = len(trainX) // k</code></pre>
</div>
<div id="obtaining-the-mae-for-each-fold" class="section level4">
<h4>5.9.2.2 Obtaining the MAE for each fold</h4>
<p>Here, each MAE for each fold is stored in <code>all_scores</code>.</p>
<pre class="r"><code>all_scores = []
for i in range(k):
    print(&#39;Processing Fold&#39;, i)
    val_data = trainX_scaled[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = trainY[i * num_val_samples: (i + 1) * num_val_samples]

    partial_train_data = np.concatenate(
        [trainX_scaled[:i * num_val_samples],
         trainX_scaled[(i + 1) * num_val_samples:]],
        axis=0)
    partial_train_targets = np.concatenate(
        [trainY[:i * num_val_samples],
         trainY[(i + 1) * num_val_samples:]],
        axis=0)

    model = build_model()
    model.fit(partial_train_data, partial_train_targets,
              epochs=n_epochs, batch_size=n_batch_size, verbose=0)
    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
    all_scores.append(val_mae)
    print(&#39;MAE: &#39; + str(val_mae))
    print(&#39;----------------------&#39;)</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p9.png" /></p>
<pre class="r"><code>for i, val in enumerate(all_scores):
    print(&#39;Fold &#39; + str(i) +&#39;: &#39; + &#39;MAE of&#39;, val)</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p10.png" /></p>
<pre class="r"><code>print(&#39;Mean MAE of all folds: &#39; + str(np.mean(all_scores)))</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p11.png" /></p>
</div>
<div id="obtaining-the-mae-for-each-epoch" class="section level4">
<h4>5.9.2.3 Obtaining the MAE for each epoch</h4>
<p>Here, each MAE of each step for each epoch for each epoch is stored in <code>all_mae_histories</code>.</p>
<pre class="r"><code>all_mae_histories = []
for i in range(k):
    print(&#39;Processing Fold&#39;, i)
    val_data = trainX_scaled[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = trainY[i * num_val_samples: (i + 1) * num_val_samples]
    partial_train_data = np.concatenate(
        [trainX_scaled[:i * num_val_samples],
         trainX_scaled[(i + 1) * num_val_samples:]],
        axis=0)
    partial_train_targets = np.concatenate(
        [trainY[:i * num_val_samples],
         trainY[(i + 1) * num_val_samples:]],
        axis=0)

    model = build_model()
    history = model.fit(partial_train_data, partial_train_targets,
                        validation_data=(val_data, val_targets),
                        epochs=n_epochs, batch_size=n_batch_size, verbose=0)
    mae_history = history.history[&#39;val_mae&#39;]
    all_mae_histories.append(mae_history)</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p12.png" /></p>
<p>Here we now calculate the average MAE achieved per epoch.</p>
<pre class="r"><code>average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(n_epochs)]

len(average_mae_history)</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p13.png" /></p>
<pre class="r"><code>plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
plt.title(&#39;Validation MAE per Epoch&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Validation MAE&#39;)
plt.show()</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p14.png" /></p>
<p>With real-world data, we often get messy curves. Here the following function can help:</p>
<pre class="r"><code>def smooth_curve(points, factor=0.9):
      &#39;&#39;&#39;
      Function for smoothing data points

      Args:
          points (float64): Array of floats to be smoothed, numpy array of floats

      Returns:
          Smoothed data points
      &#39;&#39;&#39;  
      smoothed_points = []
      for point in points:
        if smoothed_points:
          previous = smoothed_points[-1]
          smoothed_points.append(previous * factor + point * (1 - factor))
        else:
          smoothed_points.append(point)
      return smoothed_points</code></pre>
<p>Here we also have the option to exclude the first n values from the graph. So that the graphic does not become misleading with regard to the displayed epochs, I change the index accordingly before I create the plot.</p>
<pre class="r"><code>n_first_observations_to_exclude = 30

smooth_mae_history = smooth_curve(average_mae_history[n_first_observations_to_exclude:])

smooth_mae_history = pd.DataFrame(smooth_mae_history)
smooth_mae_history = smooth_mae_history.set_index(smooth_mae_history.index + n_first_observations_to_exclude)
smooth_mae_history.head()</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p15.png" /></p>
<pre class="r"><code>plt.plot(smooth_mae_history)
plt.title(&#39;Validation MAE per Epoch&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Validation MAE&#39;)
plt.show()</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p16.png" /></p>
</div>
</div>
</div>
<div id="load-best-model" class="section level2">
<h2>5.10 Load best model</h2>
<p>Again, reference to the <a href="https://michael-fuchs-python.netlify.app/2021/01/08/computer-vision-convolutional-neural-network/#load-best-model">Computer Vision posts</a> where I explained why and how I cleaned up the Model Checkpoint folders.</p>
<pre class="r"><code># Loading the automatically saved model
model_reloaded = load_model(checkpoint_no + &#39;/&#39; + model_name)

# Saving the best model in the correct path and format
root_directory = os.getcwd()
checkpoint_dir = os.path.join(root_directory, checkpoint_no)
model_name_temp = os.path.join(checkpoint_dir, model_name + &#39;.h5&#39;)
model_reloaded.save(model_name_temp)

# Deletion of the automatically created folder under Model Checkpoint File.
folder_name_temp = os.path.join(checkpoint_dir, model_name)
shutil.rmtree(folder_name_temp, ignore_errors=True)</code></pre>
<pre class="r"><code>best_model = load_model(model_name_temp)</code></pre>
<p>The overall folder structure should look like this:</p>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114s1.png" /></p>
</div>
<div id="model-testing" class="section level2">
<h2>5.11 Model Testing</h2>
<pre class="r"><code>test_loss, test_mae = best_model.evaluate(testX_scaled,
                                          testY,
                                          steps=n_test_steps)
print()
print(&#39;Test MAE:&#39;, test_mae)</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p17.png" /></p>
</div>
<div id="predictions" class="section level2">
<h2>5.12 Predictions</h2>
<pre class="r"><code>y_pred = model.predict(testX_scaled)
y_pred[:5]</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p18.png" /></p>
</div>
<div id="evaluation" class="section level2">
<h2>5.13 Evaluation</h2>
<pre class="r"><code>df_testY = pd.DataFrame(testY)
df_y_pred = pd.DataFrame(y_pred)

df_testY.reset_index(drop=True, inplace=True)
df_y_pred.reset_index(drop=True, inplace=True)

df_results = pd.concat([df_testY, df_y_pred], axis=1)
df_results.columns = [&#39;Actual&#39;, &#39;Predicted&#39;]

df_results</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p19.png" /></p>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(testY, y_pred))
print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(testY, y_pred))
print(&#39;Root Mean Squared Error:&#39;, metrics.mean_squared_error(testY, y_pred, squared=False))
print(&#39;Mean Absolute Percentage Error:&#39;, metrics.mean_absolute_percentage_error(testY, y_pred))</code></pre>
<p><img src="/post/2021-03-02-nn-artificial-neural-network-for-regression-analysis_files/p114p20.png" /></p>
<p>Now why is this designated MAE (150875) larger than the <a href="https://michael-fuchs-python.netlify.app/2021/03/02/nn-artificial-neural-network-for-regression-analysis/#model-testing">test MAE</a> (147006)?</p>
<p>This is because when we test MAE with the .evaluate() function, we go through multiple steps (25 in this case) and a separate MAE is calculated for each. On average we get a MAE of 147006 with the .evaluate() function.</p>
</div>
</div>
<div id="prevent-overfitting" class="section level1">
<h1>6 Prevent Overfitting</h1>
<p>At this point I would like to remind you of the topic of overfitting. In my post (<a href="https://michael-fuchs-python.netlify.app/2021/02/16/nn-artificial-neural-network-for-binary-classification/#prevent-overfitting">Artificial Neural Network for binary Classification</a>) I explained in more detail what can be done against overfitting. Here again a list with the corresponding links:</p>
<ul>
<li><a href="https://michael-fuchs-python.netlify.app/2021/02/16/nn-artificial-neural-network-for-binary-classification/#reduce-the-networks-size">Reduce the network’s size</a></li>
<li><a href="https://michael-fuchs-python.netlify.app/2021/02/16/nn-artificial-neural-network-for-binary-classification/#adding-weight-regularization">Adding weight regularization</a></li>
<li><a href="https://michael-fuchs-python.netlify.app/2021/02/16/nn-artificial-neural-network-for-binary-classification/#adding-dropout">Adding dropout</a></li>
</ul>
</div>
<div id="conclusion" class="section level1">
<h1>7 Conclusion</h1>
<p>Again, as a reminder which metrics should be stored additionally when using neural networks in real life:</p>
<ul>
<li>Mean values of the individual predictors in order to be able to compensate for missing values later on.</li>
<li>Encoders for predictors, if categorical features are converted.</li>
<li>If variables would have been excluded, a list with the final features should have been stored.</li>
</ul>
<p>For what reason I give these recommendations can be well read in my <a href="https://michael-fuchs-python.netlify.app/2020/08/21/the-data-science-process-crisp-dm/">Data Science Post</a>. Here I have also created <a href="https://michael-fuchs-python.netlify.app/2020/08/21/the-data-science-process-crisp-dm/#data-science-best-practice-guidlines-for-ml-model-development">best practice guidelines</a> on how to proceed with model training.</p>
<p><strong>References</strong></p>
<p>The content of the entire post was created using the following sources:</p>
<p>Chollet, F. (2018). Deep learning with Python (Vol. 361). New York: Manning.</p>
</div>
