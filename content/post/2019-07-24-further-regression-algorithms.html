---
title: Further Regression Algorithms
author: Michael Fuchs
date: '2019-07-24'
slug: further-regression-algorithms
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#loading-the-libraries-and-the-data">2 Loading the libraries and the data</a></li>
<li><a href="#linear-regression">3 Linear Regression</a></li>
<li><a href="#decision-tree-regression">4 Decision Tree Regression</a></li>
<li><a href="#support-vector-machines-regression">5 Support Vector Machines Regression</a></li>
<li><a href="#stochastic-gradient-descent-sgd-regression">6 Stochastic Gradient Descent (SGD) Regression</a></li>
<li><a href="#knn-regression">7 KNN Regression</a></li>
<li><a href="#ensemble-modeling">8 Ensemble Modeling</a>
<ul>
<li><a href="#bagging-regressor">8.1 Bagging Regressor</a></li>
<li><a href="#bagging-regressor-with-decision-tree-reg-as-base_estimator">8.2 Bagging Regressor with Decision Tree Reg as base_estimator</a></li>
<li><a href="#random-forest-regressor">8.3 Random Forest Regressor</a></li>
<li><a href="#adaboost-regressor">8.4 AdaBoost Regressor</a></li>
<li><a href="#adaboost-regressor-with-decision-tree-reg-as-base_estimator">8.5 AdaBoost Regressor with Decision Tree Reg as base_estimator</a></li>
<li><a href="#gradient-boosting-regressor">8.6 Gradient Boosting Regressor</a></li>
<li><a href="#stacking-regressor">8.7 Stacking Regressor</a></li>
</ul></li>
<li><a href="#overview-results">9 Overview Results</a></li>
<li><a href="#conclusion">10 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>In previous publications I have covered regression models from the scikit-learn library and the statsmodel library.
But besides these, there are a lot of other machine learning algorithms that can be used to create regression models.
In this publication I would like to introduce them to you.</p>
<p>Short remark in advance:
I will not go into the exact functioning of the different algorithms below. In the end I have provided a number of links to further publications of mine in which I explain the algorithms used in detail.</p>
<p>For this post the dataset <em>House Sales in King County, USA</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used. You can download it from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets">GitHub Repository</a>.</p>
<p>The goal is to find an algorithm that can best predict house prices.
The results of the algorithms will be stored in variables and presented in an overview at the end.</p>
</div>
<div id="loading-the-libraries-and-the-data" class="section level1">
<h1>2 Loading the libraries and the data</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV


from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.linear_model import SGDRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge</code></pre>
<pre class="r"><code>house = pd.read_csv(&quot;path/to/file/house_prices.csv&quot;)</code></pre>
<pre class="r"><code>house = house.drop([&#39;zipcode&#39;, &#39;lat&#39;, &#39;long&#39;, &#39;date&#39;, &#39;id&#39;], axis=1)
house.head()</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p1.png" /></p>
<pre class="r"><code>x = house.drop(&#39;price&#39;, axis=1)
y = house[&#39;price&#39;]
trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
</div>
<div id="linear-regression" class="section level1">
<h1>3 Linear Regression</h1>
<pre class="r"><code>lm = LinearRegression()

lm.fit(trainX, trainY)
y_pred = lm.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p2.png" /></p>
<pre class="r"><code>mae_lm = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_lm = lm.score(trainX, trainY)</code></pre>
</div>
<div id="decision-tree-regression" class="section level1">
<h1>4 Decision Tree Regression</h1>
<pre class="r"><code>dt_reg = DecisionTreeRegressor() 

dt_reg.fit(trainX, trainY)
y_pred = dt_reg.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p3.png" /></p>
<pre class="r"><code>mae_dt_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_dt_reg = dt_reg.score(trainX, trainY)</code></pre>
<pre class="r"><code>param_grid = {&quot;criterion&quot;: [&quot;mse&quot;, &quot;mae&quot;],
              &quot;min_samples_split&quot;: [10, 20, 40],
              &quot;max_depth&quot;: [2, 6, 8],
              &quot;min_samples_leaf&quot;: [20, 40, 100],
              &quot;max_leaf_nodes&quot;: [5, 20, 100],
              }</code></pre>
<pre class="r"><code>grid_dt_reg = GridSearchCV(dt_reg, param_grid, cv=5, n_jobs = -1) </code></pre>
<pre class="r"><code>grid_dt_reg.fit(trainX, trainY)</code></pre>
<pre class="r"><code>print(grid_dt_reg.best_params_)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p4.png" /></p>
<pre class="r"><code>y_pred = grid_dt_reg.predict(testX)

print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p5.png" /></p>
<pre class="r"><code>mae_grid_dt_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_grid_dt_reg = grid_dt_reg.score(trainX, trainY)</code></pre>
</div>
<div id="support-vector-machines-regression" class="section level1">
<h1>5 Support Vector Machines Regression</h1>
<pre class="r"><code>svr = SVR(kernel=&#39;rbf&#39;)

svr.fit(trainX, trainY)
y_pred = svr.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p6.png" /></p>
<pre class="r"><code>mae_svr = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_svr = svr.score(trainX, trainY)</code></pre>
<pre class="r"><code>k = [&#39;rbf&#39;]
c = [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]
g = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]

param_grid=dict(kernel=k, C=c, gamma=g)</code></pre>
<pre class="r"><code>grid_svr = GridSearchCV(svr, param_grid, cv=5, n_jobs = -1)
grid_svr.fit(trainX, trainY)</code></pre>
<pre class="r"><code>print(grid_svr.best_params_)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p7.png" /></p>
<pre class="r"><code>y_pred = grid_svr.predict(testX)

print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p8.png" /></p>
<pre class="r"><code>mae_grid_svr = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_grid_svr = grid_svr.score(trainX, trainY)</code></pre>
</div>
<div id="stochastic-gradient-descent-sgd-regression" class="section level1">
<h1>6 Stochastic Gradient Descent (SGD) Regression</h1>
<pre class="r"><code>n_iters = list(range(1,10,1))

scores = []
for n_iter in n_iters:
    sgd_reg = SGDRegressor(max_iter=n_iter)
    sgd_reg.fit(trainX, trainY)
    scores.append(sgd_reg.score(testX, testY))
  
plt.title(&quot;Effect of n_iter&quot;)
plt.xlabel(&quot;n_iter&quot;)
plt.ylabel(&quot;score&quot;)
plt.plot(n_iters, scores) </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p9.png" /></p>
<pre class="r"><code>n_iter=5
sgd_reg = SGDRegressor(max_iter=n_iter)

sgd_reg.fit(trainX, trainY)
y_pred = sgd_reg.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p10.png" /></p>
<pre class="r"><code>mae_sgd_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_sgd_reg = sgd_reg.score(trainX, trainY)</code></pre>
<pre class="r"><code>params = {&quot;alpha&quot; : [0.0001, 0.001, 0.01, 0.1],
    &quot;penalty&quot; : [&quot;l2&quot;, &quot;l1&quot;, &quot;elasticnet&quot;, &quot;none&quot;],
}</code></pre>
<pre class="r"><code>grid_sgd_reg = GridSearchCV(sgd_reg, param_grid=params, cv=5, n_jobs = -1)
grid_sgd_reg.fit(trainX, trainY)</code></pre>
<pre class="r"><code>print(grid_sgd_reg.best_params_)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p11.png" /></p>
<pre class="r"><code>y_pred = grid_sgd_reg.predict(testX)

print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p12.png" /></p>
<pre class="r"><code>mae_grid_sgd_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_grid_sgd_reg = grid_sgd_reg.score(trainX, trainY)</code></pre>
</div>
<div id="knn-regression" class="section level1">
<h1>7 KNN Regression</h1>
<pre class="r"><code>k_range = range(1, 33)
scores = {}
scores_list = []

k_range = range(1, 33)
scores = {}
scores_list = []

for k in k_range:
    knn_reg = KNeighborsRegressor(n_neighbors=k)
    knn_reg.fit(trainX, trainY)
    y_pred = knn_reg.predict(testX)
    scores[k] = metrics.mean_absolute_error(testY, y_pred)
    scores_list.append(metrics.mean_absolute_error(testY, y_pred))</code></pre>
<pre class="r"><code>plt.plot(k_range, scores_list)
plt.xlabel(&#39;Value of K for KNN_reg&#39;)
plt.ylabel(&#39;MSE&#39;)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p13.png" /></p>
<pre class="r"><code>n_eighbors = 16
knn_reg = KNeighborsRegressor(n_neighbors=n_eighbors)

knn_reg.fit(trainX, trainY)
y_pred = knn_reg.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2)) </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p14.png" /></p>
<pre class="r"><code>mae_knn_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_knn_reg = knn_reg.score(trainX, trainY)</code></pre>
<pre class="r"><code>k_range = list(range(1,15))
weight_options = [&quot;uniform&quot;, &quot;distance&quot;]
params = dict(n_neighbors=k_range, weights=weight_options)</code></pre>
<pre class="r"><code>grid_knn_reg = GridSearchCV(knn_reg, param_grid=params, cv=5, n_jobs = -1)
grid_knn_reg.fit(trainX, trainY)</code></pre>
<pre class="r"><code>print(grid_knn_reg.best_params_)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p15.png" /></p>
<pre class="r"><code>y_pred = grid_knn_reg.predict(testX)

print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p16.png" /></p>
<pre class="r"><code>mae_grid_knn_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_grid_knn_reg = grid_knn_reg.score(trainX, trainY)</code></pre>
</div>
<div id="ensemble-modeling" class="section level1">
<h1>8 Ensemble Modeling</h1>
<p>I’ll define a function that returns the cross-validation rmse error so we can evaluate our models and pick the best tuning part.</p>
<pre class="r"><code>def rmse_cv(model):
    rmse= np.sqrt(-cross_val_score(model, trainX, trainY, scoring=&quot;neg_mean_squared_error&quot;, cv = 5))
    return(rmse)</code></pre>
<div id="bagging-regressor" class="section level2">
<h2>8.1 Bagging Regressor</h2>
<pre class="r"><code>n_estimators = [170, 200, 250, 300]
cv_rmse_br = [rmse_cv(BaggingRegressor(n_estimators = n_estimator)).mean() 
            for n_estimator in n_estimators]</code></pre>
<pre class="r"><code>cv_br = pd.Series(cv_rmse_br , index = n_estimators)
cv_br.plot(title = &quot;Validation BaggingRegressor&quot;)
plt.xlabel(&quot;n_estimator&quot;)
plt.ylabel(&quot;rmse&quot;)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p17.png" /></p>
<pre class="r"><code>cv_br.min()</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p18.png" /></p>
<pre class="r"><code>n_estimators = 300

bagging_reg = BaggingRegressor(n_estimators = n_estimators)

bagging_reg.fit(trainX, trainY)
y_pred = bagging_reg.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2)) </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p19.png" /></p>
<pre class="r"><code>mae_bagging_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_bagging_reg = bagging_reg.score(trainX, trainY)</code></pre>
</div>
<div id="bagging-regressor-with-decision-tree-reg-as-base_estimator" class="section level2">
<h2>8.2 Bagging Regressor with Decision Tree Reg as base_estimator</h2>
<p>As the base estimator I’ll use a DecisionTreeRegressor with the best parameters calculated with grid search under chapter 4.</p>
<pre class="r"><code>dt_reg_with_grid_params = DecisionTreeRegressor(criterion = &#39;mse&#39;, max_depth= 8, max_leaf_nodes= 100, min_samples_leaf= 20, min_samples_split= 10) </code></pre>
<pre class="r"><code>n_estimators = 250

bc_params = {
    &#39;base_estimator&#39;: dt_reg_with_grid_params,
    &#39;n_estimators&#39;: n_estimators
}</code></pre>
<pre class="r"><code>bagging_reg_plus_dtr = BaggingRegressor(**bc_params)

bagging_reg_plus_dtr.fit(trainX, trainY)
y_pred = bagging_reg_plus_dtr.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p20.png" /></p>
<pre class="r"><code>mae_bagging_reg_plus_dtr = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_bagging_reg_plus_dtr = bagging_reg_plus_dtr.score(trainX, trainY)</code></pre>
</div>
<div id="random-forest-regressor" class="section level2">
<h2>8.3 Random Forest Regressor</h2>
<pre class="r"><code>n_estimators = 250

rf_reg = RandomForestRegressor(n_estimators = n_estimators)

rf_reg.fit(trainX, trainY)
y_pred = rf_reg.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p21.png" /></p>
<pre class="r"><code>mae_rf_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_rf_reg = rf_reg.score(trainX, trainY)</code></pre>
<pre class="r"><code>param_dist = {&quot;max_depth&quot;: list(range(3,20)),
              &quot;max_features&quot;: list(range(1, 10)),
              &quot;min_samples_split&quot;: list(range(2, 11)),
              &quot;bootstrap&quot;: [True, False],
              &quot;criterion&quot;: [&quot;mse&quot;, &quot;mae&quot;]}</code></pre>
<pre class="r"><code>n_iter_search = 10

rs_rf_reg = RandomizedSearchCV(rf_reg, param_distributions=param_dist,
                                   n_iter=n_iter_search, n_jobs = -1)
rs_rf_reg.fit(trainX, trainY)</code></pre>
<pre class="r"><code>print(rs_rf_reg.best_params_)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p22.png" /></p>
<pre class="r"><code>y_pred = rs_rf_reg.predict(testX)

print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p23.png" /></p>
<pre class="r"><code>mae_rs_rf_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_rs_rf_reg = rs_rf_reg.score(trainX, trainY)</code></pre>
</div>
<div id="adaboost-regressor" class="section level2">
<h2>8.4 AdaBoost Regressor</h2>
<pre class="r"><code>n_estimators = [170, 200, 250, 300]
cv_rmse_ab_reg = [rmse_cv(AdaBoostRegressor(n_estimators = n_estimator)).mean() 
            for n_estimator in n_estimators]</code></pre>
<pre class="r"><code>cv_ab_reg = pd.Series(cv_rmse_ab_reg , index = n_estimators)
cv_ab_reg.plot(title = &quot;Validation AdaBoost Regressor&quot;)
plt.xlabel(&quot;n_estimator&quot;)
plt.ylabel(&quot;rmse&quot;)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p24.png" /></p>
<pre class="r"><code>cv_ab_reg.min()</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p25.png" /></p>
<pre class="r"><code>n_estimators = 300

ab_reg = AdaBoostRegressor(n_estimators = n_estimators)

ab_reg.fit(trainX, trainY)
y_pred = ab_reg.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p26.png" /></p>
<pre class="r"><code>mae_ab_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_ab_reg = ab_reg.score(trainX, trainY)</code></pre>
</div>
<div id="adaboost-regressor-with-decision-tree-reg-as-base_estimator" class="section level2">
<h2>8.5 AdaBoost Regressor with Decision Tree Reg as base_estimator</h2>
<p>I will use again the DecisionTreeRegressor with the best parameters calculated with grid search under chapter 4.</p>
<pre class="r"><code>dt_reg_with_grid_params = DecisionTreeRegressor(criterion = &#39;mse&#39;, max_depth= 8, max_leaf_nodes= 100, min_samples_leaf= 20, min_samples_split= 10) </code></pre>
<pre class="r"><code>n_estimators = 300

ab_params = {
    &#39;n_estimators&#39;: n_estimators,
    &#39;base_estimator&#39;: dt_reg_with_grid_params
}</code></pre>
<pre class="r"><code>ab_reg_plus_dtr = AdaBoostRegressor(**ab_params)

ab_reg_plus_dtr.fit(trainX, trainY)
y_pred = ab_reg_plus_dtr.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p27.png" /></p>
<pre class="r"><code>mae_ab_reg_plus_dtr = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_ab_reg_plus_dtr = ab_reg_plus_dtr.score(trainX, trainY)</code></pre>
</div>
<div id="gradient-boosting-regressor" class="section level2">
<h2>8.6 Gradient Boosting Regressor</h2>
<pre class="r"><code>n_estimators = [150, 170 , 200, 400, 500]
cv_rmse_gb_reg = [rmse_cv(GradientBoostingRegressor(n_estimators = n_estimator)).mean() 
            for n_estimator in n_estimators]</code></pre>
<pre class="r"><code>cv_gb_reg = pd.Series(cv_rmse_gb_reg , index = n_estimators)
cv_gb_reg.plot(title = &quot;Validation Gradient Boosting Regressor&quot;)
plt.xlabel(&quot;n_estimator&quot;)
plt.ylabel(&quot;rmse&quot;)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p28.png" /></p>
<pre class="r"><code>cv_gb_reg.min()</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p29.png" /></p>
<pre class="r"><code>n_estimators = 500

gb_reg = GradientBoostingRegressor(n_estimators = n_estimators)

gb_reg.fit(trainX, trainY)
y_pred = gb_reg.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p30.png" /></p>
<pre class="r"><code>mae_gb_reg = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_gb_reg = gb_reg.score(trainX, trainY)</code></pre>
</div>
<div id="stacking-regressor" class="section level2">
<h2>8.7 Stacking Regressor</h2>
<pre class="r"><code>ridge = Ridge()
lasso = Lasso()</code></pre>
<pre class="r"><code>estimators = [
     (&#39;ridge&#39;, Ridge()),
     (&#39;lasso&#39;, Lasso())]</code></pre>
<pre class="r"><code>sr = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())

sr.fit(trainX, trainY)
y_pred = sr.predict(testX)</code></pre>
<pre class="r"><code>print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))  </code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p31.png" /></p>
<pre class="r"><code>mae_sr = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_sr = sr.score(trainX, trainY)</code></pre>
<pre class="r"><code>params = {&#39;lasso__alpha&#39;: [x*5.0 for x in range(1, 10)],
          &#39;ridge__alpha&#39;: [x/5.0 for x in range(1, 10)]}</code></pre>
<pre class="r"><code>grid_sr = GridSearchCV(sr, param_grid=params, cv=5)
grid_sr.fit(trainX, trainY)</code></pre>
<pre class="r"><code>print(grid_sr.best_params_)</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p32.png" /></p>
<pre class="r"><code>y_pred = grid_sr.predict(testX)

print(&#39;Mean Absolute Error:&#39;, round(metrics.mean_absolute_error(testY, y_pred), 2))</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p33.png" /></p>
<pre class="r"><code>mae_grid_sr = round(metrics.mean_absolute_error(testY, y_pred), 2)
r_grid_sr = grid_sr.score(trainX, trainY)</code></pre>
</div>
</div>
<div id="overview-results" class="section level1">
<h1>9 Overview Results</h1>
<p>Now it is time to collect the calculated results.</p>
<pre class="r"><code>column_names = [&quot;Algorithmus&quot;, &quot;MAE&quot;, &quot;R²&quot;]
df = pd.DataFrame(columns = column_names)

lm_df = pd.DataFrame([(&#39;lm&#39;, mae_lm, r_lm)], columns=column_names)
df = df.append(lm_df)

dt_reg_df = pd.DataFrame([(&#39;dt_reg&#39;, mae_dt_reg, r_dt_reg)], columns=column_names)
df = df.append(dt_reg_df)

grid_dt_reg_df = pd.DataFrame([(&#39;grid_dt_reg&#39;, mae_grid_dt_reg, r_grid_dt_reg)], columns=column_names)
df = df.append(grid_dt_reg_df)

svr_df = pd.DataFrame([(&#39;svr&#39;, mae_svr, r_svr)], columns=column_names)
df = df.append(svr_df)

grid_svr_df = pd.DataFrame([(&#39;grid_svr&#39;, mae_grid_svr, r_grid_svr)], columns=column_names)
df = df.append(grid_svr_df)

sgd_reg_df = pd.DataFrame([(&#39;sgd_reg&#39;, mae_sgd_reg, r_sgd_reg)], columns=column_names)
df = df.append(sgd_reg_df)

grid_sgd_reg_df = pd.DataFrame([(&#39;grid_sgd_reg&#39;, mae_grid_sgd_reg, r_grid_sgd_reg)], columns=column_names)
df = df.append(grid_sgd_reg_df)

knn_reg_df = pd.DataFrame([(&#39;knn_reg&#39;, mae_knn_reg, r_knn_reg)], columns=column_names)
df = df.append(knn_reg_df)

grid_knn_reg_df = pd.DataFrame([(&#39;grid_knn_reg&#39;, mae_grid_knn_reg, r_grid_knn_reg)], columns=column_names)
df = df.append(grid_knn_reg_df)

bagging_reg_df = pd.DataFrame([(&#39;bagging_reg&#39;, mae_bagging_reg, r_bagging_reg)], columns=column_names)
df = df.append(bagging_reg_df)

bagging_reg_plus_dtr_df = pd.DataFrame([(&#39;bagging_reg_plus_dtr&#39;, mae_bagging_reg_plus_dtr, r_bagging_reg_plus_dtr)], columns=column_names)
df = df.append(bagging_reg_plus_dtr_df)

rf_reg_df = pd.DataFrame([(&#39;rf_reg&#39;, mae_rf_reg, r_rf_reg)], columns=column_names)
df = df.append(rf_reg_df)

rs_rf_reg_df = pd.DataFrame([(&#39;rs_rf_reg&#39;, mae_rs_rf_reg, r_rs_rf_reg)], columns=column_names)
df = df.append(rs_rf_reg_df)

ab_reg_df = pd.DataFrame([(&#39;ab_reg&#39;, mae_ab_reg, r_ab_reg)], columns=column_names)
df = df.append(ab_reg_df)

ab_reg_plus_dtr_df = pd.DataFrame([(&#39;ab_reg_plus_dtr&#39;, mae_ab_reg_plus_dtr, r_ab_reg_plus_dtr)], columns=column_names)
df = df.append(ab_reg_plus_dtr_df)

gb_reg_df = pd.DataFrame([(&#39;gb_reg&#39;, mae_gb_reg, r_gb_reg)], columns=column_names)
df = df.append(gb_reg_df)

sr_df = pd.DataFrame([(&#39;sr&#39;, mae_sr, r_sr)], columns=column_names)
df = df.append(sr_df)

grid_sr_df = pd.DataFrame([(&#39;grid_sr&#39;, mae_grid_sr, r_grid_sr)], columns=column_names)
df = df.append(grid_sr_df)

df[&#39;MAE&#39;] = np.round(df[&#39;MAE&#39;], decimals=3)
df[&#39;R²&#39;] = np.round(df[&#39;R²&#39;], decimals=3)

df = df.rename(columns={&#39;R²&#39;: &#39;R&#39;})
df[&#39;R²&#39;] = abs(df.R)
df = df.drop(columns=[&#39;R&#39;])


df</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p34.png" /></p>
<p>Ok this overview is not really readable yet.
We can do better:</p>
<pre class="r"><code>pd.options.display.float_format = &#39;{:.5f}&#39;.format

df</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p35.png" /></p>
<p>Now we display the MAE values in ascending order.</p>
<pre class="r"><code>best_MAE = df.sort_values(by=&#39;MAE&#39;, ascending=True)
best_MAE</code></pre>
<p><img src="/post/2019-07-24-further-regression-algorithms_files/p68p36.png" /></p>
<p>From the overview we can see that the RandomForestRegressor is the algorithm that achieved the best results.</p>
<pre class="r"><code>pd.reset_option(&#39;display.float_format&#39;)</code></pre>
<p>What these metrics mean and how to interpret them I have described in the following post: <a href="https://michael-fuchs-python.netlify.app/2019/06/30/metrics-for-regression-analysis/">Metrics for Regression Analysis</a></p>
</div>
<div id="conclusion" class="section level1">
<h1>10 Conclusion</h1>
<p>In this post I have shown which different machine learning algorithms are available to create regression models.
The explanation of the exact functionality of the individual algorithms was not central.
But I did explain them when I used these algorithms for classification problems.
Have a look here:</p>
<ul>
<li><a href="https://michael-fuchs-python.netlify.com/2019/11/30/introduction-to-decision-trees/">Decision Trees</a></li>
<li><a href="https://michael-fuchs-python.netlify.com/2019/11/08/introduction-to-support-vector-machines/">Support Vector Machines</a></li>
<li><a href="https://michael-fuchs-python.netlify.com/2019/11/11/introduction-to-sgd-classifier/">SGD Classifier</a></li>
<li><a href="https://michael-fuchs-python.netlify.com/2019/12/27/introduction-to-knn-classifier/">K Nearest Neighbor Classifier</a></li>
<li><a href="https://michael-fuchs-python.netlify.app/2020/03/07/ensemble-modeling-bagging/">Bagging</a></li>
<li><a href="https://michael-fuchs-python.netlify.app/2020/03/26/ensemble-modeling-boosting/">Boosting</a></li>
<li><a href="https://michael-fuchs-python.netlify.app/2020/04/24/ensemble-modeling-stacking/">Stacking</a></li>
</ul>
</div>
