---
title: PCA for speed up ML models
author: Michael Fuchs
date: '2020-07-31'
slug: pca-for-speed-up-ml-models
categories:
  - R
tags:
  - R Markdown
---



<div id="table-of-content" class="section level1">
<h1>Table of Content</h1>
<ul>
<li>1 Introduction</li>
<li>2 Loading the libraries</li>
<li>3 LogReg</li>
<li>4 LogReg with PCA</li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>As already announced in post about <a href="https://michael-fuchs-python.netlify.app/2020/07/22/principal-component-analysis-pca/">“PCA”</a>, we now come to the second main application of a PCA: Principal Component Analysis for speed up machine learning models.</p>
<p>For this post the dataset <em>MNIST</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used. A copy of the record is available at <a href="https://drive.google.com/open?id=1Bfquk0uKnh6B3Yjh2N87qh0QcmLokrVk" class="uri">https://drive.google.com/open?id=1Bfquk0uKnh6B3Yjh2N87qh0QcmLokrVk</a>.</p>
</div>
<div id="loading-the-libraries-and-the-dataset" class="section level1">
<h1>2 Loading the libraries and the dataset</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

from sklearn.linear_model import LogisticRegression

from sklearn.decomposition import PCA</code></pre>
<pre class="r"><code>mnist = pd.read_csv(&#39;mnist_train.csv&#39;)
mnist</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p1.png" /></p>
<pre class="r"><code>mnist[&#39;label&#39;].value_counts().T</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p2.png" /></p>
</div>
<div id="logreg" class="section level1">
<h1>3 LogReg</h1>
<p>If you want to know how the algorithm of the logistic regression works exactly see <a href="https://michael-fuchs-python.netlify.app/2019/10/31/introduction-to-logistic-regression/">“this post”</a> of mine.</p>
<pre class="r"><code>x = mnist.drop([&#39;label&#39;], axis=1)
y = mnist[&#39;label&#39;]

trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
<pre class="r"><code>sc=StandardScaler()

# Fit on training set only!
sc.fit(trainX)

# Apply transform to both the training set and the test set.
trainX_scaled = sc.transform(trainX)
testX_scaled = sc.transform(testX)</code></pre>
<pre class="r"><code># all parameters not specified are set to their defaults

logReg = LogisticRegression()</code></pre>
<pre class="r"><code>import time

start = time.time()

print(logReg.fit(trainX_scaled, trainY))

end = time.time()
print()
print(&#39;Calculation time: &#39; + str(end - start) + &#39; seconds&#39;)</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p3.png" /></p>
<pre class="r"><code>y_pred = logReg.predict(testX_scaled)</code></pre>
<pre class="r"><code>print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred)))</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p4.png" /></p>
</div>
<div id="logreg-with-pca" class="section level1">
<h1>4 LogReg with PCA</h1>
<p>Notice the code below has .95 for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained.</p>
<pre class="r"><code>pca = PCA(.95)</code></pre>
<pre class="r"><code># Fitting PCA on the training set only
pca.fit(trainX_scaled)</code></pre>
<p>You can find out how many components PCA choose after fitting the model using pca.n_components_ . In this case, 95% of the variance amounts to 326 principal components.</p>
<pre class="r"><code>pca.n_components_</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p5.png" /></p>
<pre class="r"><code>trainX_pca = pca.transform(trainX_scaled)
testX_pca = pca.transform(testX_scaled)</code></pre>
<pre class="r"><code># all parameters not specified are set to their defaults

logReg = LogisticRegression()</code></pre>
<pre class="r"><code>import time

start = time.time()

print(logReg.fit(trainX_pca, trainY))

end = time.time()
print()
print(&#39;Calculation time: &#39; + str(end - start) + &#39; seconds&#39;)</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p6.png" /></p>
<pre class="r"><code>y_pred = logReg.predict(testX_pca)</code></pre>
<pre class="r"><code>print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred)))</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p7.png" /></p>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p.png" /></p>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p.png" /></p>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p.png" /></p>
</div>
