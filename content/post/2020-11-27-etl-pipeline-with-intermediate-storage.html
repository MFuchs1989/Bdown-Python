---
title: ETL - Pipeline with intermediate storage
author: Michael Fuchs
date: '2020-11-27'
slug: etl-pipeline-with-intermediate-storage
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#setup">2 Setup</a></li>
<li><a href="#etl-pipeline-with-intermediate-storage">3 ETL Pipeline with intermediate storage</a>
<ul>
<li><a href="#extract">3.1 Extract</a></li>
<li><a href="#transform_1">3.2 Transform_1</a></li>
<li><a href="#transform_2">3.3 Transform_2</a></li>
<li><a href="#load">3.4 Load</a></li>
</ul></li>
<li><a href="#create-etl_pipeline.py">4 Create etl_pipeline.py</a></li>
<li><a href="#test-etl_pipeline.py">5 Test etl_pipeline.py</a>
<ul>
<li><a href="#from-jupyter-notebook">5.1 from jupyter notebook</a>
<ul>
<li><a href="#the-very-first-time">5.1.1 the very first time</a></li>
<li><a href="#when-u-changed-sth.-within-preprocess_data">5.1.2 when u changed sth. within preprocess_data</a></li>
<li><a href="#when-u-continue-with-analytics">5.1.3 when u continue with analytics</a></li>
</ul></li>
<li><a href="#from-command-line">5.2 from command line</a></li>
</ul></li>
<li><a href="#conclusion">6 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>So far, we have already got to know several variants of ETL with which a large part of use cases can be covered.</p>
<p>But one important point has not been applied yet.</p>
<p>It often happens that the data has to be loaded or read out in an ‘unfavorable’ format.
Especially with large data sets this can take hours until you have the possibility to edit the data to make the loading process more effective.</p>
<p>At this point it is worthwhile to save the loaded data only partially processed.
So far we have always been lucky to be able to load, edit and save the data without any problems.
But if, as en example, numerical values are formatted as strings, the loading process can take an infinite amount of time. Hence this post about the introduction of an ETL with intermediate storage.</p>
<p><strong>Overview of the ETL steps:</strong></p>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90s1.png" /></p>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90s2.png" /></p>
<p>At the end I will test the created ETL. I will show this from another jupyter notebook and a fully automatic call from the command line.</p>
<p>For this post I use two specially created sample data sets. A copy of them is stored in my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets/ETL/Pipeline%20with%20intermediate%20storage">“GitHub Repo”</a>.</p>
</div>
<div id="setup" class="section level1">
<h1>2 Setup</h1>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p1.png" /></p>
<p>However, nothing changes in my used setup.
The used data sets are stored under data/input, the ETL is also in the data folder and for the used notebooks an extra notebook folder was created.The input_modified and output folders are automatically created by the ETL if not already present.</p>
</div>
<div id="etl-pipeline-with-intermediate-storage" class="section level1">
<h1>3 ETL Pipeline with intermediate storage</h1>
<pre class="r"><code>import pandas as pd</code></pre>
<div id="extract" class="section level2">
<h2>3.1 Extract</h2>
<pre class="r"><code>countries = pd.read_csv(&#39;../data/input/Countries.csv&#39;)
countries</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p2.png" /></p>
<pre class="r"><code>countries_metadata = pd.read_csv(&#39;../data/input/Countries_metadata.csv&#39;)
countries_metadata</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p3.png" /></p>
<p>We notice, that the values are provided with the addition V for value.
This leads to the fact that the variables Population and Land_Area are not numeric but objects.
This can lead to considerable performance problems and long loading times, especially with large data sets.
At this point it is recommended that the data set(s) be loaded once and saved temporarily.
Then the ETL pipeline should access the modified files, process them accordingly and finally save it in the output folder for final analysis.</p>
<pre class="r"><code>countries_metadata.dtypes</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p4.png" /></p>
</div>
<div id="transform_1" class="section level2">
<h2>3.2 Transform_1</h2>
<pre class="r"><code>countries.Population = countries.Population.map(lambda x: x.split(&#39;:&#39;)[1])
countries[&#39;Population&#39;] = countries[&#39;Population&#39;].astype(&#39;int64&#39;)
countries</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p5.png" /></p>
<pre class="r"><code>countries.dtypes</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p6.png" /></p>
<pre class="r"><code>countries_metadata.Land_Area = countries_metadata.Land_Area.map(lambda x: x.split(&#39;:&#39;)[1])
countries_metadata[&#39;Land_Area&#39;] = countries_metadata[&#39;Land_Area&#39;].astype(&#39;int64&#39;)
countries_metadata</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p7.png" /></p>
<p>At this point we want to store the two datasets with the correct data types in the input_modified folder.</p>
<pre class="r"><code>countries.to_csv(&#39;../data/input_modified/countries.csv&#39;)
countries_metadata.to_csv(&#39;../data/input_modified/output/countries_metadata.csv&#39;)</code></pre>
</div>
<div id="transform_2" class="section level2">
<h2>3.3 Transform_2</h2>
<p>Then we continue with the pre-processing steps.</p>
<pre class="r"><code>countries[&#39;Population&#39;] = countries[&#39;Population&#39;]/1000
countries = countries.rename(columns={&#39;Population&#39;:&#39;Population_per_k&#39;})
countries</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p8.png" /></p>
<pre class="r"><code>countries_metadata[&#39;Land_Area&#39;] = countries_metadata[&#39;Land_Area&#39;]/1000
countries_metadata = countries_metadata.rename(columns={&#39;Land_Area&#39;:&#39;Land_Area_per_k&#39;})
countries_metadata</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p9.png" /></p>
</div>
<div id="load" class="section level2">
<h2>3.4 Load</h2>
<pre class="r"><code>countries.to_csv(&#39;../data/output/countries.csv&#39;)
countries_metadata.to_csv(&#39;../data/output/countries_metadata.csv&#39;)</code></pre>
</div>
</div>
<div id="create-etl_pipeline.py" class="section level1">
<h1>4 Create etl_pipeline.py</h1>
<pre class="r"><code>import pandas as pd
import numpy as np
import os


class DataPreprocessor:
    def __init__(self, path_folder = &quot;path/to/data&quot;):

        self.path_folder = path_folder
        
        # Path to input
        self.path_input_folder = &quot;{}/input/&quot;.format(path_folder)
        self.path_input_countries = self.path_input_folder + &#39;Countries.csv&#39;
        self.path_input_countries_metadata = self.path_input_folder + &#39;Countries_metadata.csv&#39;

        # Path to modified input
        self.path_input_modified_folder = &quot;{}/input_modified/&quot;.format(path_folder)
        self.path_input_modified_countries = self.path_input_modified_folder + &#39;Countries.csv&#39;
        self.path_input_modified_countries_metadata = self.path_input_modified_folder + &#39;Countries_metadata.csv&#39;

        # Path on which output tables are saved
        self.path_output_folder = &quot;{}/output/&quot;.format(path_folder)
        self.path_output_countries = self.path_output_folder + &#39;Countries.csv&#39;
        self.path_output_countries_metadata = self.path_output_folder + &#39;Countries_metadata.csv&#39;

        # create dictionaries for read dtypes
        self.read_dtypes_countries = {&#39;Countries&#39;:&#39;category&#39;}
        self.read_dtypes_countries_metadata = {&#39;country_names&#39;:&#39;category&#39;}

        # create folders for output if not existent yet
        if not os.path.exists(self.path_input_modified_folder):
            os.makedirs(self.path_input_modified_folder)
        if not os.path.exists(self.path_output_folder):
            os.makedirs(self.path_output_folder) 

    def read_data_from_raw_input(self, save_countries=True, save_countries_metadata=True):

        print(&quot;Start:\tRead in countries Dataset&quot;)
        self.countries = pd.read_csv(self.path_input_countries, dtype=self.read_dtypes_countries)
        self.countries.Population = self.countries.Population.map(lambda x: x.split(&#39;:&#39;)[1])
        self.countries[&#39;Population&#39;] = self.countries[&#39;Population&#39;].astype(&#39;int64&#39;)
        print(&quot;Finish:\tRead in countries Dataset&quot;)

        print(&quot;Start:\tRead in countries_metadata Dataset&quot;)       
        self.countries_metadata = pd.read_csv(self.path_input_countries_metadata, dtype=self.read_dtypes_countries_metadata)
        self.countries_metadata.Land_Area = self.countries_metadata.Land_Area.map(lambda x: x.split(&#39;:&#39;)[1])
        self.countries_metadata[&#39;Land_Area&#39;] = self.countries_metadata[&#39;Land_Area&#39;].astype(&#39;int64&#39;)
        print(&quot;Finish:\tRead in countries_metadata Dataset&quot;)

        if save_countries:
            print(&quot;Start:\tSave countries Dataset to disc&quot;)
            self.countries.to_csv(self.path_input_modified_countries, index=False)
            print(&quot;Finish:\tSave countries Dataset to disc&quot;)
 
        if save_countries_metadata:
            print(&quot;Start:\tSave countries_metadata Dataset to disc&quot;)
            self.countries_metadata.to_csv(self.path_input_modified_countries_metadata, index=False)
            print(&quot;Finish:\tSave countries_metadata Dataset to disc&quot;)

    def read_data_from_modified_input(self):

        self.countries = pd.read_csv(self.path_input_modified_countries, dtype=self.read_dtypes_countries)
        self.countries_metadata = pd.read_csv(self.path_input_modified_countries_metadata, dtype=self.read_dtypes_countries_metadata)

    def preprocess_data(self, save_preprocess_countries=True, save_preprocess_countries_metadata=True):

        print(&quot;Start:\tPreprocessing countries Dataset&quot;)
        self.preprocess_countries()
        print(&quot;Finish:\tPreprocessing countries Dataset&quot;)

        print(&quot;Start:\tPreprocessing countries_metadata Dataset&quot;)
        self.preprocess_countries_metadata()
        print(&quot;Finish:\tPreprocessing countries_metadata Dataset&quot;)

        if save_preprocess_countries:
            print(&quot;Start:\tSave countries Dataset to disc&quot;)
            self.countries.to_csv(self.path_output_countries, index=False)
            print(&quot;Finish:\tSave countries Dataset to disc&quot;)

        if save_preprocess_countries_metadata:
            print(&quot;Start:\tSave countries_metadata Dataset to disc&quot;)
            self.countries_metadata.to_csv(self.path_output_countries_metadata, index=False)
            print(&quot;Finish:\tSave countries_metadata Dataset to disc&quot;)

        return self.countries, self.countries_metadata

    def preprocess_countries(self):
        
        self.countries[&#39;Population&#39;] = self.countries[&#39;Population&#39;]/1000
        self.countries = self.countries.rename(columns={&#39;Population&#39;:&#39;Population_per_k&#39;})

    def preprocess_countries_metadata(self):
        
        self.countries_metadata[&#39;Land_Area&#39;] = self.countries_metadata[&#39;Land_Area&#39;]/1000
        self.countries_metadata = self.countries_metadata.rename(columns={&#39;Land_Area&#39;:&#39;Land_Area_per_k&#39;})

    def read_preprocessed_tables(self):
        
        print(&quot;Start:\tRead in modified countries Dataset&quot;)
        self.countries = pd.read_csv(self.path_output_countries, dtype=self.read_dtypes_countries)
        print(&quot;Finish:\tRead in modified countries Dataset&quot;)

        print(&quot;Start:\tRead in modified countries_metadata Dataset&quot;)       
        self.countries_metadata = pd.read_csv(self.path_output_countries_metadata, dtype=self.read_dtypes_countries_metadata)
        print(&quot;Finish:\tRead in modified countries_metadata Dataset&quot;)

        return self.countries, self.countries_metadata


def main():

    datapreprocesssor = DataPreprocessor()
    datapreprocesssor.read_data_from_raw_input()
    datapreprocesssor.read_data_from_modified_input()
    datapreprocesssor.preprocess_data()
    print(&#39;ETL has been successfully completed !!&#39;)

#if __name__ == &#39;__main__&#39;:
#    main()</code></pre>
<p>We have commented out the main from the ETL pipeline here with ‘#’. Of course, this syntax <strong>must not</strong> be commented out in the .py file.</p>
<p>Unfortunately the view is not optimal. Therefore I recommend to copy the syntax into a .py file.
I prefer <a href="https://code.visualstudio.com/">“Visual Studio Code”</a> from Microsoft.
But I also put the file in my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets/ETL/Pipeline%20with%20intermediate%20storage">“GitHub Repo”</a> from where you can get it.</p>
</div>
<div id="test-etl_pipeline.py" class="section level1">
<h1>5 Test etl_pipeline.py</h1>
<p>Now we want to test our created ETL with intermediate storage.</p>
<div id="from-jupyter-notebook" class="section level2">
<h2>5.1 from jupyter notebook</h2>
<p>First I want to test the ETL from a notebook. For this we create and start a <strong>new</strong> notebook in the notebooks-folder with the name ‘Test ETL Pipeline with intermediate storage.ipynb’.</p>
<p>With this ETL we have the special feature that the (assumed) initial loading takes an extremely long time. Once this step has been taken there is no way around it.
But with intermediate storage we can reduce the runtime of the ETL (step 5.1.2 and 5.1.3) considerably.</p>
<div id="the-very-first-time" class="section level3">
<h3>5.1.1 the very first time</h3>
<pre class="r"><code>import sys

# Specifies the file path where the first .py file is located.
sys.path.insert(1, &#39;../data&#39;)
import etl_pipeline</code></pre>
<pre class="r"><code>datapreprocessor = etl_pipeline.DataPreprocessor()
datapreprocessor.read_data_from_raw_input(save_countries=True, save_countries_metadata=True)

datapreprocessor.read_data_from_modified_input()</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p10.png" /></p>
<pre class="r"><code>countries, countries_metadata = datapreprocessor.preprocess_data(save_preprocess_countries=True, save_preprocess_countries_metadata=True)</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p11.png" /></p>
<pre class="r"><code>countries, countries_metadata = datapreprocessor.read_preprocessed_tables()</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p12.png" /></p>
<pre class="r"><code>countries</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p13.png" /></p>
<pre class="r"><code>countries_metadata</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p14.png" /></p>
</div>
<div id="when-u-changed-sth.-within-preprocess_data" class="section level3">
<h3>5.1.2 when u changed sth. within preprocess_data</h3>
<pre class="r"><code>import sys

# Specifies the file path where the first .py file is located.
sys.path.insert(1, &#39;../data&#39;)
import etl_pipeline</code></pre>
<pre class="r"><code>datapreprocessor = etl_pipeline.DataPreprocessor()
datapreprocessor.read_data_from_modified_input()</code></pre>
<pre class="r"><code>countries, countries_metadata = datapreprocessor.preprocess_data(save_preprocess_countries=True, save_preprocess_countries_metadata=True)</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p15.png" /></p>
<pre class="r"><code>countries, countries_metadata = datapreprocessor.read_preprocessed_tables()</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p16.png" /></p>
<pre class="r"><code>countries</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p17.png" /></p>
<pre class="r"><code>countries_metadata</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p18.png" /></p>
</div>
<div id="when-u-continue-with-analytics" class="section level3">
<h3>5.1.3 when u continue with analytics</h3>
<pre class="r"><code>import sys

# Specifies the file path where the first .py file is located.
sys.path.insert(1, &#39;../data&#39;)
import etl_pipeline</code></pre>
<pre class="r"><code>datapreprocessor = etl_pipeline.DataPreprocessor()
countries, countries_metadata = datapreprocessor.read_preprocessed_tables()</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p19.png" /></p>
<pre class="r"><code>countries</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p20.png" /></p>
<pre class="r"><code>countries_metadata</code></pre>
<p><img src="/post/2020-11-27-etl-pipeline-with-intermediate-storage_files/p90p21.png" /></p>
</div>
</div>
<div id="from-command-line" class="section level2">
<h2>5.2 from command line</h2>
<p>Because we have defined a main in the .py file we are able to call and execute the ETL from the command line.
It does not matter which command line this is exactly. You can use Anaconda Power Shell, Git Bash, the Windwos Comman Line or anything else.</p>
<p>Type only the following commands in your command prompt:</p>
<pre class="r"><code>cd &quot;path/to/your/data/folder&quot;
python etl_pipeline.py</code></pre>
<p>As the main is currently written in the etl_pipeline.py, all steps (including the first loading step with a long runtime) are executed.
If you don’t want or need this (as described in one of the steps above from within the jupyter notebook) you would have to adapt the main accordingly and comment out some commands.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>6 Conclusion</h1>
<p>In this example, we assumed that due to the wrong formatting of the original data types, the loading time of the data records is extremely high.
In such a case the data would be cached to make it more accessible for everyday use (further development of the ETL and pre-processing steps as well as analytics).</p>
</div>
