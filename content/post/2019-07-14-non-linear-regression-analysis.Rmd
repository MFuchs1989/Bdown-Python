---
title: Non-linear regression analysis
author: Michael Fuchs
date: '2019-07-14'
slug: non-linear-regression-analysis
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---


# 1 Introduction

In my previous post ["Introduction to regression analysis and predictions"](https://michael-fuchs-python.netlify.com/2019/06/28/introduction-to-regression-analysis-and-predictions/) I showed how to create linear regression models. But what can be done if the data is not distributed linearly?


For this post the dataset *Auto-mpg* from the statistic platform ["Kaggle"](https://www.kaggle.com) was used. You can download it from my [GitHub Repository](https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets).


# 2 Loading the libraries and the data



```{r, eval=F, echo=T}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import math 
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model
```


```{r, eval=F, echo=T}
cars = pd.read_csv("path/to/file/auto-mpg.csv")
```


# 3 Data Preparation


```{r, eval=F, echo=T}
cars.head()
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p1.png)

Check the data types:

```{r, eval=F, echo=T}
cars.dtypes
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p2.png)

Convert horsepower from an object to a float:

```{r, eval=F, echo=T}
cars["horsepower"] = pd.to_numeric(cars.horsepower, errors='coerce')
cars.dtypes
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p3.png)

Check for missing values:

```{r, eval=F, echo=T}
cars.isnull().sum()
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p4.png)

Replace the missing values with the mean of column:

```{r, eval=F, echo=T}
cars_horsepower_mean = cars['horsepower'].fillna(cars['horsepower'].mean())
cars['horsepower'] = cars_horsepower_mean
cars.isnull().sum()    #Check replacement
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p5.png)


# 4 Hypothesis: a non-linear relationship between the variables mpg and horesepower



```{r, eval=F, echo=T}
cars.plot(kind='scatter', x='horsepower', y='mpg', color='red')
plt.xlabel('Horsepower')
plt.ylabel('Miles per Gallon')
plt.title('Scatter Plot: Horsepower vs. Miles per Gallon')
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p6.png)


# 5 Linear model

First of all, the two variables 'mpg' and 'horesepower' are to be investigated with a linear regression model.

```{r, eval=F, echo=T}
x = cars["horsepower"]
y = cars["mpg"]

lm = LinearRegression()
lm.fit(x[:,np.newaxis], y)
```

The linear regression model by default requires that x bean array of two dimensions. Therefore we have to use the np.newaxis-function.




```{r, eval=F, echo=T}
cars.plot(kind='scatter', x='horsepower', y='mpg', color='red')
plt.plot(x, lm.predict(x[:,np.newaxis]), color='blue')
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p7.png)

Calculation of R²

```{r, eval=F, echo=T}
lm.score(x[:,np.newaxis], y)
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p8.png)



Calculation of further parameters:


```{r, eval=F, echo=T}
y_pred = lm.predict(x[:,np.newaxis])

df = pd.DataFrame({'Actual': y, 'Predicted': y_pred})
df.head()
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p9.png)

```{r, eval=F, echo=T}
print('Mean Absolute Error:', metrics.mean_absolute_error(y, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_pred)))
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p10.png)


# 6 Non linear models

## 6.1 Quadratic Function

We now try using different methods of transformation, applied to the predictor, to improve the model results.



```{r, eval=F, echo=T}
x = cars["horsepower"] * cars["horsepower"]
y = cars["mpg"]

lm = LinearRegression()
lm.fit(x[:,np.newaxis], y)
```


Calculation of R² and further parameters:

```{r, eval=F, echo=T}
lm.score(x[:,np.newaxis], y)
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p11.png)

```{r, eval=F, echo=T}
y_pred = lm.predict(x[:,np.newaxis])
df = pd.DataFrame({'Actual': y, 'Predicted': y_pred})
df.head()
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p12.png)


```{r, eval=F, echo=T}
print('Mean Absolute Error:', metrics.mean_absolute_error(y, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_pred)))
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p13.png)


Conclusion: Poorer values than with the linear function. Let's try exponential function.


## 6.2 Exponential Function



```{r, eval=F, echo=T}
x = (cars["horsepower"]) ** 3
y = cars["mpg"]

lm = LinearRegression()
lm.fit(x[:,np.newaxis], y)
```


Calculation of R² and further parameters:

```{r, eval=F, echo=T}
lm.score(x[:,np.newaxis], y)
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p14.png)


```{r, eval=F, echo=T}
y_pred = lm.predict(x[:,np.newaxis])
df = pd.DataFrame({'Actual': y, 'Predicted': y_pred})
df.head()
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p15.png)


```{r, eval=F, echo=T}
print('Mean Absolute Error:', metrics.mean_absolute_error(y, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_pred)))
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p16.png)

Conclusion: even worse values than in the previous two functions. Since the relationship looks non-linear, let's try it with a log-transformation.


## 6.3 Logarithm Function


```{r, eval=F, echo=T}
x = np.log(cars['horsepower'])
y = cars["mpg"]


lm = LinearRegression()
lm.fit(x[:,np.newaxis], y)
```


Calculation of R² and further parameters:

```{r, eval=F, echo=T}
lm.score(x[:,np.newaxis], y)
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p17.png)


```{r, eval=F, echo=T}
y_pred = lm.predict(x[:,np.newaxis])
df = pd.DataFrame({'Actual': y, 'Predicted': y_pred})
df.head()
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p18.png)


```{r, eval=F, echo=T}
print('Mean Absolute Error:', metrics.mean_absolute_error(y, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_pred)))
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p19.png)

Conclusion: The model parameters have improved significantly with the use of the log function. Let's see if we can further increase this with the polynomial function.



## 6.4 Polynomials Function


```{r, eval=F, echo=T}
x = (cars["horsepower"])
y = cars["mpg"]

poly = PolynomialFeatures(degree=2)
x_ = poly.fit_transform(x[:,np.newaxis])

lm = linear_model.LinearRegression()
lm.fit(x_, y)
```


R²:

```{r, eval=F, echo=T}
lm.score(x_, y)
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p20.png)


Intercept and coefficients:

```{r, eval=F, echo=T}
print(lm.intercept_)
print(lm.coef_)
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p21.png)


The result can be interpreted as follows: 
mpg = 56,40 - 0,46 * horsepower + 0,001 * horsepower²


Further model parameters:

```{r, eval=F, echo=T}
y_pred = lm.predict(x_)
df = pd.DataFrame({'Actual': y, 'Predicted': y_pred})
df.head()
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p22.png)


```{r, eval=F, echo=T}
print('Mean Absolute Error:', metrics.mean_absolute_error(y, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_pred)))
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p23.png)


Now the degree of the polynomial function is increased until no improvement of the model can be recorded:


```{r, eval=F, echo=T}
x = (cars["horsepower"])
y = cars["mpg"]

poly = PolynomialFeatures(degree=6)
x_ = poly.fit_transform(x[:,np.newaxis])
lm = linear_model.LinearRegression()
lm.fit(x_, y)
```


R²:

```{r, eval=F, echo=T}
print(lm.score(x_, y))
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p24.png)

Intercept and coefficients:

```{r, eval=F, echo=T}
print(lm.intercept_)
print(lm.coef_)
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p25.png)


The result can be interpreted as follows: mpg = -150,46 + 1,07 * horsepower -2,34 * horsepower^2^ + 2,5 * horsepower^3^ - 1,42 * horsepower^4^ + 4,14 * horsepower^5^ - 4,82 * horsepower^6^


Further model parameters:

```{r, eval=F, echo=T}
y_pred = lm.predict(x_)
df = pd.DataFrame({'Actual': y, 'Predicted': y_pred})
df.head()
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p26.png)


```{r, eval=F, echo=T}
print('Mean Absolute Error:', metrics.mean_absolute_error(y, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_pred)))
```

![](/post/2019-07-14-non-linear-regression-analysis_files/p14p27.png)





# 7 Conclusion

In this post it was shown how model performance in non-linear contexts could be improved by using different transformation functions.


Finally, here is an overview of the created models and their parameters:


![](/post/2019-07-14-non-linear-regression-analysis_files/p14p28.png)

What these metrics mean and how to interpret them I have described in the following post: [Metrics for Regression Analysis](https://michael-fuchs-python.netlify.app/2019/06/30/metrics-for-regression-analysis/)


# Source
Kumar, A., & Babcock, J. (2017). Python: Advanced Predictive Analytics: Gain practical insights by exploiting data in your business to build advanced predictive modeling applications. Packt Publishing Ltd.

