---
title: Feature Scaling with Scikit-Learn
author: Michael Fuchs
date: '2019-08-31'
slug: feature-scaling-with-scikit-learn
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#loading-the-libraries">2 Loading the libraries</a></li>
<li><a href="#scaling-methods">3 Scaling methods</a>
<ul>
<li><a href="#standard-scaler">3.1 Standard Scaler</a></li>
<li><a href="#min-max-scaler">3.2 Min-Max Scaler</a></li>
<li><a href="#robust-scaler">3.3 Robust Scaler</a></li>
<li><a href="#comparison-of-the-previously-shown-scaling-methods">3.4 Comparison of the previously shown scaling methods</a></li>
</ul></li>
<li><a href="#inverse-transformation">4 Inverse Transformation</a></li>
<li><a href="#export-scaler-to-use-in-another-program">5 Export Scaler to use in another program</a></li>
<li><a href="#feature-scaling-in-practice">6 Feature Scaling in practice</a></li>
<li><a href="#normalize-or-standardize">7 Normalize or Standardize?</a></li>
<li><a href="#conclusion">8 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>Feature scaling can be an important part for many machine learning algorithms. It’s a step of data pre-processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.</p>
</div>
<div id="loading-the-libraries" class="section level1">
<h1>2 Loading the libraries</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
matplotlib.style.use(&#39;ggplot&#39;)


#For chapter 3.1
from sklearn.preprocessing import StandardScaler
#For chapter 3.2
from sklearn.preprocessing import MinMaxScaler
#For chapter 3.3
from sklearn.preprocessing import RobustScaler

#For chapter 5
import pickle as pk

#For chapter 6
from sklearn.model_selection import train_test_split


pd.set_option(&#39;float_format&#39;, &#39;{:f}&#39;.format)</code></pre>
</div>
<div id="scaling-methods" class="section level1">
<h1>3 Scaling methods</h1>
<p>In the following, three of the most important scaling methods are presented:</p>
<ul>
<li>Standard Scaler</li>
<li>Min-Max Scaler</li>
<li>Robust Scaler</li>
</ul>
<div id="standard-scaler" class="section level2">
<h2>3.1 Standard Scaler</h2>
<p>The Standard Scaler assumes the data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1. If you want to know wheather your data is normal distributet have a look at this post: <a href="https://michael-fuchs-python.netlify.com/2019/09/13/check-for-normal-distribution/">“Check for normal distribution”</a></p>
<p>The mean and standard deviation are calculated for the feature and then the feature is scaled based on:</p>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18e1.png" /></p>
<pre class="r"><code>np.random.seed(1)

df = pd.DataFrame({
    &#39;Col_1&#39;: np.random.normal(0, 2, 30000),
    &#39;Col_2&#39;: np.random.normal(5, 3, 30000),
    &#39;Col_3&#39;: np.random.normal(-5, 5, 30000)
})

df.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p1.png" /></p>
<pre class="r"><code>col_names = df.columns
features = df[col_names]

scaler = StandardScaler().fit(features.values)
features = scaler.transform(features.values)
scaled_features = pd.DataFrame(features, columns = col_names)
scaled_features.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p2.png" /></p>
<pre class="r"><code>scaled_features.describe()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p3.png" /></p>
<pre class="r"><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))

ax1.set_title(&#39;Before Scaling&#39;)
sns.kdeplot(df[&#39;Col_1&#39;], ax=ax1)
sns.kdeplot(df[&#39;Col_2&#39;], ax=ax1)
sns.kdeplot(df[&#39;Col_3&#39;], ax=ax1)
ax2.set_title(&#39;After Standard Scaler&#39;)
sns.kdeplot(scaled_features[&#39;Col_1&#39;], ax=ax2)
sns.kdeplot(scaled_features[&#39;Col_2&#39;], ax=ax2)
sns.kdeplot(scaled_features[&#39;Col_3&#39;], ax=ax2)
plt.show()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p4.png" /></p>
</div>
<div id="min-max-scaler" class="section level2">
<h2>3.2 Min-Max Scaler</h2>
<p>The Min-Max Scaler is the probably the most famous scaling algorithm, and follows the following formula for each feature:</p>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18e2.png" /></p>
<p>It essentially shrinks the range such that the range is now between 0 and 1. This scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the Min-Max Scaler works better. However, it is sensitive to outliers, so if there are outliers in the data, you might want to consider the Robust Scaler (shown below).</p>
<pre class="r"><code>np.random.seed(1)

df = pd.DataFrame({
    # positive skew
    &#39;Col_1&#39;: np.random.chisquare(8, 1000),
    # negative skew 
    &#39;Col_2&#39;: np.random.beta(8, 2, 1000) * 40,
    # no skew
    &#39;Col_3&#39;: np.random.normal(50, 3, 1000)
})

df.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p5.png" /></p>
<pre class="r"><code>col_names = df.columns
features = df[col_names]

scaler = MinMaxScaler().fit(features.values)
features = scaler.transform(features.values)
scaled_features = pd.DataFrame(features, columns = col_names)
scaled_features.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p6.png" /></p>
<pre class="r"><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))

ax1.set_title(&#39;Before Scaling&#39;)
sns.kdeplot(df[&#39;Col_1&#39;], ax=ax1)
sns.kdeplot(df[&#39;Col_2&#39;], ax=ax1)
sns.kdeplot(df[&#39;Col_3&#39;], ax=ax1)
ax2.set_title(&#39;After Min-Max Scaling&#39;)
sns.kdeplot(scaled_features[&#39;Col_1&#39;], ax=ax2)
sns.kdeplot(scaled_features[&#39;Col_2&#39;], ax=ax2)
sns.kdeplot(scaled_features[&#39;Col_3&#39;], ax=ax2)
plt.show()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p7.png" /></p>
</div>
<div id="robust-scaler" class="section level2">
<h2>3.3 Robust Scaler</h2>
<p>The RobustScaler uses a similar method to the Min-Max Scaler, but it instead uses the interquartile range, rathar than the Min-Max, so that it is robust to outliers. Therefore it follows the formula:</p>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18e3.png" /></p>
<p>Of course this means it is using the less of the data for scaling so it’s more suitable for when there are outliers in the data.</p>
<pre class="r"><code>np.random.seed(1)

df = pd.DataFrame({
    # Distribution with lower outliers
    &#39;Col_1&#39;: np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),
    # Distribution with higher outliers
    &#39;Col_2&#39;: np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),
})

df.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p8.png" /></p>
<pre class="r"><code>col_names = df.columns
features = df[col_names]

scaler = RobustScaler().fit(features.values)
features = scaler.transform(features.values)
scaled_features = pd.DataFrame(features, columns = col_names)
scaled_features.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p9.png" /></p>
<pre class="r"><code>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))

ax1.set_title(&#39;Before Scaling&#39;)
sns.kdeplot(df[&#39;Col_1&#39;], ax=ax1)
sns.kdeplot(df[&#39;Col_2&#39;], ax=ax1)

ax2.set_title(&#39;After Robust Scaling&#39;)
sns.kdeplot(scaled_features[&#39;Col_1&#39;], ax=ax2)
sns.kdeplot(scaled_features[&#39;Col_2&#39;], ax=ax2)

plt.show()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p10.png" /></p>
</div>
<div id="comparison-of-the-previously-shown-scaling-methods" class="section level2">
<h2>3.4 Comparison of the previously shown scaling methods</h2>
<pre class="r"><code>np.random.seed(32)

df = pd.DataFrame({
    # Distribution with lower outliers
    &#39;Col_1&#39;: np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),
    # Distribution with higher outliers
    &#39;Col_2&#39;: np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),
})

df.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p11.png" /></p>
<pre class="r"><code>col_names = df.columns
features = df[col_names]
scaler = StandardScaler().fit(features.values)
features = scaler.transform(features.values)
standard_scaler = pd.DataFrame(features, columns = col_names)


col_names = df.columns
features = df[col_names]
scaler = MinMaxScaler().fit(features.values)
features = scaler.transform(features.values)
min_max_scaler = pd.DataFrame(features, columns = col_names)


col_names = df.columns
features = df[col_names]
scaler = RobustScaler().fit(features.values)
features = scaler.transform(features.values)
robust_scaler = pd.DataFrame(features, columns = col_names)</code></pre>
<p>Now the plots in comparison:</p>
<pre class="r"><code>fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(15, 7))

ax1.set_title(&#39;Before Scaling&#39;)
sns.kdeplot(df[&#39;Col_1&#39;], ax=ax1)
sns.kdeplot(df[&#39;Col_2&#39;], ax=ax1)

ax2.set_title(&#39;After Standard Scaler&#39;)
sns.kdeplot(standard_scaler[&#39;Col_1&#39;], ax=ax2)
sns.kdeplot(standard_scaler[&#39;Col_2&#39;], ax=ax2)

ax3.set_title(&#39;After Min-Max Scaling&#39;)
sns.kdeplot(min_max_scaler[&#39;Col_1&#39;], ax=ax3)
sns.kdeplot(min_max_scaler[&#39;Col_2&#39;], ax=ax3)

ax4.set_title(&#39;After Robust Scaling&#39;)
sns.kdeplot(robust_scaler[&#39;Col_1&#39;], ax=ax4)
sns.kdeplot(robust_scaler[&#39;Col_2&#39;], ax=ax4)

plt.show()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p12.png" /></p>
</div>
</div>
<div id="inverse-transformation" class="section level1">
<h1>4 Inverse Transformation</h1>
<p>As already introduced in the post about <a href="https://michael-fuchs-python.netlify.app/2019/06/16/types-of-encoder/">“encoders”</a>, there is also the inverse_transform function for the scaling methods.
The functionality and the procedure is very similar.</p>
<p>Let’s take this dataframe as an example:</p>
<pre class="r"><code>df = pd.DataFrame({&#39;Col_1&#39;: [1,7,2,4,8],
                   &#39;Col_2&#39;: [7,1,5,3,4],
                   &#39;Col_3&#39;: [3,8,0,3,9],
                   &#39;Col_4&#39;: [4,7,9,1,4]})
df</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p15.png" /></p>
<p>Now we apply the standard scaler as shown before:</p>
<pre class="r"><code>col_names = df.columns
features = df[col_names]

scaler = StandardScaler().fit(features.values)
features = scaler.transform(features.values)
scaled_features = pd.DataFrame(features, columns = col_names)
scaled_features.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p16.png" /></p>
<p>The fit scaler has memorized the metrics with which he did the transformation. Therefore it is relatively easy to do the inverse transformation with the inverse_transform function.</p>
<pre class="r"><code>col_names = df.columns

re_scaled_features = scaler.inverse_transform(scaled_features)
re_scaled_df = pd.DataFrame(re_scaled_features, columns = col_names)
re_scaled_df</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p17.png" /></p>
</div>
<div id="export-scaler-to-use-in-another-program" class="section level1">
<h1>5 Export Scaler to use in another program</h1>
<p>It is extremely important to save the used scaler separately to be able to use it again for new predictions.
If you have scaled the training data with which the algorithm was created, you should also do this for predictions of new observations. In order to do this with the correct metrics, we use the originally fitted scaler.</p>
<pre class="r"><code>pk.dump(scaler, open(&#39;scaler.pkl&#39;, &#39;wb&#39;))</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p18.png" /></p>
<p>Now we reload the just saved scaler (scaler.pkl).</p>
<pre class="r"><code>scaler_reload = pk.load(open(&quot;scaler.pkl&quot;,&#39;rb&#39;))</code></pre>
<p>Now we are going to use the same dataframe as bevore to see that the reloaded scaler works correctly.</p>
<pre class="r"><code>df_new = pd.DataFrame({&#39;Col_1&#39;: [1,7,2,4,8],
                   &#39;Col_2&#39;: [7,1,5,3,4],
                   &#39;Col_3&#39;: [3,8,0,3,9],
                   &#39;Col_4&#39;: [4,7,9,1,4]})
df_new</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p19.png" /></p>
<pre class="r"><code>col_names2 = df_new.columns
features2 = df_new[col_names]


features2 = scaler_reload.transform(features2.values)
scaled_features2 = pd.DataFrame(features2, columns = col_names2)
scaled_features2.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p20.png" /></p>
</div>
<div id="feature-scaling-in-practice" class="section level1">
<h1>6 Feature Scaling in practice</h1>
<p>In practice, not the complete data record is usually scaled There are two reasons for this:</p>
<ol style="list-style-type: decimal">
<li><p>In the case of large data sets, it makes little sense with regard to the storage capacity (RAM) to reserve another scaled data set.</p></li>
<li><p>To train supervised machine learning algorithms, the data sets are usually divided into training and test parts. It is common to only scale the training part. The metrics used to scale the training part are then applied to the test part. This should prevent that the test part for evaluating an algorithm is really unseen.</p></li>
</ol>
<p>Sounds complicated, but it’s totally easy to implement.</p>
<p>First of all we create a random dataframe.</p>
<pre class="r"><code>df = pd.DataFrame(np.random.randint(0,100,size=(10000, 4)), columns=[&#39;Var1&#39;, &#39;Var2&#39;, &#39;Var3&#39;, &#39;Target_Var&#39;])
df.head()</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p13.png" /></p>
<p>Then we split it as if we wanted to train a machine learning model. If you want to know how the train-test-split function works, have a look at this post of mine: <a href="https://michael-fuchs-python.netlify.com/2019/05/16/random-sampling/">“Random sampling”</a></p>
<pre class="r"><code>x = df.drop(&#39;Target_Var&#39;, axis=1)
y = df[&#39;Target_Var&#39;]

trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
<p>Now the scaling is used (here StandardScaler):</p>
<pre class="r"><code>sc=StandardScaler()

scaler = sc.fit(trainX)
trainX_scaled = scaler.transform(trainX)
testX_scaled = scaler.transform(testX)</code></pre>
<p>We save the scaler on an object, adapt this object to the training part and transform the trainX and testX part with the metrics obtained.
Here we have the scaled features:</p>
<pre class="r"><code>trainX_scaled</code></pre>
<p><img src="/post/2019-08-31-feature-scaling-with-scikit-learn_files/p18p14.png" /></p>
<p>You can also save yourself a step in the syntax if you don’t use the fit &amp; transform function individually but together.
!! Don’t do this if you plan to train a machine learning algorithm !!
Use the previous method for this.</p>
<p>But if you want to scale an entire data set (for example for cluster analysis), then use the fit_transform function:</p>
<pre class="r"><code>sc=StandardScaler()

df_scaled = sc.fit_transform(df)</code></pre>
</div>
<div id="normalize-or-standardize" class="section level1">
<h1>7 Normalize or Standardize?</h1>
<p><a href="https://michael-fuchs-python.netlify.app/2019/08/31/feature-scaling-with-scikit-learn/#scaling-methods">Scaling methods</a> can be divided into two categories: <strong>Normalization</strong> and <strong>Standardization</strong></p>
<p>As a refresher, normalization is a scaling technique where values are shifted and rescaled so that they are in the range between 0 and 1 whereas standardization centers the values around the mean with a unit standard deviation. This means that the mean value of the attribute becomes zero and the resulting distribution has a unit standard deviation.</p>
<p>Accordingly, the <a href="https://michael-fuchs-python.netlify.app/2019/08/31/feature-scaling-with-scikit-learn/#scaling-methods">scaling methods</a> presented above can be classified as follows:</p>
<ul>
<li>Standardization: <a href="https://michael-fuchs-python.netlify.app/2019/08/31/feature-scaling-with-scikit-learn/#standard-scaler">Standard Scaler</a></li>
<li>Normalization: <a href="https://michael-fuchs-python.netlify.app/2019/08/31/feature-scaling-with-scikit-learn/#min-max-scaler">Min-Max Scaler</a> &amp; <a href="https://michael-fuchs-python.netlify.app/2019/08/31/feature-scaling-with-scikit-learn/#robust-scaler">Robust Scaler</a></li>
</ul>
<p>The question now is when do I have to use which scaling technique?
There is no single answer to this, but one can use the underlying distribution as a guide:</p>
<ul>
<li>Normalization is good to use when you know that the distribution of your data does <strong>not follow</strong> a Gaussian distribution.</li>
<li>Standardization, on the other hand, can be helpful in cases where the data <strong>follows a Gaussian distribution</strong>.</li>
</ul>
<p>My personal recommendation when developing machine learning models is to first fit the model to raw, normalized and standardized data and compare performance for best results.</p>
</div>
<div id="conclusion" class="section level1">
<h1>8 Conclusion</h1>
<p>As described in the introduction, scaling can significantly improve model performance.From this point of view, you should take these into account before training your machine learning algorithm.</p>
</div>
