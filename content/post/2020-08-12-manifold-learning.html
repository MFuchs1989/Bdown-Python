---
title: Manifold Learning
author: Michael Fuchs
date: '2020-08-12'
slug: manifold-learning
categories:
  - R
tags:
  - R Markdown
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#loading-the-libraries">2 Loading the libraries</a></li>
<li><a href="#manifold-learning-methods">3 Manifold Learning Methods</a>
<ul>
<li><a href="#locally-linear-embedding">3.1 Locally Linear Embedding</a></li>
<li><a href="#modified-locally-linear-embedding">3.2 Modified Locally Linear Embedding</a></li>
<li><a href="#isomap">3.3 Isomap</a></li>
<li><a href="#spectral-embedding">3.4 Spectral Embedding</a></li>
<li><a href="#multi-dimensional-scaling-mds">3.5 Multi-dimensional Scaling (MDS)</a></li>
<li><a href="#t-sne">3.6 t-SNE</a></li>
</ul></li>
<li><a href="#comparison-of-the-calculation-time">4 Comparison of the calculation time</a></li>
<li><a href="#conclusion">5 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p><strong>Curse of Dimensionality</strong></p>
<p>The curse of dimensionality is one of the most important problems in multivariate machine learning. It appears in many different forms, but all of them have the same net form and source: the fact that points in high-dimensional space are highly sparse.</p>
<p>I have already described two linear dimension reduction methods:</p>
<ul>
<li><a href="https://michael-fuchs-python.netlify.app/2020/07/22/principal-component-analysis-pca/">“PCA”</a> and</li>
<li><a href="https://michael-fuchs-python.netlify.app/2020/08/07/linear-discriminant-analysis-lda/">“LDA”</a></li>
</ul>
<p>But how do I treat data that are of a nonlinear nature?
Of course we have the option of using a <a href="https://michael-fuchs-python.netlify.app/2020/07/22/principal-component-analysis-pca/">“Kernel-PCA”</a> here, but that too has its limits.
For this reason we can use Manifold Learning Methods, which are to be dealt with in this article.</p>
</div>
<div id="loading-the-libraries" class="section level1">
<h1>2 Loading the libraries</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

from sklearn import datasets
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import time


from sklearn.manifold import LocallyLinearEmbedding
from sklearn.manifold import Isomap
from sklearn.manifold import SpectralEmbedding
from sklearn.manifold import MDS
from sklearn.manifold import TSNE</code></pre>
</div>
<div id="manifold-learning-methods" class="section level1">
<h1>3 Manifold Learning Methods</h1>
<p><a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">‘High-dimensional data, meaning data that requires more than two or three dimensions to represent, can be very difficult to interpret. One approach to simplification is to assume that the available data of interest lie on an embedded non-linear manifold within the higher-dimensional space. If the manifold is of low enough dimension, the data can be visualised in the low-dimensional space.’</a></p>
<p>In the following I will introduce some methods of how this is possible.
For this example I will use the following data:</p>
<pre class="r"><code>n_points = 1000
X, color = datasets.make_s_curve(n_points, random_state=0)

# Create figure
fig = plt.figure(figsize=(8, 8))

# Add 3d scatter plot
ax = fig.add_subplot(projection=&#39;3d&#39;)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax.view_init(10, -70)
plt.title(&quot;Manifold Learning with an S-Curve&quot;, fontsize=15)</code></pre>
<p><img src="/post/2020-08-12-manifold-learning_files/p59p1.png" /></p>
<div id="locally-linear-embedding" class="section level2">
<h2>3.1 Locally Linear Embedding</h2>
<p>Locally Linear Embedding (LLE) uses many local linear decompositions to preserve globally non-linear structures.</p>
<pre class="r"><code>embedding = LocallyLinearEmbedding(n_neighbors=15, n_components=2)

X_transformed = embedding.fit_transform(X)</code></pre>
<pre class="r"><code>plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c = color, cmap=plt.cm.Spectral)

plt.title(&#39;Projected data with LLE&#39;, fontsize=18)</code></pre>
<p><img src="/post/2020-08-12-manifold-learning_files/p59p2.png" /></p>
</div>
<div id="modified-locally-linear-embedding" class="section level2">
<h2>3.2 Modified Locally Linear Embedding</h2>
<p>Modified LLE applies a regularization parameter to LLE.</p>
<pre class="r"><code>embedding = LocallyLinearEmbedding(n_neighbors=15, n_components=2, method=&#39;modified&#39;)

X_transformed = embedding.fit_transform(X)</code></pre>
<pre class="r"><code>plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c = color, cmap=plt.cm.Spectral)

plt.title(&#39;Projected data with modified LLE&#39;, fontsize=18)</code></pre>
<p><img src="/post/2020-08-12-manifold-learning_files/p59p3.png" /></p>
</div>
<div id="isomap" class="section level2">
<h2>3.3 Isomap</h2>
<p>Isomap seeks a lower dimensional embedding that maintains geometric distances between each instance.</p>
<pre class="r"><code>embedding = Isomap(n_neighbors=15, n_components=2)

X_transformed = embedding.fit_transform(X)</code></pre>
<pre class="r"><code>plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c = color, cmap=plt.cm.Spectral)

plt.title(&#39;Projected data with Isomap&#39;, fontsize=18)</code></pre>
<p><img src="/post/2020-08-12-manifold-learning_files/p59p4.png" /></p>
</div>
<div id="spectral-embedding" class="section level2">
<h2>3.4 Spectral Embedding</h2>
<p>Spectral Embedding a discrete approximation of the low dimensional manifold using a graph representation.</p>
<pre class="r"><code>embedding = SpectralEmbedding(n_neighbors=15, n_components=2)

X_transformed = embedding.fit_transform(X)</code></pre>
<pre class="r"><code>plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c = color, cmap=plt.cm.Spectral)

plt.title(&#39;Projected data with Spectral Embedding&#39;, fontsize=18)</code></pre>
<p><img src="/post/2020-08-12-manifold-learning_files/p59p5.png" /></p>
</div>
<div id="multi-dimensional-scaling-mds" class="section level2">
<h2>3.5 Multi-dimensional Scaling (MDS)</h2>
<p>Multi-dimensional Scaling (MDS) uses similarity to plot points that are near to each other close in the embedding.</p>
<pre class="r"><code>embedding = MDS(n_components=2)

X_transformed = embedding.fit_transform(X)</code></pre>
<pre class="r"><code>plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c = color, cmap=plt.cm.Spectral)

plt.title(&#39;Projected data with MDS&#39;, fontsize=18)</code></pre>
<p><img src="/post/2020-08-12-manifold-learning_files/p59p6.png" /></p>
</div>
<div id="t-sne" class="section level2">
<h2>3.6 t-SNE</h2>
<p>t-SNE converts the similarity of points into probabilities then uses those probabilities to create an embedding.</p>
<pre class="r"><code>embedding = TSNE(n_components=2)

X_transformed = embedding.fit_transform(X)</code></pre>
<pre class="r"><code>plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c = color, cmap=plt.cm.Spectral)

plt.title(&#39;Projected data with t-SNE&#39;, fontsize=18)</code></pre>
<p><img src="/post/2020-08-12-manifold-learning_files/p59p7.png" /></p>
</div>
</div>
<div id="comparison-of-the-calculation-time" class="section level1">
<h1>4 Comparison of the calculation time</h1>
<p>Please find below a comparison of the calculation time to the models just used.</p>
<p><img src="/post/2020-08-12-manifold-learning_files/p59p8.png" /></p>
</div>
<div id="conclusion" class="section level1">
<h1>5 Conclusion</h1>
<p>In this post, I showed how one can graphically represent high-dimensional data using manifold learning algorithms so that valuable insights can be extracted.</p>
</div>
