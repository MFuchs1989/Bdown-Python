---
title: NLP - Text Pre-Processing - All in One
author: Michael Fuchs
date: '2021-06-23'
slug: nlp-text-pre-processing-all-in-one
categories: []
tags: []
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction">1 Introduction</a></li>
<li><a href="#import-the-libraries-and-the-data">2 Import the Libraries and the Data</a></li>
<li><a href="#text-pre-processing">3 Text Pre-Processing</a>
<ul>
<li><a href="#text-cleaning">3.1 Text Cleaning</a></li>
<li><a href="#tokenization">3.2 Tokenization</a></li>
<li><a href="#stop-words">3.3 Stop Words</a></li>
<li><a href="#normalization">3.4 Normalization</a></li>
<li><a href="#removing-single-characters">3.5 Removing Single Characters</a></li>
<li><a href="#text-exploration">3.6 Text Exploration</a>
<ul>
<li><a href="#most-common-words">3.6.1 Most common Words</a>
<ul>
<li><a href="#for-the-whole-df">3.6.1.1 for the whole DF</a></li>
<li><a href="#for-parts-of-the-df">3.6.1.2 for parts of the DF</a></li>
</ul></li>
<li><a href="#least-common-words">3.6.2 Least common Words</a></li>
</ul></li>
<li><a href="#removing-specific-words">3.7 Removing specific Words</a></li>
<li><a href="#removing-rare-words">3.8 Removing Rare words</a></li>
<li><a href="#final-results">3.9 Final Results</a></li>
</ul></li>
<li><a href="#conclusion">4 Conclusion</a></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>I have focused heavily on the topic of Text Pre-Processing in my past publications. At this point, I would like to summarize all the important steps in one post.
Here I will not go into the theoretical background. For that, please read my earlier posts, where I explained in detail what I did and why.</p>
<p>For this publication the dataset <em>Amazon Unlocked Mobile</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used. You can download it from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets/NLP/Text%20Pre-Processing%20-%20All%20in%20One">“GitHub Repository”</a>.</p>
</div>
<div id="import-the-libraries-and-the-data" class="section level1">
<h1>2 Import the Libraries and the Data</h1>
<p>As you have seen in my past posts, I have written some useful functions for text pre-processing.
Since my notebook would look pretty cluttered if I listed all the functions here, I created a <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/blob/main/datasets/NLP/Text%20Pre-Processing%20-%20All%20in%20One/text_pre_processing.py">separate .py file</a> that contains all the featured functions. Feel free to download it from my <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/tree/main/datasets/NLP/Text%20Pre-Processing%20-%20All%20in%20One">“GitHub Repository”</a>.</p>
<p>Below I will call some of these functions from the <a href="https://github.com/MFuchs1989/Datasets-and-Miscellaneous/blob/main/datasets/NLP/Text%20Pre-Processing%20-%20All%20in%20One/text_pre_processing.py">.py file</a> and use them in this post.</p>
<pre class="r"><code>import pandas as pd

import warnings
warnings.filterwarnings(&quot;ignore&quot;)

import matplotlib.pyplot as plt

from text_pre_processing import remove_html_tags_func
from text_pre_processing import remove_url_func
from text_pre_processing import remove_accented_chars_func
from text_pre_processing import remove_punctuation_func
from text_pre_processing import remove_irr_char_func
from text_pre_processing import remove_extra_whitespaces_func
from text_pre_processing import word_count_func
from text_pre_processing import word_tokenize
from text_pre_processing import remove_english_stopwords_func
from text_pre_processing import norm_lemm_v_a_func
from text_pre_processing import remove_single_char_func
from text_pre_processing import most_common_word_func
from text_pre_processing import least_common_word_func
from text_pre_processing import single_word_remove_func
from text_pre_processing import multiple_word_remove_func
from text_pre_processing import most_rare_word_func


pd.set_option(&#39;display.max_colwidth&#39;, 30)</code></pre>
<pre class="r"><code>df = pd.read_csv(&#39;Amazon_Unlocked_Mobile_small.csv&#39;)
df = df[[&#39;Rating&#39;, &#39;Reviews&#39;]]
df[&#39;Reviews&#39;] = df[&#39;Reviews&#39;].astype(str)
df.head()</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p1.png" /></p>
</div>
<div id="text-pre-processing" class="section level1">
<h1>3 Text Pre-Processing</h1>
<div id="text-cleaning" class="section level2">
<h2>3.1 Text Cleaning</h2>
<p>First, we perform some text cleaning steps. These are:</p>
<ul>
<li>Conversion to Lower Case</li>
<li>Removing HTML-Tags</li>
<li>Removing URLs</li>
<li>Removing Accented Characters</li>
<li>Removing Punctuation</li>
<li>Removing irrelevant Characters (Numbers and Punctuation)</li>
<li>Removing extra Whitespaces</li>
</ul>
<pre class="r"><code>df[&#39;Clean_Reviews&#39;] = df[&#39;Reviews&#39;].str.lower()
df[&#39;Clean_Reviews&#39;] = df[&#39;Clean_Reviews&#39;].apply(remove_html_tags_func)
df[&#39;Clean_Reviews&#39;] = df[&#39;Clean_Reviews&#39;].apply(remove_url_func)
df[&#39;Clean_Reviews&#39;] = df[&#39;Clean_Reviews&#39;].apply(remove_accented_chars_func)
df[&#39;Clean_Reviews&#39;] = df[&#39;Clean_Reviews&#39;].apply(remove_punctuation_func)
df[&#39;Clean_Reviews&#39;] = df[&#39;Clean_Reviews&#39;].apply(remove_irr_char_func)
df[&#39;Clean_Reviews&#39;] = df[&#39;Clean_Reviews&#39;].apply(remove_extra_whitespaces_func)

df</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p2.png" /></p>
</div>
<div id="tokenization" class="section level2">
<h2>3.2 Tokenization</h2>
<pre class="r"><code>df[&#39;Reviews_Tokenized&#39;] = df[&#39;Clean_Reviews&#39;].apply(word_tokenize)

df</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p3.png" /></p>
</div>
<div id="stop-words" class="section level2">
<h2>3.3 Stop Words</h2>
<pre class="r"><code>df[&#39;Reviews_wo_Stop_Words&#39;] = df[&#39;Reviews_Tokenized&#39;].apply(remove_english_stopwords_func)

df</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p4.png" /></p>
</div>
<div id="normalization" class="section level2">
<h2>3.4 Normalization</h2>
<pre class="r"><code>df[&#39;Reviews_lemmatized&#39;] = df[&#39;Reviews_wo_Stop_Words&#39;].apply(norm_lemm_v_a_func)

df.head(3).T</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p5.png" /></p>
</div>
<div id="removing-single-characters" class="section level2">
<h2>3.5 Removing Single Characters</h2>
<pre class="r"><code>df[&#39;Reviews_cleaned_wo_single_char&#39;] = df[&#39;Reviews_lemmatized&#39;].apply(remove_single_char_func)

df.head(3).T</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p6.png" /></p>
</div>
<div id="text-exploration" class="section level2">
<h2>3.6 Text Exploration</h2>
<div id="most-common-words" class="section level3">
<h3>3.6.1 Most common Words</h3>
<div id="for-the-whole-df" class="section level4">
<h4>3.6.1.1 for the whole DF</h4>
<pre class="r"><code>text_corpus = df[&#39;Reviews_cleaned_wo_single_char&#39;].str.cat(sep=&#39; &#39;)

df_most_common_words_text_corpus = most_common_word_func(text_corpus)

df_most_common_words_text_corpus.head(10)</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p7.png" /></p>
<pre class="r"><code>plt.figure(figsize=(11,7))
plt.bar(df_most_common_words_text_corpus[&#39;Word&#39;], 
        df_most_common_words_text_corpus[&#39;Frequency&#39;])

plt.xticks(rotation = 45)

plt.xlabel(&#39;Most common Words&#39;)
plt.ylabel(&quot;Frequency&quot;)
plt.title(&quot;Frequency distribution of the 25 most common words&quot;)

plt.show()</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p8.png" /></p>
</div>
<div id="for-parts-of-the-df" class="section level4">
<h4>3.6.1.2 for parts of the DF</h4>
<pre class="r"><code>def label_func(rating):
    if rating &lt;= 2:
        return &#39;negative&#39;
    if rating &gt;= 4:
        return &#39;positive&#39;
    else:
        return &#39;neutral&#39;

df[&#39;Label&#39;] = df[&#39;Rating&#39;].apply(label_func)</code></pre>
<pre class="r"><code>cols = list(df.columns)
cols = [cols[-1]] + cols[:-1]
df = df[cols]</code></pre>
<pre class="r"><code>positive_review = df[(df[&quot;Label&quot;] == &#39;positive&#39;)][&#39;Reviews_cleaned_wo_single_char&#39;]
neutral_review = df[(df[&quot;Label&quot;] == &#39;neutral&#39;)][&#39;Reviews_cleaned_wo_single_char&#39;]
negative_review = df[(df[&quot;Label&quot;] == &#39;negative&#39;)][&#39;Reviews_cleaned_wo_single_char&#39;]</code></pre>
<pre class="r"><code>text_corpus_positive_review = positive_review.str.cat(sep=&#39; &#39;)
text_corpus_neutral_review = neutral_review.str.cat(sep=&#39; &#39;)
text_corpus_negative_review = negative_review.str.cat(sep=&#39; &#39;)</code></pre>
<pre class="r"><code>df_most_common_words_text_corpus_positive_review = most_common_word_func(text_corpus_positive_review)
df_most_common_words_text_corpus_neutral_review = most_common_word_func(text_corpus_neutral_review)
df_most_common_words_text_corpus_negative_review = most_common_word_func(text_corpus_negative_review)</code></pre>
<pre class="r"><code>splited_data = [df_most_common_words_text_corpus_positive_review,
                df_most_common_words_text_corpus_neutral_review,
                df_most_common_words_text_corpus_negative_review]

color_list = [&#39;green&#39;, &#39;red&#39;, &#39;cyan&#39;]
title_list = [&#39;Positive Review&#39;, &#39;Neutral Review&#39;, &#39;Negative Review&#39;]


for item in range(3):
    plt.figure(figsize=(11,7))
    plt.bar(splited_data[item][&#39;Word&#39;], 
            splited_data[item][&#39;Frequency&#39;],
            color=color_list[item])
    plt.xticks(rotation = 45)
    plt.xlabel(&#39;Most common Words&#39;)
    plt.ylabel(&quot;Frequency&quot;)
    plt.title(&quot;Frequency distribution of the 25 most common words&quot;)
    plt.suptitle(title_list[item], fontsize=15)
    plt.show()</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p9.png" /></p>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p10.png" /></p>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p11.png" /></p>
</div>
</div>
<div id="least-common-words" class="section level3">
<h3>3.6.2 Least common Words</h3>
<pre class="r"><code>df_least_common_words_text_corpus = least_common_word_func(text_corpus, n_words=2500)

df_least_common_words_text_corpus</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p12.png" /></p>
<pre class="r"><code>df_least_common_words_text_corpus[(df_least_common_words_text_corpus[&quot;Frequency&quot;] &gt; 1)].shape[0] </code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p13.png" /></p>
</div>
</div>
<div id="removing-specific-words" class="section level2">
<h2>3.7 Removing specific Words</h2>
<p>In the analysis from the previous chapter, we saw that the word ‘phone’ occurs very frequently both in the total and in the dataset split by rating. For this reason, this word is excluded in a newly generated column.</p>
<pre class="r"><code>df[&quot;Reviews_cleaned_wo_specific_word&quot;] = df.apply(lambda x: single_word_remove_func(x[&quot;Reviews_cleaned_wo_single_char&quot;], 
                                                            &quot;phone&quot;), axis = 1)

df.head(3).T</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p14.png" /></p>
</div>
<div id="removing-rare-words" class="section level2">
<h2>3.8 Removing Rare words</h2>
<p>Furthermore, the analysis from the previous chapter revealed that at least 2,500 words occur only once in the entire text corpus of the ‘Reviews_cleaned_wo_single_char’ column. In order to be able to check later whether these words are profitable or not, I generate a new column without these 2,500 words.</p>
<pre class="r"><code>most_rare_words_list_DataFrame = most_rare_word_func(text_corpus, n_words=2500)

df[&quot;Reviews_cleaned_wo_rare_words&quot;] = df.apply(lambda x: multiple_word_remove_func(x[&quot;Reviews_cleaned_wo_specific_word&quot;], 
                                                         most_rare_words_list_DataFrame), axis = 1)</code></pre>
</div>
<div id="final-results" class="section level2">
<h2>3.9 Final Results</h2>
<p>The final data set looks like this:</p>
<pre class="r"><code>df.head(3).T</code></pre>
<p><img src="/post/2021-06-23-nlp-text-pre-processing-all-in-one_files/p129p15.png" /></p>
<p>Here we generated three columns that presumably have different information densities.</p>
<p>On the one hand we have the column ‘Reviews_cleaned_wo_single_char’ where we have only applied all necessary pre-processing steps.</p>
<p>Then we have another column (‘Reviews_cleaned_wo_specific_word’) where we excluded the word ‘phone’.</p>
<p>Finally, we generated the column ‘Reviews_cleaned_wo_rare_words’, where we excluded the word ‘phone’ and 2500 other words that occurred only once in the entire text corpus.</p>
<p>Now we need to vectorize the individual text columns to make them usable for machine learning algorithms.
The validation of the model performance of the individual algorithms will then reveal which text column is most profitable for the analysis.
It may be necessary to further adjust the columns if, for example, it turns out that the removal of rare words was very profitable.</p>
<p>I will describe this in detail in further posts.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>4 Conclusion</h1>
<p>In this post I have presented all possible text pre-processing steps, most of which I had also described in more detail in my post series on text pre-processing.</p>
</div>
