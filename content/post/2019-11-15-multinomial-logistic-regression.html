---
title: Multinomial logistic regression
author: Michael Fuchs
date: '2019-11-15'
slug: multinomial-logistic-regression
categories:
  - R
tags:
  - R Markdown
---



<div id="table-of-content" class="section level1">
<h1>Table of Content</h1>
<ul>
<li>1 Introduction</li>
<li>2 Loading the libraries and the data</li>
<li>3 Multinomial logistic regression with scikit-learn</li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27s1.png" />

</div>
<p>In my previous post, I explained how the logistic regression works. Short wrap up: we used a logistic regression to create a binary classification model. With a Multinomial Logistic Regression (also known as Softmax Regression) it is possible to predict multipe classes. And this is the content this publication is about.</p>
<p>For this post the dataset <em>Iris</em> from the statistic platform <a href="https://www.kaggle.com/c/santander-customer-satisfaction/data">“Kaggle”</a> was used. A copy of the record is available at <a href="https://drive.google.com/open?id=13KXvBAEKx_IYRX3iCYPnLtO9S9-a6JTS" class="uri">https://drive.google.com/open?id=13KXvBAEKx_IYRX3iCYPnLtO9S9-a6JTS</a>.</p>
</div>
<div id="loading-the-libraries-and-the-data" class="section level1">
<h1>2 Loading the libraries and the data</h1>
<pre class="r"><code>import numpy as np
import pandas as pd

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import preprocessing


#for chapter XY
import statsmodels.api as sm

#for readable figures
pd.set_option(&#39;float_format&#39;, &#39;{:f}&#39;.format)</code></pre>
<pre class="r"><code>iris = pd.read_csv(&quot;path/to/file/Iris_Data.csv&quot;)</code></pre>
<pre class="r"><code>iris.head()</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p1.png" />

</div>
</div>
<div id="multinomial-logistic-regression-with-scikit-learn" class="section level1">
<h1>3 Multinomial logistic regression with scikit-learn</h1>
<p>First of all we assign the predictors and the criterion to each object and split the datensatz into a training and a test part.</p>
<pre class="r"><code>x = iris.drop(&#39;species&#39;, axis=1)
y = iris[&#39;species&#39;]
trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
<p>Here comes the Multinomial Logistic Regression:</p>
<pre class="r"><code>log_reg = LogisticRegression(solver=&#39;newton-cg&#39;, multi_class=&#39;multinomial&#39;)
log_reg.fit(trainX, trainY)
y_pred = log_reg.predict(testX)</code></pre>
<p>Let’s print the accuracy and error rate:</p>
<pre class="r"><code>print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred)))
print(&#39;Error rate: {:.2f}&#39;.format(1 - accuracy_score(testY, y_pred)))</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p2.png" />

</div>
<p>We also have the opportunity to get the probabilities of the predicted classes:</p>
<pre class="r"><code>probability = log_reg.predict_proba(testX)
probability</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p3.png" />

</div>
<p>Each column here represents a class. The class with the highest probability is the output of the predicted class. Here we can see that the length of the probability data is the same as the length of the test data.</p>
<pre class="r"><code>print(probability.shape[0])
print(testX.shape[0])</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p4.png" />

</div>
<p>Let’s bring the above shown output into shape and a readable format.</p>
<pre class="r"><code>df = pd.DataFrame(log_reg.predict_proba(testX), columns=log_reg.classes_)
df.head()</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p5.png" />

</div>
<p>Tip: with the .classes_ function we get the order of the classes that Python gave.</p>
<p>The sum of the probabilities must always be 1. We can see here:</p>
<pre class="r"><code>df[&#39;sum&#39;] = df.sum(axis=1)
df.head()</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p6.png" />

</div>
<p>Now let’s add the predicted classes…</p>
<pre class="r"><code>df[&#39;predicted_class&#39;] = y_pred
df.head()</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p7.png" />

</div>
<p>.. and the actual classes:</p>
<pre class="r"><code>df[&#39;actual_class&#39;] = testY.to_frame().reset_index().drop(columns=&#39;index&#39;)
df.head()</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p8.png" />

</div>
<p>Now we can do a plausibility check whether the classes were predicted correctly. Unfortunately, the comparison of two object columns works very badly in my test attempts. Therefore I built a small word around in which I convert the predicted_classes and actual_classes using the label encoder from scikit-learn and then continue to work with numerical values.</p>
<pre class="r"><code>le = preprocessing.LabelEncoder()

df[&#39;label_pred&#39;] = le.fit_transform(df[&#39;predicted_class&#39;])
df[&#39;label_actual&#39;] = le.fit_transform(df[&#39;actual_class&#39;])
df.head()</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p9.png" />

</div>
<p>Here we see that the two variables (predicted_class &amp; actual_class) were coded the same and can therefore be continued properly.</p>
<pre class="r"><code>targets = df[&#39;predicted_class&#39;]   
integerEncoded = le.fit_transform(targets)
integerMapping=dict(zip(targets,integerEncoded))
integerMapping</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p10.png" />

</div>
<pre class="r"><code>targets = df[&#39;actual_class&#39;]   
integerEncoded = le.fit_transform(targets)
integerMapping=dict(zip(targets,integerEncoded))
integerMapping</code></pre>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p11.png" />

</div>
<div class="figure">
<img src="/post/2019-11-15-multinomial-logistic-regression_files/p27p.png" />

</div>
</div>
