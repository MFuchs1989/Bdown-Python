<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R Markdown on Michael Fuchs Python</title>
    <link>/tags/r-markdown/</link>
    <description>Recent content in R Markdown on Michael Fuchs Python</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/r-markdown/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Introduction to Naive Bayes Classifier</title>
      <link>/2019/12/15/introduction-to-naive-bayes-classifier/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/15/introduction-to-naive-bayes-classifier/</guid>
      <description>Table of Content1 Introduction2 Background information on Naive Bayes Classifier3 Loading the libraries and the data4 Data pre-processing5 Naive Bayes in scikit-learn5.1 Binary Classification5.1.1 Gaussian Naive Bayes5.1.2 Bernoulli Naive Bayes5.2 Multiple Classification5.2.1 Gaussian Naive Bayes5.2.2 Multinomial Naive Bayes6 Conclusion1 IntroductionNow in the series of multiple classifiers we come to a very easy to use probability model: The Naive Bayes Classifier.</description>
    </item>
    
    <item>
      <title>Introduction to Decision Trees</title>
      <link>/2019/11/30/introduction-to-decision-trees/</link>
      <pubDate>Sat, 30 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/30/introduction-to-decision-trees/</guid>
      <description>Table of Content1 Introduction2 Background information on decision trees3 Loading the libraries and the data4 Decision Trees with scikit-learn5 Visualization of the decision tree6 Model evaluation7 Model improvement7.1 Hyperparameter optimization via Grid Search8 Conclusion1 IntroductionAfter “Multinomial logistic regression” we come to a further multiple class classifier: Decision Trees.
For this post the dataset Iris from the statistic platform “Kaggle” was used.</description>
    </item>
    
    <item>
      <title>Multinomial logistic regression</title>
      <link>/2019/11/15/multinomial-logistic-regression/</link>
      <pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/15/multinomial-logistic-regression/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Multinomial logistic regression with scikit-learn3.1 Fit the model3.2 Model validation3.3 Calculated probabilities4 Multinomial Logit with the statsmodel library5 Conclusion1 IntroductionIn my previous posts, I explained how “Logistic Regression” and “Support Vector Machines” works. Short wrap up: we used a logistic regression or a support vector machine to create a binary classification model.</description>
    </item>
    
    <item>
      <title>OvO and OvR Classifier</title>
      <link>/2019/11/13/ovo-and-ovr-classifier/</link>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/13/ovo-and-ovr-classifier/</guid>
      <description>Table of Content1 Introduction2 Background information on OvO and OvR3 Loading the libraries and the data4 OvO/OvR with Logistic Regression4.1 One-vs-Rest4.2 One-vs-One4.3 Grid Search5 OvO/OvR with SVM5.1 One-vs-Rest5.2 One-vs-One5.3 Grid Search6 Conclusion1 IntroductionWe already know from my previous posts how to train a binary classifier using “Logistic Regression” or “Support Vector Machines”. We have learned that these machine learning algorithms are strictly binary classifiers.</description>
    </item>
    
    <item>
      <title>Introduction to SGD Classifier</title>
      <link>/2019/11/11/introduction-to-sgd-classifier/</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/11/introduction-to-sgd-classifier/</guid>
      <description>Table of Content1 Introduction2 Background information on SGD Classifiers3 Loading the libraries and the data4 Data pre-processing5 SGDClassifier5.1 Logistic Regression with SGD training5.2 Linear SVM with SGD training6 Model improvement6.1 Performance comparison of the different linear models6.2 GridSearch7 Conclusion1 IntroductionThe name Stochastic Gradient Descent - Classifier (SGD-Classifier) might mislead some user to think that SGD is a classifier.</description>
    </item>
    
    <item>
      <title>Introduction to Support Vector Machines</title>
      <link>/2019/11/08/introduction-to-support-vector-machines/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/08/introduction-to-support-vector-machines/</guid>
      <description>Table of Content1 Introduction2 Background information on Support Vector Machines3 Loading the libraries and the data4 Data pre-processing5 SVM with scikit-learn5.1 Model Fitting5.2 Model evaluation6 Kernel SVM with Scikit-Learn6.1 Polynomial Kernel6.2 Gaussian Kernel6.3 Sigmoid Kernel7 Hyperparameter optimization via Grid Search8 Conclusion1 IntroductionIn addition to “Logistic Regression”, there is another very well-known algorithm for binary classifications: the Support Vector Machine (SVM).</description>
    </item>
    
    <item>
      <title>Introduction to Logistic Regression</title>
      <link>/2019/10/31/introduction-to-logistic-regression/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/31/introduction-to-logistic-regression/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Descriptive statistics3.1 Mean values of the features3.2 Description of the target variable3.3 Description of the predictor variables4 Data pre-processing4.1 Conversion of the target variable4.2 Creation of dummy variables4.3 Feature Selection5 Logistic Regression with the statsmodel library6 Logistic Regression with scikit-learn6.1 Over-sampling using SMOTE6.2 Model Fitting6.</description>
    </item>
    
    <item>
      <title>Roadmap for Regression Analysis</title>
      <link>/2019/10/14/roadmap-for-regression-analysis/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/14/roadmap-for-regression-analysis/</guid>
      <description>Table of Content1 Introduction2 Roadmap for Regression Analysis3 Conclusion1 IntroductionIn my most recent publications, I have dealt extensively with individual topics in the field of regression analysis. This post should serve as a summary of the topics covered.
2 Roadmap for Regression AnalysisHere are the links to the individual topics.
Data pre-processing:
“Handling Missing Values”“Dealing with outliers”“Feature Encoding”“Feature Scaling”Feature Selection:</description>
    </item>
    
    <item>
      <title>Embedded methods</title>
      <link>/2019/10/08/embedded-methods/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/08/embedded-methods/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Embedded methods3.1 Ridge Regression3.2 Lasso Regression3.3 Elastic Net4 Conclusion1 IntroductionImage Source: “Analytics Vidhya”
Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.</description>
    </item>
    
    <item>
      <title>Wrapper methods</title>
      <link>/2019/09/27/wrapper-methods/</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/27/wrapper-methods/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Wrap up: Filter methods4 Wrapper methods4.1 Data Preparation4.1.1 Check for missing values4.1.2 Removing highly correlated features4.1.3 SelectKBest4.2 Syntax for wrapper methods4.2.1 Forward Feature Selection4.2.2 Backward Elimination4.2.3 Recursive Feature Elimination (RFE)5 Conclusion1 IntroductionFeature selection is pretty important in machine learning primarily because it serves as a fundamental technique to direct the use of variables to what’s most efficient and effective for a given machine learning system.</description>
    </item>
    
    <item>
      <title>Check for normal distribution</title>
      <link>/2019/09/13/check-for-normal-distribution/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/13/check-for-normal-distribution/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Visual Normality Checks3.1 Quantile-Quantile Plot3.2 Histogram Plot4 Statistical Normality Tests4.1 Shapiro-Wilk Test4.2 D’Agostino’s K2 Test5 Conclusion1 IntroductionIn my previous “post” the question came up of how to check its data on normal distribution. There are several possibilities for this.
2 Loading the librariesimport pandas as pdimport numpy as npimport pylab import scipy.</description>
    </item>
    
    <item>
      <title>Feature Scaling with Scikit-Learn</title>
      <link>/2019/08/31/feature-scaling-with-scikit-learn/</link>
      <pubDate>Sat, 31 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/31/feature-scaling-with-scikit-learn/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Scaling methods3.1 Standard Scaler3.2 Min-Max Scaler3.3 Robust Scaler3.4 Comparison of the previously shown scaling methods4 Feature Scaling in practice5 Conclusion1 IntroductionFeature scaling can be an important part for many machine learning algorithms. It’s a step of data pre-processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range.</description>
    </item>
    
    <item>
      <title>Dealing with outliers</title>
      <link>/2019/08/20/dealing-with-outliers/</link>
      <pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/20/dealing-with-outliers/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Boxplots - Method4 Z-score method5 IQR method5.1 Detect outlier for column ‘age’5.2 Detect outlier for column ‘salary’5.3 Remove outlier from dataframe6 Conclusion1 IntroductionNext to “higly correlated” and “constant” features outlier detection is also a central element of data pre-processing.
In statistics, outliers are data points that do not belong to any particular population.</description>
    </item>
    
    <item>
      <title>Dealing with constant and duplicate features</title>
      <link>/2019/08/09/dealing-with-constant-and-duplicate-features/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/09/dealing-with-constant-and-duplicate-features/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Removing Constant features4 Removing Quasi-Constant features5 Removing Duplicate Features6 Conclusion1 IntroductionIn addition to “removing highly correlated features” as one of the data pre processing steps we also have to take care of constant and duplicate features. Constant features have a variance close to zero and duplicate features are too similar to other variables in the record.</description>
    </item>
    
    <item>
      <title>Dealing with highly correlated features</title>
      <link>/2019/07/28/dealing-with-highly-correlated-features/</link>
      <pubDate>Sun, 28 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/28/dealing-with-highly-correlated-features/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Preparation4 Correlations with the output variable5 Identification of highly correlated features6 Removing highly correlated features6.1 Selecting numerical variables6.2 Train / Test Split7 Conclusion1 IntroductionOne of the points to remember about data pre-processing for regression analysis is multicollinearity. This post is about finding highly correlated predictors within a dataframe.</description>
    </item>
    
    <item>
      <title>Non-linear regression analysis</title>
      <link>/2019/07/14/non-linear-regression-analysis/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/14/non-linear-regression-analysis/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Data Preparation4 Hypothesis: a non-linear relationship5 Linear model6 Non linear models6.1 Quadratic Function6.2 Exponential Function6.3 Logarithm Function6.4 Polynomials Function7 Conclusion1 IntroductionIn my previous post “Introduction to regression analysis and predictions” I showed how to create linear regression models. But what can be done if the data is not distributed linearly?</description>
    </item>
    
    <item>
      <title>statsmodel.formula.api vs statsmodel.api</title>
      <link>/2019/07/02/statsmodel-formula-api-vs-statsmodel-api/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/02/statsmodel-formula-api-vs-statsmodel-api/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 The statsmodel.formula.api4 The statsmodel.api5 Conclusion1 IntroductionImage Source: “Statsmodels.org”
In my last post “Introduction to regression analysis and predictions” I used the statsmodel library to identify significant features influencing the property price. In this publication I would like to show the difference of the statsmodel.formula.api (smf) and the statsmodel.api (sm).</description>
    </item>
    
    <item>
      <title>Introduction to regression analysis and predictions</title>
      <link>/2019/06/28/introduction-to-regression-analysis-and-predictions/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/28/introduction-to-regression-analysis-and-predictions/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Implementing linear regression with the statsmodel library3.1 Simple linear Regression3.2 Multiple Regression3.3 Model validation4 Linear Regression with scikit-learn5 Conclusion1 IntroductionRegression analyzes are very common and should therefore be mastered by every data scientist.
For this post the dataset House Sales in King County, USA from the statistic platform “Kaggle” was used.</description>
    </item>
    
    <item>
      <title>Types of Encoder</title>
      <link>/2019/06/16/types-of-encoder/</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/16/types-of-encoder/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Encoder for predictor variables3.1 One Hot Encoder3.1.1 via scikit-learn3.1.2 via pandas3.2 Ordinal Encoder4 Encoder for target variables4.1 Label Binarizer4.2 Label Encoding5 Conclusion1 IntroductionAs mentioned in my previous “post”, before you can start modeling, a lot of preparatory work is often necessary when preparing the data.</description>
    </item>
    
    <item>
      <title>The use of dummy variables</title>
      <link>/2019/06/14/the-use-of-dummy-variables/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/14/the-use-of-dummy-variables/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Preparation of the dataframe4 How to create dummy variables5 Use dummy variables in a regression analysis6 Dummy variables with more than two characteristics7 How to deal with multiple categorical features in a dataset8 Conclusion1 IntroductionIn a nutshell: a dummy variable is a numeric variable that represents categorical data.</description>
    </item>
    
    <item>
      <title>The use of the groupby function</title>
      <link>/2019/05/30/the-use-of-the-groupby-function/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/30/the-use-of-the-groupby-function/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Group by3.1 with size3.2 with count3.2.1 Count Non - Zero Observations3.3 with sum3.4 with nunique3.5 with mean3.6 with agg.4 Conclusion1 IntroductionGoupby is one of the most used functions in data analysis. Therefore, it is worth to take a closer look at their functioning.
For this post the dataset flight from the statistic platform “Kaggle” was used.</description>
    </item>
    
    <item>
      <title>Random sampling</title>
      <link>/2019/05/16/random-sampling/</link>
      <pubDate>Thu, 16 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/16/random-sampling/</guid>
      <description>Table of Content1 Introduction2 Preparation3 Split-Methods3.1 Customer Churn Model3.2 Train-Test Split via scikit-learn4 Train-Test-Validation Split5 Conclusion1 IntroductionSplitting the dataset in training and testing the dataset is one operation every Data Scientist has to perform befor applying any models. The training dataset is the one on which the model is built and the testing dataset is used to check the accuracy of the model.</description>
    </item>
    
    <item>
      <title>NumPy. An intuition.</title>
      <link>/2019/05/07/numpy-an-intuition/</link>
      <pubDate>Tue, 07 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/07/numpy-an-intuition/</guid>
      <description>Table of Content1 Introduction2 Attributes of NumPy Arrays3 Indexing of Arrays3.1 Access to individual elements3.2 via Slicing3.3 Multidimensional subsets of an Array4 Reshape5 Concatenate Arrays6 Split Arrays7 UFuncs7.1 Array-Arithmetik7.2 Exponential function7.3 Logarithm7.4 Comparison operators8 Aggregation8.1 Multi-dimensional aggregation9 Timing of functions10 Conclusion1 IntroductionNumPy is a library of Python that makes it easy to handle vectors, matrices, or large multidimensional arrays in general.</description>
    </item>
    
    <item>
      <title>Pivot Tables with Python</title>
      <link>/2019/04/24/pivot-tables-with-python/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/24/pivot-tables-with-python/</guid>
      <description>Table of Content1 Introduction2 Getting an overview of our data3 Categorizing the data by Year and Region4 Creating a multi-index pivot table5 Manipulating the data using aggfunc6 Applying a custom function to remove outlier7 Categorizing using string manipulation8 Conclusion1 IntroductionMany people like to work with pivot tables in Excel. This possibility also exists in Python.
For this post the dataset WorldHappinessReport from the statistic platform “Kaggle” was used.</description>
    </item>
    
    <item>
      <title>Data Management</title>
      <link>/2019/04/16/data-management/</link>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/16/data-management/</guid>
      <description>Table of Content1 Introduction2 Join the two dataframes along rows3 Merge two dataframes3.1 Merge with inner join3.2 Merge with outer join4 Merge multiple data frames4.1 Preparation4.2 Merge up to 3 data frames4.3 Merge more than 3 data frames5 Conclusion1 IntroductionOne of the essential skills of a data scientist is to generate and bring together data from different sources.</description>
    </item>
    
    <item>
      <title>Python&#39;s Pipe - Operator</title>
      <link>/2019/04/04/python-s-pipe-operator/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/04/python-s-pipe-operator/</guid>
      <description>Table of Content1 Introduction2 Python’s Pipe - Operator like R’s %&amp;gt;%2.1 Filter and select2.2 Multiple filter and select2.3 Sample and sort2.4 Multiple group by and summarize2.5 Group by and multiple summarize3 Conclusion1 IntroductionAnyone who has ever worked with R probably knows the very useful pipe operator %&amp;gt;%. Python also has a similar one that will be presented in different versions below.</description>
    </item>
    
    <item>
      <title>String Manipulation. An intuition.</title>
      <link>/2019/03/27/string-manipulation-an-intuition/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/27/string-manipulation-an-intuition/</guid>
      <description>Table of Content1 Introduction2 Separate2.1 via map - function2.2 via string function3 Unite3.1 two columns3.2 three and more columns4 add_prefix5 add_suffix6 Conclusion1 IntroductionIt happens again and again that in the course of the planned analysis text variables are unfavorably filled and therefore have to be changed. Here are some useful build in methods for string manipulation from Python.</description>
    </item>
    
    <item>
      <title>Dealing with missing values</title>
      <link>/2019/03/18/dealing-with-missing-values/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/18/dealing-with-missing-values/</guid>
      <description>Table of Content1 Introduction2 Checking for missing values2.1 Missing Value Function2.2 Visualization of missing values3 Deletion of missing values4 Replace missings with values4.1 Variant 14.2 Variant 25 Replace values with missings5.1 Variant 15.2 Variant 26 Further imputations6.1 with mean6.2 with ffill6.3 with backfill7 Conclusion1 IntroductionIn the real world, there is virtually no record that has no missing values.</description>
    </item>
    
    <item>
      <title>Data Manipulation</title>
      <link>/2019/03/12/data-manipulation/</link>
      <pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/12/data-manipulation/</guid>
      <description>Table of Content1 Introduction2 Index2.1 Resetting index2.2 Resetting multiindex2.3 Setting index3 Modifying Columns3.1 Rename Columns3.1.1 add_prefix3.3 Add columns3.4 Drop and Delete Columns3.5 Insert Columns3.6 Rearrange Columns4 Modifying Rows4.1 Round each column4.2 Round columns differently within a df5 Replacing Values5.1 One by One5.2 Collective replacement6 Conclusion1 IntroductionData manipulation is an elementary component in the data science field that requires the most time, among other things.</description>
    </item>
    
    <item>
      <title>Data type conversion</title>
      <link>/2019/03/10/data-type-conversion/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/10/data-type-conversion/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Overview of the exisitng data types4 Type Conversion4.1 Conversion of a single variable4.1.1 float64 to float324.1.2 float to int5 Conversion of multiple variables6 Conversion of date and time variables7 Conclusion1 IntroductionIt will always happen that you have an incorrect or unsuitable data type and you have to change it.</description>
    </item>
    
    <item>
      <title>Add new columns</title>
      <link>/2019/03/06/add-new-columns/</link>
      <pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/06/add-new-columns/</guid>
      <description>Table of Content1 Introduction2 Normal Calculation3 If-else statements4 Multiple If-else statements5 Row Sum6 With a defined list7 Conclusion1 IntroductionThere are several ways to generate new variables in Python. Below the most common methods will be shown.
For this post the dataset flight from the statistic platform “Kaggle” was used. A copy of the record is available at https://drive.google.com/open?id=1w3c818UAJW4VVqwYmgDIsn-b8WqcZQzL.</description>
    </item>
    
    <item>
      <title>Selection of columns per data type</title>
      <link>/2019/03/04/selection-of-columns-per-data-type/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/04/selection-of-columns-per-data-type/</guid>
      <description>1 Introduction2 Loading the libraries and the data3 Selection of numeric variables4 Selection of categorical variables5 Conclusion1 IntroductionIn some situations it is necessary to select all columns of a certain data type. For example if you want to convert all categorical variables into dummy variables in order to be able to calculate a regression.
For this post the dataset Bank Data from the platform “UCI Machine Learning repository” was used.</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>/2019/03/03/data-wrangling/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/03/data-wrangling/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Overview of the data4 Get some statistics5 Select data5.1 Easy selection5.2 Conditional selection5.3 Set option6 Filter6.1 Normal filter6.2 Filter with a defined list6.3 Exclude some columns with a defined list7 Panda’s query8 Conclusion1 IntroductionNever stop learning !
The entry into the field of data science with “R / R-Studio” was a smart matter.</description>
    </item>
    
    <item>
      <title>Read and write to files</title>
      <link>/2019/03/01/read-and-write-to-files/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/01/read-and-write-to-files/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Reading csv-files4 To read json files5 To read text files5.1 with a for loop5.2 with read_csv5.2.1 Convert epoch time to DateTime5.2.2 Write to csv6 How to read further data types7 Conclusion1 IntroductionOne funcion you always need to work with data is to import the records you want to analyze.</description>
    </item>
    
  </channel>
</rss>