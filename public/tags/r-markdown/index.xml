<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R Markdown on Michael Fuchs Python</title>
    <link>/tags/r-markdown/</link>
    <description>Recent content in R Markdown on Michael Fuchs Python</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/r-markdown/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Spectral Clustering</title>
      <link>/2020/07/08/spectral-clustering/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/08/spectral-clustering/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Introducing Spectral Clustering4 Generating some test data5 k-Means6 Spectral Clustering7 Digression: Feature-Engineering &amp;amp; k-Means8 Conclusion1 IntroductionMy post series from the unsupervised machine learning area about cluster algorithms is slowly coming to an end.However, what cluster algorithm cannot be missing in any case is Spectral Clustering.And this is what the following post is about.</description>
    </item>
    
    <item>
      <title>Mean Shift Clustering</title>
      <link>/2020/07/01/mean-shift-clustering/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/01/mean-shift-clustering/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Generating some test data4 Introducing Mean Shift Clustering5 Mean Shift with scikit-learn6 Conclusion1 IntroductionSuppose you have been given the task of discovering groups, or clusters, that share certain characteristics within a dataset. There are various unsupervised machine learning algorithms that can be used to do this.
As we’ve seen in past posts, “k-Means Clustering” and “Affinity Propagation” can be used if you have good or easily separable data, respectively.</description>
    </item>
    
    <item>
      <title>Affinity Propagation</title>
      <link>/2020/06/29/affinity-propagation/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/29/affinity-propagation/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Generating some test data4 Introducing Affinity Propagation5 Affinity Propagation with scikit-learn6 Conclusion1 IntroductionIn the past few posts some cluster algorithms were presented.I wrote extensively about “k-Means Clustering”, “Hierarchical Clustering”, “DBSCAN”, “HDBSCAN” and finally about “Gaussian Mixture Models”.
Fortunately, we are not yet through with the most common cluster algorithms. So now we come to affinity propagation.</description>
    </item>
    
    <item>
      <title>Gaussian Mixture Models</title>
      <link>/2020/06/24/gaussian-mixture-models/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/24/gaussian-mixture-models/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Generating some test data4 Weaknesses of k-Means5 Gaussian Mixture Models6 Determine the optimal k for GMM7 GMM for density estimation8 Conclusion1 IntroductionLet’s come to a further unsupervised learning cluster algorithm: The Gaussian Mixture Models.As simple or good as the K-Means algorithm is, it is often difficult to use in real world situations.</description>
    </item>
    
    <item>
      <title>HDBSCAN</title>
      <link>/2020/06/20/hdbscan/</link>
      <pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/20/hdbscan/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Introducing HDBSCAN4 Parameter Selection for HDBSCAN5 HDBSCAN in action5.1 Functionality of the HDBSCAN algorithm5.2 Visualization options5.3 Predictions with HDBSCAN6 Conclustion1 IntroductionIn the series of unsupervised learning cluster algorithms, we have already got to know “hierarchical clustering” and “density-based clustering (DBSCAN)”. Now we come to an expansion of the DBSCAN algorithm in which the hierarchical approach is integrated.</description>
    </item>
    
    <item>
      <title>DBSCAN</title>
      <link>/2020/06/15/dbscan/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/15/dbscan/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Introducing DBSCAN4 DBSCAN with Scikit-Learn4.1 Data preparation4.2 DBSCAN5 Conclusion1 IntroductionThe next unsupervised machine learning cluster algorithms is the Density-Based Spatial Clustering and Application with Noise (DBSCAN).DBSCAN is a density-based clusering algorithm, which can be used to identify clusters of any shape in a data set containing noise and outliers.</description>
    </item>
    
    <item>
      <title>Hierarchical Clustering</title>
      <link>/2020/06/04/hierarchical-clustering/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/04/hierarchical-clustering/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Introducing hierarchical clustering4 Dendrograms explained5 Hierarchical Clustering with Scikit-Learn6 Hierarchical clustering on real-world data7 Conclusion1 IntroductionThe second cluster algorithm I would like present is hierarchical clustering.Hierarchical clustering is also a type of unsupervised machine learning algorithm used to cluster unlabeled data points within a dataset. Like “k-Means Clustering”, hierarchical clustering also groups together the data points with similar characteristics.</description>
    </item>
    
    <item>
      <title>k-Means Clustering</title>
      <link>/2020/05/19/k-means-clustering/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/05/19/k-means-clustering/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Introducing k-Means4 Preparation of the data record5 Application of k-Means6 Determine the optimal k for k-Means7 Visualization8 Conclusion1 IntroductionAfter dealing with supervised machine learning models, we now come to another important data science area: unsupervised machine learning models.One class of the unsupervised machine learning models are the cluster algorithms.</description>
    </item>
    
    <item>
      <title>Ensemble Modeling - Stacking</title>
      <link>/2020/04/24/ensemble-modeling-stacking/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/24/ensemble-modeling-stacking/</guid>
      <description>Table of Content1 Introduction2 Backgroundinformation on Stacking3 Loading the libraries and the data4 Data pre-processing4.1 One-hot-encoding4.2 LabelBinarizer4.3 Train-Test-Split4.4 Convert to a numpy array5 Building a stacked model5.1 Create a new training set5.2 Train base models5.3 Create a new test set5.4 Fit base models on the complete training set5.5 Train the stacked model6 Comparison of the accuracy7 Conclusion1 IntroductionAfter “Bagging” and “Boosting” we come to the last type of ensemble method: Stacking.</description>
    </item>
    
    <item>
      <title>Ensemble Modeling - Boosting</title>
      <link>/2020/03/26/ensemble-modeling-boosting/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/26/ensemble-modeling-boosting/</guid>
      <description>Table of Content1 Introduction2 Backgroundinformation on Boosting3 Loading the libraries and the data4 Data pre-processing5 AdaBoost (Adaptive Boosting)6 Gradient Boosting7 Conclusion1 IntroductionAfter “Bagging” we come to another type of ensemble method: Boosting.
For this post the dataset Bank Data from the platform “UCI Machine Learning repository” was used. A copy of the record is available at https://drive.google.com/open?id=1MEt3YiQfNxkCl75WSROWf1L5p9_f4FcD.</description>
    </item>
    
    <item>
      <title>Ensemble Modeling - Bagging</title>
      <link>/2020/03/07/ensemble-modeling-bagging/</link>
      <pubDate>Sat, 07 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/07/ensemble-modeling-bagging/</guid>
      <description>Table of Content1 Introduction2 Backgroundinformation on Bagging3 Loading the libraries and the data4 Data pre-processing5 Decision Tree Classifier6 Bagging Classifier7 Random Forest Classifier7.1 Train the Random Forest Classifier7.2 Evaluate the Forest Classifier7.2.1 StratifiedKFold7.2.2 KFold7.3 Hyperparameter optimization via Randomized Search7.4 Determination of feature importance8 Conclusion1 IntroductionSo far we have dealt very intensively with the use of different classification algorithms.</description>
    </item>
    
    <item>
      <title>Saving machine learning models to disc</title>
      <link>/2020/02/29/saving-machine-learning-models-to-disc/</link>
      <pubDate>Sat, 29 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/29/saving-machine-learning-models-to-disc/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Visualization of the data4 Model training5 Safe a model to disc6 Load a model from disc7 Conclusion1 IntroductionWe have seen how to train and use different types of machine learning models. But how do we proceed when we have developed and trained a model with the desired performance?</description>
    </item>
    
    <item>
      <title>Roadmap for Classification Tasks</title>
      <link>/2020/02/19/roadmap-for-classification-tasks/</link>
      <pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/19/roadmap-for-classification-tasks/</guid>
      <description>Table of Content1 Introduction2 Roadmap for Classification Tasks2.1 Data pre-processing2.2 Feature Selection Methods2.3 Classification Algorithms3 Conclusion1 IntroductionAnother big chapter from the supervised machine learning area comes to an end. In the past 4 months I wrote in detail about the functionality and use of the most common classification algorithms within data science.
Analogous to my post “Roadmap for Regression Analysis”, I will give again an overview of the handling of classification tasks.</description>
    </item>
    
    <item>
      <title>Feature selection methods for classification tasks</title>
      <link>/2020/01/31/feature-selection-methods-for-classification-tasks/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/01/31/feature-selection-methods-for-classification-tasks/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Filter methods4 Wrapper methods4.1 SelectKBest4.2 Step Forward Feature Selection4.3 Backward Elimination4.4 Recursive Feature Elimination (RFE)4.5 Exhaustive Feature Selection5 Conclusion1 IntroductionI already wrote about feature selection for regression analysis in this “post”. Feature selection is also relevant for classification problems. And that’s what this post is about.</description>
    </item>
    
    <item>
      <title>Dealing with imbalanced classes</title>
      <link>/2020/01/16/dealing-with-imbalanced-classes/</link>
      <pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/01/16/dealing-with-imbalanced-classes/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Data pre-processing4 Logistic Regression5 Resampling methods5.1 Oversampling5.2 Undersampling6 ML Algorithms for imbalanced datasets6.1 SMOTE (Synthetic Minority Over-sampling Technique)6.2 NearMiss7 Penalize Algorithms8 Tree-Based Algorithms9 Conclusion1 IntroductionThe validation metric ‚Accuracy‘ is a surprisingly common problem in machine learning (specifically in classification), occurring in datasets with a disproportionate ratio of observations in each class.</description>
    </item>
    
    <item>
      <title>Introduction to KNN Classifier</title>
      <link>/2019/12/27/introduction-to-knn-classifier/</link>
      <pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/27/introduction-to-knn-classifier/</guid>
      <description>Table of Content1 Introduction2 Background information on KNN3 Loading the libraries and the data4 KNN - Model Fitting and Evaluation5 Determination of K and Model Improvement6 Conclusion1 IntroductionK Nearest Neighbor (KNN) is a very simple supervised classification algorithm which is easy to understand, versatile and one of the topmost machine learning algorithms. The KNN algorithm can be used for both classification (binary and multiple) and regression problems.</description>
    </item>
    
    <item>
      <title>Introduction to Naive Bayes Classifier</title>
      <link>/2019/12/15/introduction-to-naive-bayes-classifier/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/15/introduction-to-naive-bayes-classifier/</guid>
      <description>Table of Content1 Introduction2 Background information on Naive Bayes Classifier3 Loading the libraries and the data4 Data pre-processing5 Naive Bayes in scikit-learn5.1 Binary Classification5.1.1 Gaussian Naive Bayes5.1.2 Bernoulli Naive Bayes5.2 Multiple Classification5.2.1 Gaussian Naive Bayes5.2.2 Multinomial Naive Bayes6 Conclusion1 IntroductionNow in the series of multiple classifiers we come to a very easy to use probability model: The Naive Bayes Classifier.</description>
    </item>
    
    <item>
      <title>Introduction to Decision Trees</title>
      <link>/2019/11/30/introduction-to-decision-trees/</link>
      <pubDate>Sat, 30 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/30/introduction-to-decision-trees/</guid>
      <description>Table of Content1 Introduction2 Background information on decision trees3 Loading the libraries and the data4 Decision Trees with scikit-learn5 Visualization of the decision tree5.1 via graphviz5.2 via scikit-learn6 Model evaluation7 Model improvement7.1 Hyperparameter optimization via Grid Search8 Conclusion1 IntroductionAfter “Multinomial logistic regression” we come to a further multiple class classifier: Decision Trees.
For this post the dataset Iris from the statistic platform “Kaggle” was used.</description>
    </item>
    
    <item>
      <title>Multinomial logistic regression</title>
      <link>/2019/11/15/multinomial-logistic-regression/</link>
      <pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/15/multinomial-logistic-regression/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Multinomial logistic regression with scikit-learn3.1 Fit the model3.2 Model validation3.3 Calculated probabilities4 Multinomial Logit with the statsmodel library5 Conclusion1 IntroductionIn my previous posts, I explained how “Logistic Regression” and “Support Vector Machines” works. Short wrap up: we used a logistic regression or a support vector machine to create a binary classification model.</description>
    </item>
    
    <item>
      <title>Introduction to Perceptron Algorithm</title>
      <link>/2019/11/14/introduction-to-perceptron-algorithm/</link>
      <pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/14/introduction-to-perceptron-algorithm/</guid>
      <description>Table of Content1 Introduction2 Background information on Perceptron Algorithm3 Loading the libraries and the data4 Perceptron - Model Fitting and Evaluation5 Hyperparameter optimization via Grid Search6 OvO/OvR with the Perceptron7 Perceptron with SGD training8 Conclusion1 IntroductionI already wrote about “Logistic Regression” and “Support Vector Machines”. I also showed how to optimize these linear classifiers using “SGD training” and how to use the “OneVersusRest and OneVersusAll” Classifier to convert binary classifiers to multiple classifiers.</description>
    </item>
    
    <item>
      <title>OvO and OvR Classifier</title>
      <link>/2019/11/13/ovo-and-ovr-classifier/</link>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/13/ovo-and-ovr-classifier/</guid>
      <description>Table of Content1 Introduction2 Background information on OvO and OvR3 Loading the libraries and the data4 OvO/OvR with Logistic Regression4.1 One-vs-Rest4.2 One-vs-One4.3 Grid Search5 OvO/OvR with SVM5.1 One-vs-Rest5.2 One-vs-One5.3 Grid Search6 Conclusion1 IntroductionWe already know from my previous posts how to train a binary classifier using “Logistic Regression” or “Support Vector Machines”.</description>
    </item>
    
    <item>
      <title>Introduction to SGD Classifier</title>
      <link>/2019/11/11/introduction-to-sgd-classifier/</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/11/introduction-to-sgd-classifier/</guid>
      <description>Table of Content1 Introduction2 Background information on SGD Classifiers3 Loading the libraries and the data4 Data pre-processing5 SGDClassifier5.1 Logistic Regression with SGD training5.2 Linear SVM with SGD training6 Model improvement6.1 Performance comparison of the different linear models6.2 GridSearch7 Conclusion1 IntroductionThe name Stochastic Gradient Descent - Classifier (SGD-Classifier) might mislead some user to think that SGD is a classifier.</description>
    </item>
    
    <item>
      <title>Introduction to Support Vector Machines</title>
      <link>/2019/11/08/introduction-to-support-vector-machines/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/08/introduction-to-support-vector-machines/</guid>
      <description>Table of Content1 Introduction2 Background information on Support Vector Machines3 Loading the libraries and the data4 Data pre-processing5 SVM with scikit-learn5.1 Model Fitting5.2 Model evaluation6 Kernel SVM with Scikit-Learn6.1 Polynomial Kernel6.2 Gaussian Kernel6.3 Sigmoid Kernel7 Hyperparameter optimization via Grid Search8 Conclusion1 IntroductionIn addition to “Logistic Regression”, there is another very well-known algorithm for binary classifications: the Support Vector Machine (SVM).</description>
    </item>
    
    <item>
      <title>Grid Search</title>
      <link>/2019/11/04/grid-search/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/04/grid-search/</guid>
      <description>Table of Content1 Introduction2 Background information on Grid Searach3 Loading the libraries and the data4 Data pre-processing5 LogReg6 Grid Search6.1 Grid Search with LogReg6.2 Grid Search with other machine learning algorithms6.3 Grid Search with more than one estimator7 Conclusion1 IntroductionGrid Search is the process of performing hyperparameter tuning in order to determine the optimal values for a given model.</description>
    </item>
    
    <item>
      <title>Introduction to Logistic Regression</title>
      <link>/2019/10/31/introduction-to-logistic-regression/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/31/introduction-to-logistic-regression/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Descriptive statistics3.1 Mean values of the features3.2 Description of the target variable3.3 Description of the predictor variables4 Data pre-processing4.1 Conversion of the target variable4.2 Creation of dummy variables4.3 Feature Selection5 Logistic Regression with the statsmodel library6 Logistic Regression with scikit-learn6.1 Over-sampling using SMOTE6.2 Model Fitting6.</description>
    </item>
    
    <item>
      <title>Roadmap for Regression Analysis</title>
      <link>/2019/10/14/roadmap-for-regression-analysis/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/14/roadmap-for-regression-analysis/</guid>
      <description>Table of Content1 Introduction2 Roadmap for Regression Analysis3 Different types of regression models4 Conclusion1 IntroductionIn my most recent publications, I have dealt extensively with individual topics in the field of regression analysis. This post should serve as a summary of the topics covered.
2 Roadmap for Regression AnalysisHere are the links to the individual topics.
Data pre-processing:</description>
    </item>
    
    <item>
      <title>Embedded methods</title>
      <link>/2019/10/08/embedded-methods/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/08/embedded-methods/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Embedded methods3.1 Ridge Regression3.2 Lasso Regression3.3 Elastic Net4 Grid Search4.1 Grid for Ridge4.2 Grid for embedded methods5 Conclusion1 IntroductionImage Source: “Analytics Vidhya”
Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration.</description>
    </item>
    
    <item>
      <title>Wrapper methods</title>
      <link>/2019/09/27/wrapper-methods/</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/27/wrapper-methods/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Wrap up: Filter methods4 Wrapper methods4.1 Data Preparation4.1.1 Check for missing values4.1.2 Removing highly correlated features4.2 Syntax for wrapper methods4.2.1 SelectKBest4.2.2 Forward Feature Selection4.2.3 Backward Elimination4.2.4 Recursive Feature Elimination (RFE)5 Conclusion1 IntroductionFeature selection is pretty important in machine learning primarily because it serves as a fundamental technique to direct the use of variables to what’s most efficient and effective for a given machine learning system.</description>
    </item>
    
    <item>
      <title>Check for normal distribution</title>
      <link>/2019/09/13/check-for-normal-distribution/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/13/check-for-normal-distribution/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Visual Normality Checks3.1 Quantile-Quantile Plot3.2 Histogram Plot4 Statistical Normality Tests4.1 Shapiro-Wilk Test4.2 D’Agostino’s K2 Test5 Conclusion1 IntroductionIn my previous “post” the question came up of how to check its data on normal distribution. There are several possibilities for this.
2 Loading the librariesimport pandas as pdimport numpy as npimport pylab import scipy.</description>
    </item>
    
    <item>
      <title>Feature Scaling with Scikit-Learn</title>
      <link>/2019/08/31/feature-scaling-with-scikit-learn/</link>
      <pubDate>Sat, 31 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/31/feature-scaling-with-scikit-learn/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Scaling methods3.1 Standard Scaler3.2 Min-Max Scaler3.3 Robust Scaler3.4 Comparison of the previously shown scaling methods4 Feature Scaling in practice5 Conclusion1 IntroductionFeature scaling can be an important part for many machine learning algorithms. It’s a step of data pre-processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range.</description>
    </item>
    
    <item>
      <title>Dealing with outliers</title>
      <link>/2019/08/20/dealing-with-outliers/</link>
      <pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/20/dealing-with-outliers/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Boxplots - Method4 Z-score method5 IQR method5.1 Detect outlier for column ‘age’5.2 Detect outlier for column ‘salary’5.3 Remove outlier from dataframe6 Conclusion1 IntroductionNext to “higly correlated” and “constant” features outlier detection is also a central element of data pre-processing.
In statistics, outliers are data points that do not belong to any particular population.</description>
    </item>
    
    <item>
      <title>Dealing with constant and duplicate features</title>
      <link>/2019/08/09/dealing-with-constant-and-duplicate-features/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/09/dealing-with-constant-and-duplicate-features/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Removing Constant features4 Removing Quasi-Constant features5 Removing Duplicate Features6 Conclusion1 IntroductionIn addition to “removing highly correlated features” as one of the data pre processing steps we also have to take care of constant and duplicate features. Constant features have a variance close to zero and duplicate features are too similar to other variables in the record.</description>
    </item>
    
    <item>
      <title>Dealing with highly correlated features</title>
      <link>/2019/07/28/dealing-with-highly-correlated-features/</link>
      <pubDate>Sun, 28 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/28/dealing-with-highly-correlated-features/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Preparation4 Correlations with the output variable5 Identification of highly correlated features6 Removing highly correlated features6.1 Selecting numerical variables6.2 Train / Test Split7 Conclusion1 IntroductionOne of the points to remember about data pre-processing for regression analysis is multicollinearity. This post is about finding highly correlated predictors within a dataframe.</description>
    </item>
    
    <item>
      <title>Non-linear regression analysis</title>
      <link>/2019/07/14/non-linear-regression-analysis/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/14/non-linear-regression-analysis/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Data Preparation4 Hypothesis: a non-linear relationship5 Linear model6 Non linear models6.1 Quadratic Function6.2 Exponential Function6.3 Logarithm Function6.4 Polynomials Function7 Conclusion1 IntroductionIn my previous post “Introduction to regression analysis and predictions” I showed how to create linear regression models. But what can be done if the data is not distributed linearly?</description>
    </item>
    
    <item>
      <title>statsmodel.formula.api vs statsmodel.api</title>
      <link>/2019/07/02/statsmodel-formula-api-vs-statsmodel-api/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/02/statsmodel-formula-api-vs-statsmodel-api/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 The statsmodel.formula.api4 The statsmodel.api5 Conclusion1 IntroductionImage Source: “Statsmodels.org”
In my last post “Introduction to regression analysis and predictions” I used the statsmodel library to identify significant features influencing the property price. In this publication I would like to show the difference of the statsmodel.formula.api (smf) and the statsmodel.api (sm).</description>
    </item>
    
    <item>
      <title>Introduction to regression analysis and predictions</title>
      <link>/2019/06/28/introduction-to-regression-analysis-and-predictions/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/28/introduction-to-regression-analysis-and-predictions/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Implementing linear regression with the statsmodel library3.1 Simple linear Regression3.2 Multiple Regression3.3 Model validation4 Linear Regression with scikit-learn5 Conclusion1 IntroductionRegression analyzes are very common and should therefore be mastered by every data scientist.
For this post the dataset House Sales in King County, USA from the statistic platform “Kaggle” was used.</description>
    </item>
    
    <item>
      <title>Types of Encoder</title>
      <link>/2019/06/16/types-of-encoder/</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/16/types-of-encoder/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Encoder for predictor variables3.1 One Hot Encoder3.1.1 via scikit-learn3.1.2 via pandas3.2 Ordinal Encoder4 Encoder for target variables4.1 Label Binarizer4.2 Label Encoding5 Conclusion1 IntroductionAs mentioned in my previous “post”, before you can start modeling, a lot of preparatory work is often necessary when preparing the data.</description>
    </item>
    
    <item>
      <title>The use of dummy variables</title>
      <link>/2019/06/14/the-use-of-dummy-variables/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/14/the-use-of-dummy-variables/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Preparation of the dataframe4 How to create dummy variables5 Use dummy variables in a regression analysis6 Dummy variables with more than two characteristics7 How to deal with multiple categorical features in a dataset8 Conclusion1 IntroductionIn a nutshell: a dummy variable is a numeric variable that represents categorical data.</description>
    </item>
    
    <item>
      <title>The use of the groupby function</title>
      <link>/2019/05/30/the-use-of-the-groupby-function/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/30/the-use-of-the-groupby-function/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Group by3.1 with size3.2 with count3.2.1 Count Non - Zero Observations3.3 with sum3.4 with nunique3.5 with mean3.6 with agg.4 Conclusion1 IntroductionGoupby is one of the most used functions in data analysis. Therefore, it is worth to take a closer look at their functioning.</description>
    </item>
    
    <item>
      <title>Random sampling</title>
      <link>/2019/05/16/random-sampling/</link>
      <pubDate>Thu, 16 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/16/random-sampling/</guid>
      <description>Table of Content1 Introduction2 Preparation3 Split-Methods3.1 Customer Churn Model3.2 Train-Test Split via scikit-learn4 Train-Test-Validation Split5 Conclusion1 IntroductionSplitting the dataset in training and testing the dataset is one operation every Data Scientist has to perform befor applying any models. The training dataset is the one on which the model is built and the testing dataset is used to check the accuracy of the model.</description>
    </item>
    
    <item>
      <title>NumPy. An intuition.</title>
      <link>/2019/05/07/numpy-an-intuition/</link>
      <pubDate>Tue, 07 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/07/numpy-an-intuition/</guid>
      <description>Table of Content1 Introduction2 Attributes of NumPy Arrays3 Indexing of Arrays3.1 Access to individual elements3.2 via Slicing3.3 Multidimensional subsets of an Array4 Reshape5 Concatenate Arrays6 Split Arrays7 UFuncs7.1 Array-Arithmetik7.2 Exponential function7.3 Logarithm7.4 Comparison operators8 Aggregation8.1 Multi-dimensional aggregation9 Timing of functions10 Conclusion1 IntroductionNumPy is a library of Python that makes it easy to handle vectors, matrices, or large multidimensional arrays in general.</description>
    </item>
    
    <item>
      <title>Pivot Tables with Python</title>
      <link>/2019/04/24/pivot-tables-with-python/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/24/pivot-tables-with-python/</guid>
      <description>Table of Content1 Introduction2 Getting an overview of our data3 Categorizing the data by Year and Region4 Creating a multi-index pivot table5 Manipulating the data using aggfunc6 Applying a custom function to remove outlier7 Categorizing using string manipulation8 Conclusion1 IntroductionMany people like to work with pivot tables in Excel. This possibility also exists in Python.
For this post the dataset WorldHappinessReport from the statistic platform “Kaggle” was used.</description>
    </item>
    
    <item>
      <title>Data Management</title>
      <link>/2019/04/16/data-management/</link>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/16/data-management/</guid>
      <description>Table of Content1 Introduction2 Join the two dataframes along rows3 Merge two dataframes3.1 Merge with inner join3.2 Merge with outer join4 Merge multiple data frames4.1 Preparation4.2 Merge up to 3 data frames4.3 Merge more than 3 data frames5 Conclusion1 IntroductionOne of the essential skills of a data scientist is to generate and bring together data from different sources.</description>
    </item>
    
    <item>
      <title>Python&#39;s Pipe - Operator</title>
      <link>/2019/04/04/python-s-pipe-operator/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/04/python-s-pipe-operator/</guid>
      <description>Table of Content1 Introduction2 Python’s Pipe - Operator like R’s %&amp;gt;%2.1 Filter and select2.2 Multiple filter and select2.3 Sample and sort2.4 Multiple group by and summarize2.5 Group by and multiple summarize3 Conclusion1 IntroductionAnyone who has ever worked with R probably knows the very useful pipe operator %&amp;gt;%. Python also has a similar one that will be presented in different versions below.</description>
    </item>
    
    <item>
      <title>String Manipulation. An intuition.</title>
      <link>/2019/03/27/string-manipulation-an-intuition/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/27/string-manipulation-an-intuition/</guid>
      <description>Table of Content1 Introduction2 Separate2.1 via map - function2.2 via string function3 Unite3.1 two columns3.2 three and more columns4 add_prefix5 add_suffix6 Conclusion1 IntroductionIt happens again and again that in the course of the planned analysis text variables are unfavorably filled and therefore have to be changed. Here are some useful build in methods for string manipulation from Python.</description>
    </item>
    
    <item>
      <title>Dealing with missing values</title>
      <link>/2019/03/18/dealing-with-missing-values/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/18/dealing-with-missing-values/</guid>
      <description>Table of Content1 Introduction2 Checking for missing values2.1 Missing Value Function2.2 Visualization of missing values3 Deletion of missing values4 Replace missings with values4.1 Variant 14.2 Variant 25 Replace values with missings5.1 Variant 15.2 Variant 26 Further imputations6.1 with mean6.2 with ffill6.3 with backfill7 Conclusion1 IntroductionIn the real world, there is virtually no record that has no missing values.</description>
    </item>
    
    <item>
      <title>Data Manipulation</title>
      <link>/2019/03/12/data-manipulation/</link>
      <pubDate>Tue, 12 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/12/data-manipulation/</guid>
      <description>Table of Content1 Introduction2 Index2.1 Resetting index2.2 Resetting multiindex2.3 Setting index3 Modifying Columns3.1 Rename Columns3.1.1 add_prefix3.3 Add columns3.4 Drop and Delete Columns3.5 Insert Columns3.6 Rearrange Columns4 Modifying Rows4.1 Round each column4.2 Round columns differently within a df5 Replacing Values5.1 One by One5.2 Collective replacement5.3 Conditional replacement6 Conclusion1 IntroductionData manipulation is an elementary component in the data science field that requires the most time, among other things.</description>
    </item>
    
    <item>
      <title>Data type conversion</title>
      <link>/2019/03/10/data-type-conversion/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/10/data-type-conversion/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Overview of the exisitng data types4 Type Conversion4.1 Conversion of a single variable4.1.1 float64 to float324.1.2 float to int5 Conversion of multiple variables6 Conversion of date and time variables7 Conclusion1 IntroductionIt will always happen that you have an incorrect or unsuitable data type and you have to change it.</description>
    </item>
    
    <item>
      <title>Add new columns</title>
      <link>/2019/03/06/add-new-columns/</link>
      <pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/06/add-new-columns/</guid>
      <description>Table of Content1 Introduction2 Normal Calculation3 If-else statements4 Multiple If-else statements4.1 with conditional output values4.2 with conditional calculation5 Row Sum6 With a defined list7 Conclusion1 IntroductionThere are several ways to generate new variables in Python. Below the most common methods will be shown.
For this post the dataset flight from the statistic platform “Kaggle” was used.</description>
    </item>
    
    <item>
      <title>Selection of columns per data type</title>
      <link>/2019/03/04/selection-of-columns-per-data-type/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/04/selection-of-columns-per-data-type/</guid>
      <description>1 Introduction2 Loading the libraries and the data3 Selection of numeric variables4 Selection of categorical variables5 Conclusion1 IntroductionIn some situations it is necessary to select all columns of a certain data type. For example if you want to convert all categorical variables into dummy variables in order to be able to calculate a regression.
For this post the dataset Bank Data from the platform “UCI Machine Learning repository” was used.</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>/2019/03/03/data-wrangling/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/03/data-wrangling/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries and the data3 Overview of the data4 Get some statistics5 Select data5.1 Easy selection5.2 Conditional selection5.3 Set option6 Filter6.1 Normal filter6.2 Filter with a defined list6.3 Exclude some columns with a defined list7 Panda’s query8 Conclusion1 IntroductionNever stop learning !
The entry into the field of data science with “R / R-Studio” was a smart matter.</description>
    </item>
    
    <item>
      <title>Read and write to files</title>
      <link>/2019/03/01/read-and-write-to-files/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/01/read-and-write-to-files/</guid>
      <description>Table of Content1 Introduction2 Loading the libraries3 Reading csv-files4 Reading json files5 Read text files5.1 with a for loop5.2 with read_csv5.2.1 Convert epoch time to DateTime6 Write to csv7 Write to excel8 How to read further data types9 Conclusion1 IntroductionOne funcion you always need to work with data is to import the records you want to analyze.</description>
    </item>
    
  </channel>
</rss>