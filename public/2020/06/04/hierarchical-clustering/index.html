<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.69.0" />


<title>Hierarchical Clustering - Michael Fuchs Python</title>
<meta property="og:title" content="Hierarchical Clustering - Michael Fuchs Python">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/MFuchs.png"
         width="50"
         height="50"
         alt="MFuchs">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/MFuchs1989/">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/michael-fuchs-139172131/">LinkedIn</a></li>
    
    <li><a href="https://twitter.com/Stat_Michael">Twitter</a></li>
    
    <li><a href="https://www.xing.com/profile/Michael_Fuchs426/cv?sc_o=mxb_p">XING</a></li>
    
    <li><a href="https://michael-fuchs.netlify.com/">zum R-Blog</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">Hierarchical Clustering</h1>

    
    <span class="article-date">2020-06-04</span>
    

    <div class="article-content">
      


<div id="table-of-content" class="section level1">
<h1>Table of Content</h1>
<ul>
<li>1 Introduction</li>
<li>2 Loading the libraries</li>
<li>3 Introducing hierarchical clustering</li>
<li>4 Dendrograms explained</li>
<li>5 Hierarchical Clustering with Scikit-Learn</li>
<li>6 Hierarchical clustering on real-world data</li>
<li>7 Conclusion</li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>The second cluster algorithm I would like present is hierarchical clustering.
Hierarchical clustering is also a type of unsupervised machine learning algorithm used to cluster unlabeled data points within a dataset. Like <a href="https://michael-fuchs-python.netlify.app/2020/05/19/k-means-clustering/">“k-Means Clustering”</a>, hierarchical clustering also groups together the data points with similar characteristics.</p>
<p>For this post the dataset <em>Mall_Customers</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used. A copy of the record is available at <a href="https://drive.google.com/file/d/16aeG6dy8j1P70kqTEBsJm4fojK3_a6cq/view?usp=sharing" class="uri">https://drive.google.com/file/d/16aeG6dy8j1P70kqTEBsJm4fojK3_a6cq/view?usp=sharing</a>.</p>
</div>
<div id="loading-the-libraries" class="section level1">
<h1>2 Loading the libraries</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler</code></pre>
</div>
<div id="introducing-hierarchical-clustering" class="section level1">
<h1>3 Introducing hierarchical clustering</h1>
<p><strong>Theory of hierarchical clustering</strong></p>
<p>There are two types of hierarchical clustering:</p>
<ul>
<li>Agglomerative and</li>
<li>Divisive</li>
</ul>
<p>In the course of the first variant, data points are clustered using a bottom-up approach starting with individual data points, while in the second variant top-down approach is followed where all the data points are treated as one big cluster and the clustering process involves dividing the one big cluster into several small clusters.</p>
<p>In this post we will focus on agglomerative clustering that involves the bottom-up approach since this method is used almost exclusively in the real world.</p>
<p><strong>Steps to perform hierarchical clustering</strong></p>
<p>Following are the steps involved in agglomerative clustering:</p>
<ul>
<li>At the beginning, treat each data point as one cluster. Therefore, the number of clusters at the start will be k, while k is an integer representing the number of data points</li>
<li>Second: Form a cluster by joining the two closest data points resulting in k-1 clusters</li>
<li>Third: Form more clusters by joining the two closest clusters resulting in k-2 clusters</li>
</ul>
<p>Repeat the above three steps until one big cluster is formed.
Once single cluster is formed, dendrograms are used to divide into multiple clusters depending upon the problem.</p>
</div>
<div id="dendrograms-explained" class="section level1">
<h1>4 Dendrograms explained</h1>
<p>Let me explain the use of dendrograms with the following example dataframe.</p>
<pre class="r"><code>df = pd.DataFrame({&#39;Col1&#39;: [5, 9, 13, 22, 31, 90, 81, 70, 45, 73, 85],
                   &#39;Col2&#39;: [2, 8, 11, 10, 25, 80, 90, 80, 60, 62, 90]})
df</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p1.png" /></p>
<p>First we’ll convert this dataframe to a numpy array.</p>
<pre class="r"><code>arr = np.array(df)
arr</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p2.png" /></p>
<p>Now we plot the example dataframe:</p>
<pre class="r"><code>labels = range(1, 12)
plt.figure(figsize=(10, 7))
plt.subplots_adjust(bottom=0.1)
plt.scatter(arr[:,0],arr[:,1], label=&#39;True Position&#39;)

for label, x, y in zip(labels, arr[:, 0], arr[:, 1]):
    plt.annotate(
        label,
        xy=(x, y), xytext=(-3, 3),
        textcoords=&#39;offset points&#39;, ha=&#39;right&#39;, va=&#39;bottom&#39;)
plt.show()</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p3.png" /></p>
<p>Now we have a rough idea of the underlying distribution of the data points.
Let’s plot our first dendrogram:</p>
<pre class="r"><code>linked = shc.linkage(arr, &#39;single&#39;)

labelList = range(1, 12)

plt.figure(figsize=(10, 7))
shc.dendrogram(linked,
            orientation=&#39;top&#39;,
            labels=labelList,
            distance_sort=&#39;descending&#39;,
            show_leaf_counts=True)
plt.axhline(y=23, color=&#39;r&#39;, linestyle=&#39;--&#39;)
plt.show()</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p4.png" /></p>
<p>The algorithm starts by finding the two points that are closest to each other on the basis of euclidean distance.
The vertical height of the dendogram shows the euclidean distances between points.</p>
<p>In the end, we can read from the present graphic that data points 1-5 forms a cluster as well as 6,7,8, 10 and 11.
Data point 9 seems to be a own cluster.</p>
<p>We can see that the largest vertical distance without any horizontal line passing through it is represented by blue line.
So I draw a horizontal red line within the dendogram that passes through the blue line.
Since it crosses the blue line at three points, therefore the number of clusters will be 3.</p>
</div>
<div id="hierarchical-clustering-with-scikit-learn" class="section level1">
<h1>5 Hierarchical Clustering with Scikit-Learn</h1>
<p>Now it’s time to use scikits’AgglomerativeClustering class and call its fit_predict method to predict the clusters that each data point belongs to.</p>
<pre class="r"><code>hc = AgglomerativeClustering(n_clusters=3, affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39;)
hc.fit_predict(arr)</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p5.png" /></p>
<p>Finally, let’s plot our clusters.</p>
<pre class="r"><code>plt.scatter(arr[:,0],arr[:,1], c=hc.labels_, cmap=&#39;rainbow&#39;)</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p6.png" /></p>
</div>
<div id="hierarchical-clustering-on-real-world-data" class="section level1">
<h1>6 Hierarchical clustering on real-world data</h1>
<p>That was an easy example of how to use dendograms and the AgglomerativeClustering algorithm.
Now let’s see how it works with real-world data.</p>
<pre class="r"><code>df = pd.read_csv(&quot;Mall_Customers.csv&quot;)
mall = df.drop([&#39;CustomerID&#39;, &#39;Gender&#39;], axis=1)
mall</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p7.png" /></p>
<p>The steps are similar as before.</p>
<pre class="r"><code>mall_arr = np.array(mall)</code></pre>
<pre class="r"><code>plt.figure(figsize=(10, 7))
plt.title(&quot;Customer Dendograms&quot;)
plt.axhline(y=350, color=&#39;r&#39;, linestyle=&#39;--&#39;)
dend = shc.dendrogram(shc.linkage(mall_arr, method=&#39;ward&#39;))</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p8.png" /></p>
<p>I have again drawn the threshold I chose with a red line.
Now it’s time for some predictions:</p>
<pre class="r"><code>hc = AgglomerativeClustering(n_clusters=3, affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39;)
predictions = hc.fit_predict(mall_arr)
predictions</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p9.png" /></p>
<pre class="r"><code>plt.figure(figsize=(10, 7))
plt.scatter(mall_arr[:,0],mall_arr[:,1], c=hc.labels_, cmap=&#39;rainbow&#39;)
plt.title(&quot;Plot of the clusters&quot;)</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p10.png" /></p>
<p>That looks a bit messy now … let’s see if we can do it better if we take out the variable age.</p>
<pre class="r"><code>mall = mall.drop([&#39;Age&#39;], axis=1)
mall</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p11.png" /></p>
<pre class="r"><code>mall_arr = np.array(mall)</code></pre>
<pre class="r"><code>plt.figure(figsize=(10, 7))
plt.title(&quot;Customer Dendograms&quot;)
plt.axhline(y=200, color=&#39;r&#39;, linestyle=&#39;--&#39;)
dend = shc.dendrogram(shc.linkage(mall_arr, method=&#39;ward&#39;))</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p12.png" /></p>
<p>Very nice. Now there are apparently 5 clusters.
Let’s do some predictions again.</p>
<pre class="r"><code>hc = AgglomerativeClustering(n_clusters=5, affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39;)
predictions = hc.fit_predict(mall_arr)
predictions</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p13.png" /></p>
<pre class="r"><code>plt.figure(figsize=(10, 7))
plt.scatter(mall_arr[predictions==0, 0], mall_arr[predictions==0, 1], s=55, c=&#39;red&#39;, label =&#39;Cluster 1&#39;)
plt.scatter(mall_arr[predictions==1, 0], mall_arr[predictions==1, 1], s=55, c=&#39;blue&#39;, label =&#39;Cluster 2&#39;)
plt.scatter(mall_arr[predictions==2, 0], mall_arr[predictions==2, 1], s=55, c=&#39;green&#39;, label =&#39;Cluster 3&#39;)
plt.scatter(mall_arr[predictions==3, 0], mall_arr[predictions==3, 1], s=55, c=&#39;cyan&#39;, label =&#39;Cluster 4&#39;)
plt.scatter(mall_arr[predictions==4, 0], mall_arr[predictions==4, 1], s=55, c=&#39;magenta&#39;, label =&#39;Cluster 5&#39;)

plt.title(&#39;Clusters of Mall Customers \n (Hierarchical Clustering Model)&#39;)
plt.xlabel(&#39;Annual Income(k$) \n\n Cluster1(Red), Cluster2 (Blue), Cluster3(Green), Cluster4(Cyan), Cluster5 (Magenta)&#39;)
plt.ylabel(&#39;Spending Score(1-100)&#39;)
plt.show()</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p14.png" /></p>
<p>That looks useful now.
What we can now read from the graphic are the customers’ segments.</p>
<p>I would have made the following interpretation:</p>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46s1.png" /></p>
<p>Now that I want to examine the clusters more closely, I add the predictions to my data set. Be careful at this point since python always starts with 0 for counting. That is the reason why I use predictions + 1 in the code below. This way I generate clusters from 1-5.</p>
<pre class="r"><code>df_pred = pd.DataFrame(predictions)

#be carefull here ... +1 because we start with count 1 python 0
mall[&#39;cluster&#39;] = predictions + 1
mall.head()</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p15.png" /></p>
<p>In order to be able to better assess the target group, I add the variables ‘Age’ and ‘Gender’ from the original data set and filter according to the target group.</p>
<pre class="r"><code>mall[&#39;age&#39;] = df[&#39;Age&#39;]
mall[&#39;gender&#39;] = df[&#39;Gender&#39;]

#filter for the target group
target_group = mall[(mall[&quot;cluster&quot;] == 3)] 
target_group.head()</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p16.png" /></p>
<p>Now it’s time for some statistics:</p>
<pre class="r"><code>df = target_group.agg([&#39;mean&#39;, &#39;median&#39;, &#39;std&#39;, &#39;min&#39;, &#39;max&#39;]).reset_index()
df.drop([&#39;cluster&#39;, &#39;gender&#39;], axis=1)</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p17.png" /></p>
<p>Last but not least, the division of men and women:</p>
<pre class="r"><code>target_group[&#39;gender&#39;].value_counts()</code></pre>
<p><img src="/post/2020-06-04-hierarchical-clustering_files/p46p18.png" /></p>
</div>
<div id="conclusion" class="section level1">
<h1>7 Conclusion</h1>
<p>Unfortunately, the data set does not provide any other sensible variables.
At this point it starts to get exciting and valuable knowledge about the different customer segments can be gained.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

