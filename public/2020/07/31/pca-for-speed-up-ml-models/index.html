<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.69.0" />


<title>PCA for speed up ML models - Michael Fuchs Python</title>
<meta property="og:title" content="PCA for speed up ML models - Michael Fuchs Python">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/MFuchs.png"
         width="50"
         height="50"
         alt="MFuchs">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/MFuchs1989/">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/michael-fuchs-139172131/">LinkedIn</a></li>
    
    <li><a href="https://twitter.com/Stat_Michael">Twitter</a></li>
    
    <li><a href="https://www.xing.com/profile/Michael_Fuchs426/cv?sc_o=mxb_p">XING</a></li>
    
    <li><a href="https://michael-fuchs.netlify.com/">zum R-Blog</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">4 min read</span>
    

    <h1 class="article-title">PCA for speed up ML models</h1>

    
    <span class="article-date">2020-07-31</span>
    

    <div class="article-content">
      


<div id="table-of-content" class="section level1">
<h1>Table of Content</h1>
<ul>
<li>1 Introduction</li>
<li>2 Loading the libraries</li>
<li>3 LogReg</li>
<li>4 LogReg with PCA</li>
<li>4.1 PCA with 95% variance explanation</li>
<li>4.2 PCA with 80% variance explanation</li>
<li>4.3 Summary</li>
<li>5 Export PCA to use in another program</li>
<li>6 Conclusion</li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>As already announced in post about <a href="https://michael-fuchs-python.netlify.app/2020/07/22/principal-component-analysis-pca/">“PCA”</a>, we now come to the second main application of a PCA: Principal Component Analysis for speed up machine learning models.</p>
<p>For this post the dataset <em>MNIST</em> from the statistic platform <a href="https://www.kaggle.com">“Kaggle”</a> was used. A copy of the record is available at <a href="https://drive.google.com/open?id=1Bfquk0uKnh6B3Yjh2N87qh0QcmLokrVk" class="uri">https://drive.google.com/open?id=1Bfquk0uKnh6B3Yjh2N87qh0QcmLokrVk</a>.</p>
</div>
<div id="loading-the-libraries-and-the-dataset" class="section level1">
<h1>2 Loading the libraries and the dataset</h1>
<pre class="r"><code>import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

from sklearn.linear_model import LogisticRegression

from sklearn.decomposition import PCA

import pickle as pk</code></pre>
<pre class="r"><code>mnist = pd.read_csv(&#39;mnist_train.csv&#39;)
mnist</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p1.png" /></p>
<pre class="r"><code>mnist[&#39;label&#39;].value_counts().T</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p2.png" /></p>
</div>
<div id="logreg" class="section level1">
<h1>3 LogReg</h1>
<p>If you want to know how the algorithm of the logistic regression works exactly see <a href="https://michael-fuchs-python.netlify.app/2019/10/31/introduction-to-logistic-regression/">“this post”</a> of mine.</p>
<pre class="r"><code>x = mnist.drop([&#39;label&#39;], axis=1)
y = mnist[&#39;label&#39;]

trainX, testX, trainY, testY = train_test_split(x, y, test_size = 0.2)</code></pre>
<pre class="r"><code>sc=StandardScaler()

# Fit on training set only!
sc.fit(trainX)

# Apply transform to both the training set and the test set.
trainX_scaled = sc.transform(trainX)
testX_scaled = sc.transform(testX)</code></pre>
<pre class="r"><code># all parameters not specified are set to their defaults

logReg = LogisticRegression()</code></pre>
<pre class="r"><code>import time

start = time.time()

print(logReg.fit(trainX_scaled, trainY))

end = time.time()
print()
print(&#39;Calculation time: &#39; + str(end - start) + &#39; seconds&#39;)</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p3.png" /></p>
<pre class="r"><code>y_pred = logReg.predict(testX_scaled)</code></pre>
<pre class="r"><code>print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred)))</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p4.png" /></p>
</div>
<div id="logreg-with-pca" class="section level1">
<h1>4 LogReg with PCA</h1>
</div>
<div id="pca-with-95-variance-explanation" class="section level1">
<h1>4.1 PCA with 95% variance explanation</h1>
<p>Notice the code below has .95 for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained.</p>
<pre class="r"><code>pca = PCA(.95)</code></pre>
<pre class="r"><code># Fitting PCA on the training set only
pca.fit(trainX_scaled)</code></pre>
<p>You can find out how many components PCA choose after fitting the model using pca.n_components_ . In this case, 95% of the variance amounts to 326 principal components.</p>
<pre class="r"><code>pca.n_components_</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p5.png" /></p>
<pre class="r"><code>trainX_pca = pca.transform(trainX_scaled)
testX_pca = pca.transform(testX_scaled)</code></pre>
<pre class="r"><code># all parameters not specified are set to their defaults

logReg = LogisticRegression()</code></pre>
<pre class="r"><code>import time

start = time.time()

print(logReg.fit(trainX_pca, trainY))

end = time.time()
print()
print(&#39;Calculation time: &#39; + str(end - start) + &#39; seconds&#39;)</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p6.png" /></p>
<pre class="r"><code>y_pred = logReg.predict(testX_pca)</code></pre>
<pre class="r"><code>print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred)))</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p7.png" /></p>
<p>Now let’s try 80% variance explanation.</p>
</div>
<div id="pca-with-80-variance-explanation" class="section level1">
<h1>4.2 PCA with 80% variance explanation</h1>
<pre class="r"><code>pca = PCA(.80)</code></pre>
<pre class="r"><code># Fitting PCA on the training set only
pca.fit(trainX_scaled)</code></pre>
<pre class="r"><code>pca.n_components_</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p8.png" /></p>
<pre class="r"><code>trainX_pca = pca.transform(trainX_scaled)
testX_pca = pca.transform(testX_scaled)</code></pre>
<pre class="r"><code># all parameters not specified are set to their defaults

logReg = LogisticRegression()</code></pre>
<pre class="r"><code>import time

start = time.time()

print(logReg.fit(trainX_pca, trainY))

end = time.time()
print()
print(&#39;Calculation time: &#39; + str(end - start) + &#39; seconds&#39;)</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p9.png" /></p>
<pre class="r"><code>y_pred = logReg.predict(testX_pca)</code></pre>
<pre class="r"><code>print(&#39;Accuracy: {:.2f}&#39;.format(accuracy_score(testY, y_pred)))</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p10.png" /></p>
</div>
<div id="summary" class="section level1">
<h1>4.3 Summary</h1>
<p>As we can see in the overview below, not only has the training time has been reduced by PCA, but the prediction accuracy of the trained model has also increased.</p>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57s1.png" /></p>
</div>
<div id="export-pca-to-use-in-another-program" class="section level1">
<h1>5 Export PCA to use in another program</h1>
<p>For a nice example we create the following artificial data set:</p>
<pre class="r"><code>df = pd.DataFrame({&#39;Col1&#39;: [5464, 2484, 846546],
                   &#39;Col2&#39;: [5687,78455,845684],
                   &#39;Col3&#39;: [8754,7686,4585],
                   &#39;Col4&#39;: [49864, 89481, 92254],
                   &#39;Col5&#39;: [22168, 63689, 5223]})
df</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p11.png" /></p>
<pre class="r"><code>df[&#39;Target&#39;] = df.sum(axis=1)
df</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p12.png" /></p>
<p><strong>Note:</strong> We skip the scaling step and the train test split here. In the following, we only want to train the algorithms as well as their storage and use in other programs. Validation is also not a focus here.</p>
<pre class="r"><code>X = df.drop([&#39;Target&#39;], axis=1)
Y = df[&#39;Target&#39;]</code></pre>
<pre class="r"><code>pca = PCA(n_components=2)</code></pre>
<pre class="r"><code>pca.fit(X)
result = pca.transform(X)</code></pre>
<pre class="r"><code>components = pd.DataFrame(pca.components_, columns = X.columns, index=[1, 2])
components = components.T
components.columns = [&#39;Principle_Component_1&#39;, &#39;Principle_Component_2&#39;]
components</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p13.png" /></p>
<pre class="r"><code># all parameters not specified are set to their defaults

logReg = LogisticRegression()

logReg.fit(result, Y)</code></pre>
<pre class="r"><code>pk.dump(pca, open(&quot;pca.pkl&quot;,&quot;wb&quot;))
pk.dump(logReg, open(&quot;logReg.pkl&quot;,&quot;wb&quot;))</code></pre>
<p>The models are saved in the corresponding path and should look like this:</p>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57s2.png" /></p>
<p>In order to show that the principal component analysis has been saved with the correct weightings and reloaded accordingly, we create exactly the same artificial data set (only without target variable) as at the beginning of this exercise.</p>
<pre class="r"><code>df_new = pd.DataFrame({&#39;Col1&#39;: [5464, 2484, 846546],
                   &#39;Col2&#39;: [5687,78455,845684],
                   &#39;Col3&#39;: [8754,7686,4585],
                   &#39;Col4&#39;: [49864, 89481, 92254],
                   &#39;Col5&#39;: [22168, 63689, 5223]})
df_new</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p14.png" /></p>
<p>Now we reload the saved models:</p>
<pre class="r"><code>pca_reload = pk.load(open(&quot;pca.pkl&quot;,&#39;rb&#39;))
logReg_reload = pk.load(open(&quot;logReg.pkl&quot;,&#39;rb&#39;))</code></pre>
<pre class="r"><code>result_new = pca_reload .transform(df_new)</code></pre>
<pre class="r"><code>components = pd.DataFrame(pca.components_, columns = X.columns, index=[1, 2])
components = components.T
components.columns = [&#39;Principle_Component_1&#39;, &#39;Principle_Component_2&#39;]
components</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p15.png" /></p>
<p>We see that the weights have been adopted, as we can compare this output with the first transformation (see above).</p>
<pre class="r"><code>y_pred = logReg_reload.predict(result_new)
y_pred</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p16.png" /></p>
<p>Last but not least we’ll add the predicted values to our original dataframe.</p>
<pre class="r"><code>df_y_pred = pd.DataFrame(y_pred)
df_result_new = pd.DataFrame(result_new)

result_new = pd.concat([df_result_new, df_y_pred], axis=1)
result_new.columns = [&#39;Principle_Component_1&#39;, &#39;Principle_Component_2&#39;, &#39;Prediction&#39;]
result_new</code></pre>
<p><img src="/post/2020-07-31-pca-for-speed-up-ml-models_files/p57p17.png" /></p>
</div>
<div id="conclusion" class="section level1">
<h1>6 Conclusion</h1>
<p>In this post, I showed how much a PCA can improve the training speed of machine learning algorithms and also increase the quality of the forecast.
I also showed how the weights of principal component analysis can be saved and reused for future pre-processing steps.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    

    
  </body>
</html>

